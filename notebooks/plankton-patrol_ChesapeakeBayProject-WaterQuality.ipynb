{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary: SHAP and XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import shap\n",
    "import xgboost\n",
    "from sklearn.model_selection import KFold, train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import cupy as cp\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterqualitypath = \"../data/plankton-patrol/Plankton Patrol/Data/plankton-patrol_ChesapeakeWaterQuality.csv\"\n",
    "data = pd.read_csv(waterqualitypath)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Qualifier\"] = data[\"Qualifier\"].replace(np.nan, \"=\")\n",
    "\n",
    "\n",
    "columns_to_exclude = [\"Parameter\", \"MeasureValue\", \"Unit\"]\n",
    "unique_columns = [col for col in data.columns if col not in columns_to_exclude]\n",
    "\n",
    "df_unique = data[unique_columns].drop_duplicates(subset=\"EventId\")\n",
    "print(df_unique.shape, data.shape)\n",
    "data_r = data.pivot_table(\n",
    "    index=[\"EventId\"], columns=\"Parameter\", values=\"MeasureValue\", aggfunc=\"first\"\n",
    ").reset_index()\n",
    "exclude_from_pivoted = [\"Parameter\", \"MeasureValue\", \"Unit\", \"SampleDate\", \"SampleTime\"]\n",
    "pivoted_columns = data_r.columns.tolist()\n",
    "for ce in exclude_from_pivoted:\n",
    "    if ce in pivoted_columns:\n",
    "        pivoted_columns.remove(ce)\n",
    "data_m = pd.merge(df_unique, data_r, on=\"EventId\", how=\"left\")\n",
    "\n",
    "\n",
    "print(data_m.columns, data_m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['EventId']\n",
    "\n",
    "# Drop the specified columns\n",
    "data_md = data_m.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_md.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_numeric = [\n",
    "    \"TotalDepth\",\n",
    "    \"UpperPycnocline\",\n",
    "    \"LowerPycnocline\",\n",
    "    \"Depth\",\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "    \"MeasureValue\",\n",
    "    \"CHLA\",\n",
    "    \"DIN\",\n",
    "    \"DO\",\n",
    "    \"DOC\",\n",
    "    \"DON\",\n",
    "    \"DOP\",\n",
    "    \"DO_SAT_P\",\n",
    "    \"FSS\",\n",
    "    \"KD\",\n",
    "    \"NH4F\",\n",
    "    \"NO23F\",\n",
    "    \"NO2F\",\n",
    "    \"NO3F\",\n",
    "    \"PC\",\n",
    "    \"PH\",\n",
    "    \"PHEO\",\n",
    "    \"PIP\",\n",
    "    \"PN\",\n",
    "    \"PO4F\",\n",
    "    \"PP\",\n",
    "    \"Parameter\",\n",
    "    \"SALINITY\",\n",
    "    \"SECCHI\",\n",
    "    \"SIF\",\n",
    "    \"SIGMA_T\",\n",
    "    \"SPCOND\",\n",
    "    \"TDN\",\n",
    "    \"TDP\",\n",
    "    \"TN\",\n",
    "    \"TON\",\n",
    "    \"TP\",\n",
    "    \"TSS\",\n",
    "    \"TURB_NTU\",\n",
    "    \"VSS\",\n",
    "    \"WTEMP\",\n",
    "]\n",
    "# set_string = ['\n",
    "set_date = [\"SampleDate\", \"SampleTime\"]\n",
    "\n",
    "\n",
    "def combine_date_time_strings(df, date_col, time_col):\n",
    "    # Combine date and time strings\n",
    "    combined_col = df[date_col] + \" \" + df[time_col]\n",
    "    # Convert the combined string to datetime\n",
    "    datetime_col = \"Sample\"\n",
    "\n",
    "    df[datetime_col] = pd.to_datetime(combined_col, errors=\"coerce\")\n",
    "    df[datetime_col + \"_year\"] = df[datetime_col].dt.year\n",
    "    df[datetime_col + \"_month\"] = df[datetime_col].dt.month\n",
    "    df[datetime_col + \"_day\"] = df[datetime_col].dt.day\n",
    "    df[datetime_col + \"_hour\"] = df[datetime_col].dt.hour\n",
    "    df[datetime_col + \"_minute\"] = df[datetime_col].dt.minute\n",
    "    df[datetime_col + \"_second\"] = df[datetime_col].dt.second\n",
    "    newcolnames = [\n",
    "        f\"{datetime_col}_year\",\n",
    "        f\"{datetime_col}_month\",\n",
    "        f\"{datetime_col}_day\",\n",
    "        f\"{datetime_col}_hour\",\n",
    "        f\"{datetime_col}_minute\",\n",
    "        f\"{datetime_col}_second\",\n",
    "    ]\n",
    "    # Drop the original date, time, and combined datetime columns\n",
    "    df.drop(columns=[date_col, time_col, datetime_col], inplace=True)\n",
    "\n",
    "    return df, newcolnames\n",
    "\n",
    "\n",
    "# Function to convert columns to appropriate types\n",
    "def convert_dtypes(df):\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].replace(\"nan\", np.nan)\n",
    "        # print(df[col][564463])\n",
    "        print(f\"converting column {col}\", end=\"\\t\")\n",
    "        print(df[col].dtype)\n",
    "\n",
    "        if col not in (set_numeric + set_date):\n",
    "            df[col] = pd.Categorical(df[col])\n",
    "            print(\"Categorical\")\n",
    "        elif col in set_numeric:\n",
    "            # try:\n",
    "            # Try converting to numeric (float)\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "            print(\"Numeric\")\n",
    "        # except (ValueError, TypeError):\n",
    "        # try:\n",
    "        elif col in set_date:\n",
    "            print(\"Date, skipped\")\n",
    "            pass\n",
    "            # Try converting to datetime (date)\n",
    "            # df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "            # except (ValueError, TypeError):\n",
    "            # Check for categorical\n",
    "            # unique_ratio = df[col].nunique() / df[col].count()\n",
    "            # if unique_ratio < 0.2:  # heuristic for categorical, adjustable threshold\n",
    "            # Convert to string if not numeric, date, or categorical\n",
    "        else:\n",
    "            print(f\"{col}: string\")\n",
    "            df[col] = df[col].astype(str)\n",
    "            print(\"string\")\n",
    "        print(df[col].dtype)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply the conversion function\n",
    "datacopy = data_md.__deepcopy__()\n",
    "data_conv = convert_dtypes(datacopy)\n",
    "data_cleana, newcols = combine_date_time_strings(data_conv, \"SampleDate\", \"SampleTime\")\n",
    "# Check the result\n",
    "print(data_cleana.dtypes)\n",
    "print(data_cleana.shape)\n",
    "# data.dropna(axis='TotalDepth',how='all')\n",
    "data_cleana = data_cleana[:-1]\n",
    "print(data_cleana.shape)\n",
    "\n",
    "# data_conv = convert_dtypes(data_conv)\n",
    "# for col in data.columns:\n",
    "#     # print(data[col])\n",
    "#     if col not in (set_float + set_date):\n",
    "#         data[col]= pd.Categorical(df[col])(data[col][1:])\n",
    "#     # if col in set_string:\n",
    "#     #     data[col][1:]=data[col][1:].astype(\"string\")\n",
    "#     if col in set_float:\n",
    "#         data[col]= pd.to_numeric(data[col], errors='raise')\n",
    "#     if col in set_date:\n",
    "#         print(col)\n",
    "#         data[col][1:]=pd.to_datetime(data[col][1:],errors='coerce')\n",
    "#         # data[col]=data[col].dt.strftime(f'%m/%d/%Y')\n",
    "data_cleana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_columns= pivoted_columns+newcols\n",
    "print(len(pivoted_columns), pivoted_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivoted_columns = [col for col in pivoted_columns if col in data_clean.columns]\n",
    "\n",
    "# data_clean = data_clean.replace([np.inf, -np.inf], np.nan).dropna(\n",
    "#     subset=[pivoted_columns]\n",
    "# )\n",
    "data_cleana = data_cleana.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "\n",
    "missing_percentage = data_cleana.isnull().mean()\n",
    "print(missing_percentage)\n",
    "clean_columns = data_cleana.columns.tolist()\n",
    "\n",
    "# Drop columns where more than 95% of the data is missing\n",
    "columns_to_drop = missing_percentage[missing_percentage > 0.90].index\n",
    "print(columns_to_drop)\n",
    "data_cleana = data_cleana.drop(columns=columns_to_drop)\n",
    "data_clean = data_cleana.copy()\n",
    "data_clean = data_cleana.dropna(subset=['CHLA'])\n",
    "# data_clean = data_cleana.dropna(\n",
    "#     subset=[\n",
    "#         \"CHLA\",\n",
    "#         \"DIN\",\n",
    "#         \"DO\",\n",
    "#         \"DOC\",\n",
    "#         \"DON\",\n",
    "#         \"DOP\",\n",
    "#         \"DO_SAT_P\",\n",
    "#         \"FSS\",\n",
    "#         \"KD\",\n",
    "#         \"NH4F\",\n",
    "#         \"NO23F\",\n",
    "#         \"NO2F\",\n",
    "#         \"NO3F\",\n",
    "#         \"PC\",\n",
    "#         \"PH\",\n",
    "#         \"PHEO\",\n",
    "#         \"PN\",\n",
    "#         \"PO4F\",\n",
    "#         \"PP\",\n",
    "#         \"SALINITY\",\n",
    "#         \"SECCHI\",\n",
    "#         \"SIF\",\n",
    "#         \"SIGMA_T\",\n",
    "#         \"SPCOND\",\n",
    "#         \"TDN\",\n",
    "#         \"TDP\",\n",
    "#         \"TN\",\n",
    "#         \"TON\",\n",
    "#         \"TP\",\n",
    "#         \"TSS\",\n",
    "#         \"VSS\",\n",
    "#         \"WTEMP\",\n",
    "#     ]\n",
    "# )\n",
    "dropped_columns = [col for col in clean_columns if col not in data_clean.columns]\n",
    "\n",
    "data_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dropped_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X, y = data_clean.drop(['CHLA'], axis=1), data_clean['CHLA']\n",
    "# X = pd.get_dummies(X, drop_first=True) # TOO much memory\n",
    "# Label encode categorical variables\n",
    "for col in X.select_dtypes(include=['category']).columns:\n",
    "    X[col] = X[col].cat.codes\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def visualize_param_map(df, parameter, path):\n",
    "    latitude = df['Latitude']\n",
    "    longitude = df['Longitude']\n",
    "    cha = df[parameter]\n",
    "\n",
    "    # Create a scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(longitude, latitude, c=cha, cmap='coolwarm', marker='o')\n",
    "\n",
    "    # Add a color bar\n",
    "    plt.colorbar(scatter, label='CHA Values')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.title(f'{parameter} Map')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.grid(True)\n",
    "    # plt.show()\n",
    "    plt.savefig(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listparams =['DIN', 'DO', 'DOC', 'DON', 'DOP',\n",
    "       'DO_SAT_P', 'FSS', 'KD', 'NH4F', 'NO23F', 'NO2F', 'NO3F', 'PC', 'PH',\n",
    "       'PHEO', 'PN', 'PO4F', 'PP', 'SALINITY', 'SECCHI',\n",
    "       'SIF', 'SIGMA_T', 'SPCOND', 'TDN', 'TDP', 'TN', 'TON', 'TP', 'TSS',\n",
    "       'VSS', 'WTEMP']\n",
    "\n",
    "\n",
    "# import os\n",
    "# path = \"../data/visualizations\"\n",
    "# for p in listparams:\n",
    "#     visualize_param_map(data_clean, p, os.path.join(path, f\"{p}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = X_train\n",
    "# print(\"Original DataFrame:\")\n",
    "# print(df)\n",
    "\n",
    "# # Identify NaNs in the DataFrame\n",
    "# nan_locations = df.isna()\n",
    "# print(\"\\nLocations of NaNs in the DataFrame:\")\n",
    "# print(nan_locations)\n",
    "\n",
    "# # Count NaNs in each column\n",
    "# nan_count_per_column = df.isna().sum()\n",
    "# print(\"\\nCount of NaNs in each column:\")\n",
    "# print(nan_count_per_column)\n",
    "\n",
    "# # Count NaNs in each row\n",
    "# nan_count_per_row = df.isna().sum(axis=1)\n",
    "# print(\"\\nCount of NaNs in each row:\")\n",
    "# print(nan_count_per_row)\n",
    "\n",
    "# # Rows with at least one NaN\n",
    "# rows_with_nans = df[df.isna().any(axis=1)]\n",
    "# print(\"\\nRows with at least one NaN:\")\n",
    "# print(rows_with_nans)\n",
    "\n",
    "# # Columns with at least one NaN\n",
    "# columns_with_nans = df.columns[df.isna().any()].tolist()\n",
    "# print(\"\\nColumns with at least one NaN:\")\n",
    "# print(columns_with_nans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train an XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid to search over\n",
    "# model = xgboost.XGBRegressor(\n",
    "#     max_depth=3,  # Maximum depth of a tree\n",
    "#     learning_rate=0.1,  # Learning rate\n",
    "#     n_estimators=100,  # Number of trees\n",
    "#     subsample=0.8,  # Subsample ratio of the training instances\n",
    "#     colsample_bytree=0.8,  # Subsample ratio of columns when constructing each tree\n",
    "#     gamma=0,  # Minimum loss reduction required to make a further partition\n",
    "#     min_child_weight=1,  # Minimum sum of instance weight needed in a child\n",
    "#     device=\"cuda\",\n",
    "#     enable_categorical=True,\n",
    "# )\n",
    "param_grid = {\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"learning_rate\": [0.1, 0.01, 0.001, 0.0001],\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "}\n",
    "models = {\n",
    "    \"LightGBM\": (\n",
    "        lgb.LGBMRegressor(),\n",
    "        {\n",
    "            \"max_depth\": [3, 5, 7],\n",
    "            \"n_estimators\": [50, 100, 200],\n",
    "            \"learning_rate\": [0.1, 0.01, 0.001, 0.0001]\n",
    "        },\n",
    "    ),\n",
    "    \"XGBoost\": (\n",
    "        xgb.XGBRegressor(),\n",
    "        {\n",
    "            \"max_depth\": [3, 5, 7],\n",
    "            \"n_estimators\": [50, 100, 200],\n",
    "            \"learning_rate\": [0.1, 0.01, 0.001, 0.0001]\n",
    "        },\n",
    "    ),\n",
    "    \"MLPRegressor\": (\n",
    "        MLPRegressor(max_iter=1000),\n",
    "        {\"hidden_layer_sizes\": [(50, 50), (100,)], \"learning_rate_init\": [0.001, 0.01]},\n",
    "    ),\n",
    "}\n",
    "\n",
    "# models = {\n",
    "#     'LightGBM': lgb.LGBMRegressor(),\n",
    "#     'XGBoost': xgb.XGBRegressor(),\n",
    "#     'MLPRegressor': CatBoostRegressor(silent=True)\n",
    "# }\n",
    "# Make custom scorer\n",
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "mse_scores = []\n",
    "kfold_bestparams = []\n",
    "allshapvalues = []\n",
    "best_models = {}\n",
    "kf = KFold(n_splits=5)\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "\n",
    "\n",
    "    # Create a GridSearchCV object\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model, param_grid=param_grid, cv=kf, scoring=mse_scorer\n",
    "    )\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(\n",
    "    #     X, y, test_size=0.2, random_state=42\n",
    "    # )\n",
    "    # Fit the grid search to the training data\n",
    "    # grid_search.fit(X_train, y_train)\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Get the best parameters\n",
    "    # best_params = grid_search.best_params_\n",
    "    # print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "    # Create a new model with the best parameters\n",
    "    # best_model = model(**best_params)\n",
    "    # # Train the best model\n",
    "    # best_model.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Store the best model and its parameters\n",
    "    best_models[model_name] = {\n",
    "        \"best_estimator\": best_model,\n",
    "        \"best_params\": grid_search.best_params_,\n",
    "        \"best_score\": grid_search.best_score_\n",
    "    }\n",
    "    # Predict on the test set\n",
    "    # y_pred_best = best_model.predict(X_test)\n",
    "    # mse = mean_squared_error(y_test, y_pred_best)\n",
    "\n",
    "    # Evaluate the best model\n",
    "    # accuracy_best = (y_pred_best == y_test).mean()\n",
    "    \n",
    "    print(f\"MSE with Best Parameters: {best_models[model_name]['best_score']}\")\n",
    "    mse_scores.append(best_models[model_name]['best_score'])\n",
    "    kfold_bestparams.append(best_models[model_name]['best_params'])\n",
    "\n",
    "    # plt.figure()\n",
    "    explainer = shap.Explainer(model=best_model, masker=X)\n",
    "    # plt.savefig(\n",
    "    # f\"shap_explainer_best_{model_name}_fold-{i}.png\", bbox_inches=\"tight\"\n",
    "    # )\n",
    "    # plt.close()\n",
    "    # for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    #     X_train, X_test, y_train, y_test = (\n",
    "    #         X.iloc[train_index],\n",
    "    #         X.iloc[test_index],\n",
    "    #         y.iloc[train_index],\n",
    "    #         y.iloc[test_index],\n",
    "    #     )\n",
    "    shap_values = explainer(X)\n",
    "    allshapvalues.append(shap_values)\n",
    "    explainer.__class__\n",
    "\n",
    "    # plt.figure()\n",
    "    shap.summary_plot(shap_values, X, show=False)\n",
    "    # plt.savefig(\n",
    "    #     f\"shap_summary_plot_best_{model_name}_fold-{i}.png\", bbox_inches=\"tight\"\n",
    "    # )\n",
    "    # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for shap_values in allshapvalues:\n",
    "    # shap.plots.waterfall(shap_values[50], max_display=58)\n",
    "    # shap.initjs()\n",
    "    shap.plots.bar(shap_values, max_display=11)\n",
    "\n",
    "    # shap.plots.beeswarm(shap_values, max_display=58)\n",
    "    shap.plots.beeswarm(shap_values.abs, max_display=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# most_freq = max(set(kfold_bestparams.items()), key=kfold_bestparams.count)\n",
    "\n",
    "print(44.31 + 79.62)\n",
    "print(38.40+89.59)\n",
    "# for bestparams,msescore in zip(kfold_bestparams, mse_scores):\n",
    "        \n",
    "# # params = dict()\n",
    "# # params[\"device\"] = \"cuda\"\n",
    "# # params[\"tree_method\"] = \"hist\"\n",
    "\n",
    "# model = xgboost.XGBRegressor(missing=np.nan, enable_categorical=True, device=\"cuda\")\n",
    "# model.fit(X_train, y_train)\n",
    "# grid_search.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Estimate the Shapley values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "explainer = shap.Explainer(model=model, masker=X_train)\n",
    "explainer.__class__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.force(shap_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values, max_display=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap.initjs()\n",
    "shap.plots.force(shap_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values,max_display=58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap[:, \"PN\"], color=shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
