{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary: SHAP and XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import shap\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearnex import patch_sklearn, config_context\n",
    "patch_sklearn()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pushk\\AppData\\Local\\Temp\\ipykernel_24428\\2510223144.py:2: DtypeWarning: Columns (1,10,11,12,13,19,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(waterqualitypath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2162314 entries, 0 to 2162313\n",
      "Data columns (total 26 columns):\n",
      " #   Column               Dtype \n",
      "---  ------               ----- \n",
      " 0   CBSeg2003            object\n",
      " 1   EventId              object\n",
      " 2   Cruise               object\n",
      " 3   Program              object\n",
      " 4   Project              object\n",
      " 5   Agency               object\n",
      " 6   Source               object\n",
      " 7   Station              object\n",
      " 8   SampleDate           object\n",
      " 9   SampleTime           object\n",
      " 10  TotalDepth           object\n",
      " 11  UpperPycnocline      object\n",
      " 12  LowerPycnocline      object\n",
      " 13  Depth                object\n",
      " 14  Layer                object\n",
      " 15  SampleType           object\n",
      " 16  SampleReplicateType  object\n",
      " 17  Parameter            object\n",
      " 18  Qualifier            object\n",
      " 19  MeasureValue         object\n",
      " 20  Unit                 object\n",
      " 21  Method               object\n",
      " 22  Lab                  object\n",
      " 23  Latitude             object\n",
      " 24  Longitude            object\n",
      " 25  TierLevel            object\n",
      "dtypes: object(26)\n",
      "memory usage: 428.9+ MB\n"
     ]
    }
   ],
   "source": [
    "waterqualitypath = \"../data/plankton-patrol/Plankton Patrol/Data/plankton-patrol_ChesapeakeWaterQuality.csv\"\n",
    "data = pd.read_csv(waterqualitypath)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CBSeg2003', 'EventId', 'Cruise', 'Program', 'Project', 'Agency',\n",
       "       'Source', 'Station', 'SampleDate', 'SampleTime', 'TotalDepth',\n",
       "       'UpperPycnocline', 'LowerPycnocline', 'Depth', 'Layer', 'SampleType',\n",
       "       'SampleReplicateType', 'Parameter', 'Qualifier', 'MeasureValue', 'Unit',\n",
       "       'Method', 'Lab', 'Latitude', 'Longitude', 'TierLevel'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20855, 23) (2162314, 26)\n",
      "Index(['CBSeg2003', 'EventId', 'Cruise', 'Program', 'Project', 'Agency',\n",
      "       'Source', 'Station', 'SampleDate', 'SampleTime', 'TotalDepth',\n",
      "       'UpperPycnocline', 'LowerPycnocline', 'Depth', 'Layer', 'SampleType',\n",
      "       'SampleReplicateType', 'Qualifier', 'Method', 'Lab', 'Latitude',\n",
      "       'Longitude', 'TierLevel', 'CHLA', 'DIN', 'DO', 'DOC', 'DON', 'DOP',\n",
      "       'DO_SAT_P', 'FSS', 'KD', 'NH4F', 'NO23F', 'NO2F', 'NO3F', 'PC', 'PH',\n",
      "       'PHEO', 'PIP', 'PN', 'PO4F', 'PP', 'Parameter', 'SALINITY', 'SECCHI',\n",
      "       'SIF', 'SIGMA_T', 'SPCOND', 'TDN', 'TDP', 'TN', 'TON', 'TP', 'TSS',\n",
      "       'TURB_NTU', 'VSS', 'WTEMP'],\n",
      "      dtype='object') (20855, 58)\n"
     ]
    }
   ],
   "source": [
    "data[\"Qualifier\"] = data[\"Qualifier\"].replace(np.nan, \"=\")\n",
    "\n",
    "\n",
    "columns_to_exclude = [\"Parameter\", \"MeasureValue\", \"Unit\"]\n",
    "unique_columns = [col for col in data.columns if col not in columns_to_exclude]\n",
    "\n",
    "df_unique = data[unique_columns].drop_duplicates(subset=\"EventId\")\n",
    "print(df_unique.shape, data.shape)\n",
    "data_r = data.pivot_table(\n",
    "    index=[\"EventId\"], columns=\"Parameter\", values=\"MeasureValue\", aggfunc=\"first\"\n",
    ").reset_index()\n",
    "exclude_from_pivoted = [\"Parameter\", \"MeasureValue\", \"Unit\", \"SampleDate\", \"SampleTime\"]\n",
    "pivoted_columns = data_r.columns.tolist()\n",
    "for ce in exclude_from_pivoted:\n",
    "    if ce in pivoted_columns:\n",
    "        pivoted_columns.remove(ce)\n",
    "data_m = pd.merge(df_unique, data_r, on=\"EventId\", how=\"left\")\n",
    "\n",
    "\n",
    "print(data_m.columns, data_m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['EventId']\n",
    "\n",
    "# Drop the specified columns\n",
    "data_md = data_m.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CBSeg2003</th>\n",
       "      <th>Cruise</th>\n",
       "      <th>Program</th>\n",
       "      <th>Project</th>\n",
       "      <th>Agency</th>\n",
       "      <th>Source</th>\n",
       "      <th>Station</th>\n",
       "      <th>SampleDate</th>\n",
       "      <th>SampleTime</th>\n",
       "      <th>TotalDepth</th>\n",
       "      <th>...</th>\n",
       "      <th>SPCOND</th>\n",
       "      <th>TDN</th>\n",
       "      <th>TDP</th>\n",
       "      <th>TN</th>\n",
       "      <th>TON</th>\n",
       "      <th>TP</th>\n",
       "      <th>TSS</th>\n",
       "      <th>TURB_NTU</th>\n",
       "      <th>VSS</th>\n",
       "      <th>WTEMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CB1TF</td>\n",
       "      <td>BAY451</td>\n",
       "      <td>TWQM</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>CB1.1</td>\n",
       "      <td>12/13/2006</td>\n",
       "      <td>10:58:00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>195.0</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>1.7183</td>\n",
       "      <td>0.2773</td>\n",
       "      <td>0.0308</td>\n",
       "      <td>7.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CB1TF</td>\n",
       "      <td>BAY452</td>\n",
       "      <td>TWQM</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>CB1.1</td>\n",
       "      <td>1/12/2007</td>\n",
       "      <td>13:00:00</td>\n",
       "      <td>5.5</td>\n",
       "      <td>...</td>\n",
       "      <td>137.0</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>1.4958</td>\n",
       "      <td>0.2498</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>11.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CB1TF</td>\n",
       "      <td>BAY454</td>\n",
       "      <td>TWQM</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>CB1.1</td>\n",
       "      <td>3/14/2007</td>\n",
       "      <td>10:41:00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>234.0</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>2.055</td>\n",
       "      <td>0.2902</td>\n",
       "      <td>0.0305</td>\n",
       "      <td>7.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CB1TF</td>\n",
       "      <td>BAY456</td>\n",
       "      <td>TWQM</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>CB1.1</td>\n",
       "      <td>4/11/2007</td>\n",
       "      <td>10:32:00</td>\n",
       "      <td>6.5</td>\n",
       "      <td>...</td>\n",
       "      <td>185.0</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>1.4153</td>\n",
       "      <td>0.1653</td>\n",
       "      <td>0.0237</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.8</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CB1TF</td>\n",
       "      <td>BAY457</td>\n",
       "      <td>TWQM</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>CB1.1</td>\n",
       "      <td>4/26/2007</td>\n",
       "      <td>11:46:00</td>\n",
       "      <td>5.5</td>\n",
       "      <td>...</td>\n",
       "      <td>142.0</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>1.1297</td>\n",
       "      <td>0.1568</td>\n",
       "      <td>0.0331</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  CBSeg2003  Cruise Program Project Agency Source Station  SampleDate  \\\n",
       "0    CB1TF   BAY451    TWQM    MAIN  MDDNR  MDDNR   CB1.1  12/13/2006   \n",
       "1    CB1TF   BAY452    TWQM    MAIN  MDDNR  MDDNR   CB1.1   1/12/2007   \n",
       "2    CB1TF   BAY454    TWQM    MAIN  MDDNR  MDDNR   CB1.1   3/14/2007   \n",
       "3    CB1TF   BAY456    TWQM    MAIN  MDDNR  MDDNR   CB1.1   4/11/2007   \n",
       "4    CB1TF   BAY457    TWQM    MAIN  MDDNR  MDDNR   CB1.1   4/26/2007   \n",
       "\n",
       "  SampleTime TotalDepth  ... SPCOND   TDN     TDP      TN     TON      TP  \\\n",
       "0   10:58:00        7.0  ...  195.0  1.67  0.0181  1.7183  0.2773  0.0308   \n",
       "1   13:00:00        5.5  ...  137.0  1.41  0.0168  1.4958  0.2498  0.0384   \n",
       "2   10:41:00        7.0  ...  234.0  1.98  0.0183   2.055  0.2902  0.0305   \n",
       "3   10:32:00        6.5  ...  185.0  1.35  0.0098  1.4153  0.1653  0.0237   \n",
       "4   11:46:00        5.5  ...  142.0  1.04  0.0132  1.1297  0.1568  0.0331   \n",
       "\n",
       "    TSS TURB_NTU  VSS WTEMP  \n",
       "0   7.3      NaN  2.0   5.5  \n",
       "1  11.3      NaN  5.3   5.3  \n",
       "2   7.4      NaN  3.2   2.6  \n",
       "3   8.0      NaN  2.8   7.6  \n",
       "4  15.0      NaN  3.0  13.9  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_md.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EventId',\n",
       " 'CHLA',\n",
       " 'DIN',\n",
       " 'DO',\n",
       " 'DOC',\n",
       " 'DON',\n",
       " 'DOP',\n",
       " 'DO_SAT_P',\n",
       " 'FSS',\n",
       " 'KD',\n",
       " 'NH4F',\n",
       " 'NO23F',\n",
       " 'NO2F',\n",
       " 'NO3F',\n",
       " 'PC',\n",
       " 'PH',\n",
       " 'PHEO',\n",
       " 'PIP',\n",
       " 'PN',\n",
       " 'PO4F',\n",
       " 'PP',\n",
       " 'SALINITY',\n",
       " 'SECCHI',\n",
       " 'SIF',\n",
       " 'SIGMA_T',\n",
       " 'SPCOND',\n",
       " 'TDN',\n",
       " 'TDP',\n",
       " 'TN',\n",
       " 'TON',\n",
       " 'TP',\n",
       " 'TSS',\n",
       " 'TURB_NTU',\n",
       " 'VSS',\n",
       " 'WTEMP']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting column CBSeg2003\tobject\n",
      "Categorical\n",
      "category\n",
      "converting column Cruise\tobject\n",
      "Categorical\n",
      "category\n",
      "converting column Program\tobject\n",
      "Categorical\n",
      "category\n",
      "converting column Project\tobject\n",
      "Categorical\n",
      "category\n",
      "converting column Agency\tobject\n",
      "Categorical\n",
      "category\n",
      "converting column Source\tobject\n",
      "Categorical\n",
      "category\n",
      "converting column Station\tobject\n",
      "Categorical\n",
      "category\n",
      "converting column SampleDate\tobject\n",
      "Date, skipped\n",
      "object\n",
      "converting column SampleTime\tobject\n",
      "Date, skipped\n",
      "object\n",
      "converting column TotalDepth\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column UpperPycnocline\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column LowerPycnocline\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column Depth\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column Layer\tobject\n",
      "Categorical\n",
      "category\n",
      "converting column SampleType\tobject\n",
      "Categorical\n",
      "category\n",
      "converting column SampleReplicateType\tobject\n",
      "Categorical\n",
      "category\n",
      "converting column Qualifier\tobject\n",
      "Categorical\n",
      "category\n",
      "converting column Method\tobject\n",
      "Categorical\n",
      "category\n",
      "converting column Lab\tobject\n",
      "Categorical\n",
      "category\n",
      "converting column Latitude\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column Longitude\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column TierLevel\tobject\n",
      "Categorical\n",
      "category\n",
      "converting column CHLA\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column DIN\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column DO\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column DOC\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column DON\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column DOP\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column DO_SAT_P\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column FSS\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column KD\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column NH4F\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column NO23F\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column NO2F\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column NO3F\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column PC\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column PH\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column PHEO\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column PIP\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column PN\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column PO4F\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column PP\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column Parameter\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column SALINITY\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column SECCHI\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column SIF\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column SIGMA_T\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column SPCOND\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column TDN\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column TDP\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column TN\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column TON\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column TP\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column TSS\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column TURB_NTU\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column VSS\tobject\n",
      "Numeric\n",
      "float64\n",
      "converting column WTEMP\tobject\n",
      "Numeric\n",
      "float64\n",
      "CBSeg2003        category\n",
      "Cruise           category\n",
      "Program          category\n",
      "Project          category\n",
      "Agency           category\n",
      "                   ...   \n",
      "Sample_month      float64\n",
      "Sample_day        float64\n",
      "Sample_hour       float64\n",
      "Sample_minute     float64\n",
      "Sample_second     float64\n",
      "Length: 61, dtype: object\n",
      "(20855, 61)\n",
      "(20854, 61)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CBSeg2003</th>\n",
       "      <th>Cruise</th>\n",
       "      <th>Program</th>\n",
       "      <th>Project</th>\n",
       "      <th>Agency</th>\n",
       "      <th>Source</th>\n",
       "      <th>Station</th>\n",
       "      <th>TotalDepth</th>\n",
       "      <th>UpperPycnocline</th>\n",
       "      <th>LowerPycnocline</th>\n",
       "      <th>...</th>\n",
       "      <th>TSS</th>\n",
       "      <th>TURB_NTU</th>\n",
       "      <th>VSS</th>\n",
       "      <th>WTEMP</th>\n",
       "      <th>Sample_year</th>\n",
       "      <th>Sample_month</th>\n",
       "      <th>Sample_day</th>\n",
       "      <th>Sample_hour</th>\n",
       "      <th>Sample_minute</th>\n",
       "      <th>Sample_second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CB1TF</td>\n",
       "      <td>BAY451</td>\n",
       "      <td>TWQM</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>CB1.1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CB1TF</td>\n",
       "      <td>BAY452</td>\n",
       "      <td>TWQM</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>CB1.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>11.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CB1TF</td>\n",
       "      <td>BAY454</td>\n",
       "      <td>TWQM</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>CB1.1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CB1TF</td>\n",
       "      <td>BAY456</td>\n",
       "      <td>TWQM</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>CB1.1</td>\n",
       "      <td>6.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.8</td>\n",
       "      <td>7.6</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CB1TF</td>\n",
       "      <td>BAY457</td>\n",
       "      <td>TWQM</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>CB1.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20849</th>\n",
       "      <td>TANMH</td>\n",
       "      <td>PART19</td>\n",
       "      <td>TWQM</td>\n",
       "      <td>PARTTRIB</td>\n",
       "      <td>CMC</td>\n",
       "      <td>MDE</td>\n",
       "      <td>MDE.1802025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.9</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20850</th>\n",
       "      <td>TANMH</td>\n",
       "      <td>PART19</td>\n",
       "      <td>TWQM</td>\n",
       "      <td>PARTTRIB</td>\n",
       "      <td>CMC</td>\n",
       "      <td>MDE</td>\n",
       "      <td>MDE.1802025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.9</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20851</th>\n",
       "      <td>TANMH</td>\n",
       "      <td>PART22</td>\n",
       "      <td>TWQM</td>\n",
       "      <td>PARTTRIB</td>\n",
       "      <td>CMC</td>\n",
       "      <td>MDE</td>\n",
       "      <td>MDE.1802025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.3</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20852</th>\n",
       "      <td>TANMH</td>\n",
       "      <td>PART22</td>\n",
       "      <td>TWQM</td>\n",
       "      <td>PARTTRIB</td>\n",
       "      <td>CMC</td>\n",
       "      <td>MDE</td>\n",
       "      <td>MDE.1802025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.7</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20853</th>\n",
       "      <td>TANMH</td>\n",
       "      <td>BAY841</td>\n",
       "      <td>SWM</td>\n",
       "      <td>CMON</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>XCI3607</td>\n",
       "      <td>2.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.7</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20854 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CBSeg2003  Cruise Program   Project Agency Source      Station  \\\n",
       "0        CB1TF   BAY451    TWQM      MAIN  MDDNR  MDDNR        CB1.1   \n",
       "1        CB1TF   BAY452    TWQM      MAIN  MDDNR  MDDNR        CB1.1   \n",
       "2        CB1TF   BAY454    TWQM      MAIN  MDDNR  MDDNR        CB1.1   \n",
       "3        CB1TF   BAY456    TWQM      MAIN  MDDNR  MDDNR        CB1.1   \n",
       "4        CB1TF   BAY457    TWQM      MAIN  MDDNR  MDDNR        CB1.1   \n",
       "...         ...     ...     ...       ...    ...    ...          ...   \n",
       "20849    TANMH   PART19    TWQM  PARTTRIB    CMC    MDE  MDE.1802025   \n",
       "20850    TANMH   PART19    TWQM  PARTTRIB    CMC    MDE  MDE.1802025   \n",
       "20851    TANMH   PART22    TWQM  PARTTRIB    CMC    MDE  MDE.1802025   \n",
       "20852    TANMH   PART22    TWQM  PARTTRIB    CMC    MDE  MDE.1802025   \n",
       "20853    TANMH   BAY841     SWM      CMON  MDDNR  MDDNR      XCI3607   \n",
       "\n",
       "       TotalDepth  UpperPycnocline  LowerPycnocline  ...   TSS TURB_NTU  VSS  \\\n",
       "0             7.0              NaN              NaN  ...   7.3      NaN  2.0   \n",
       "1             5.5              NaN              NaN  ...  11.3      NaN  5.3   \n",
       "2             7.0              NaN              NaN  ...   7.4      NaN  3.2   \n",
       "3             6.5              NaN              NaN  ...   8.0      NaN  2.8   \n",
       "4             5.5              NaN              NaN  ...  15.0      NaN  3.0   \n",
       "...           ...              ...              ...  ...   ...      ...  ...   \n",
       "20849         NaN              NaN              NaN  ...   NaN      NaN  NaN   \n",
       "20850         NaN              NaN              NaN  ...   NaN      NaN  NaN   \n",
       "20851         NaN              NaN              NaN  ...   NaN      NaN  NaN   \n",
       "20852         NaN              NaN              NaN  ...   NaN      NaN  NaN   \n",
       "20853         2.3              NaN              NaN  ...   NaN      NaN  NaN   \n",
       "\n",
       "      WTEMP Sample_year Sample_month Sample_day  Sample_hour  Sample_minute  \\\n",
       "0       5.5      2006.0         12.0       13.0         10.0           58.0   \n",
       "1       5.3      2007.0          1.0       12.0         13.0            0.0   \n",
       "2       2.6      2007.0          3.0       14.0         10.0           41.0   \n",
       "3       7.6      2007.0          4.0       11.0         10.0           32.0   \n",
       "4      13.9      2007.0          4.0       26.0         11.0           46.0   \n",
       "...     ...         ...          ...        ...          ...            ...   \n",
       "20849  27.9      2019.0          8.0       21.0         12.0           19.0   \n",
       "20850  25.9      2019.0          9.0       16.0         12.0           10.0   \n",
       "20851  26.3      2022.0          7.0        6.0         10.0           10.0   \n",
       "20852  28.7      2022.0          8.0        3.0         12.0           42.0   \n",
       "20853  20.7      2023.0          5.0       22.0          9.0            0.0   \n",
       "\n",
       "      Sample_second  \n",
       "0               0.0  \n",
       "1               0.0  \n",
       "2               0.0  \n",
       "3               0.0  \n",
       "4               0.0  \n",
       "...             ...  \n",
       "20849           0.0  \n",
       "20850           0.0  \n",
       "20851           0.0  \n",
       "20852           0.0  \n",
       "20853           0.0  \n",
       "\n",
       "[20854 rows x 61 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_numeric = [\n",
    "    \"TotalDepth\",\n",
    "    \"UpperPycnocline\",\n",
    "    \"LowerPycnocline\",\n",
    "    \"Depth\",\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "    \"MeasureValue\",\n",
    "    \"CHLA\",\n",
    "    \"DIN\",\n",
    "    \"DO\",\n",
    "    \"DOC\",\n",
    "    \"DON\",\n",
    "    \"DOP\",\n",
    "    \"DO_SAT_P\",\n",
    "    \"FSS\",\n",
    "    \"KD\",\n",
    "    \"NH4F\",\n",
    "    \"NO23F\",\n",
    "    \"NO2F\",\n",
    "    \"NO3F\",\n",
    "    \"PC\",\n",
    "    \"PH\",\n",
    "    \"PHEO\",\n",
    "    \"PIP\",\n",
    "    \"PN\",\n",
    "    \"PO4F\",\n",
    "    \"PP\",\n",
    "    \"Parameter\",\n",
    "    \"SALINITY\",\n",
    "    \"SECCHI\",\n",
    "    \"SIF\",\n",
    "    \"SIGMA_T\",\n",
    "    \"SPCOND\",\n",
    "    \"TDN\",\n",
    "    \"TDP\",\n",
    "    \"TN\",\n",
    "    \"TON\",\n",
    "    \"TP\",\n",
    "    \"TSS\",\n",
    "    \"TURB_NTU\",\n",
    "    \"VSS\",\n",
    "    \"WTEMP\",\n",
    "]\n",
    "# set_string = ['\n",
    "set_date = [\"SampleDate\", \"SampleTime\"]\n",
    "\n",
    "\n",
    "def combine_date_time_strings(df, date_col, time_col):\n",
    "    # Combine date and time strings\n",
    "    combined_col = df[date_col] + \" \" + df[time_col]\n",
    "    # Convert the combined string to datetime\n",
    "    datetime_col = \"Sample\"\n",
    "\n",
    "    df[datetime_col] = pd.to_datetime(combined_col, errors=\"coerce\")\n",
    "    df[datetime_col + \"_year\"] = df[datetime_col].dt.year\n",
    "    df[datetime_col + \"_month\"] = df[datetime_col].dt.month\n",
    "    df[datetime_col + \"_day\"] = df[datetime_col].dt.day\n",
    "    df[datetime_col + \"_hour\"] = df[datetime_col].dt.hour\n",
    "    df[datetime_col + \"_minute\"] = df[datetime_col].dt.minute\n",
    "    df[datetime_col + \"_second\"] = df[datetime_col].dt.second\n",
    "    newcolnames = [\n",
    "        f\"{datetime_col}_year\",\n",
    "        f\"{datetime_col}_month\",\n",
    "        f\"{datetime_col}_day\",\n",
    "        f\"{datetime_col}_hour\",\n",
    "        f\"{datetime_col}_minute\",\n",
    "        f\"{datetime_col}_second\",\n",
    "    ]\n",
    "    # Drop the original date, time, and combined datetime columns\n",
    "    df.drop(columns=[date_col, time_col, datetime_col], inplace=True)\n",
    "\n",
    "    return df, newcolnames\n",
    "\n",
    "\n",
    "# Function to convert columns to appropriate types\n",
    "def convert_dtypes(df):\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].replace(\"nan\", np.nan)\n",
    "        # print(df[col][564463])\n",
    "        print(f\"converting column {col}\", end=\"\\t\")\n",
    "        print(df[col].dtype)\n",
    "\n",
    "        if col not in (set_numeric + set_date):\n",
    "            df[col] = pd.Categorical(df[col])\n",
    "            print(\"Categorical\")\n",
    "        elif col in set_numeric:\n",
    "            # try:\n",
    "            # Try converting to numeric (float)\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "            print(\"Numeric\")\n",
    "        # except (ValueError, TypeError):\n",
    "        # try:\n",
    "        elif col in set_date:\n",
    "            print(\"Date, skipped\")\n",
    "            pass\n",
    "            # Try converting to datetime (date)\n",
    "            # df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "            # except (ValueError, TypeError):\n",
    "            # Check for categorical\n",
    "            # unique_ratio = df[col].nunique() / df[col].count()\n",
    "            # if unique_ratio < 0.2:  # heuristic for categorical, adjustable threshold\n",
    "            # Convert to string if not numeric, date, or categorical\n",
    "        else:\n",
    "            print(f\"{col}: string\")\n",
    "            df[col] = df[col].astype(str)\n",
    "            print(\"string\")\n",
    "        print(df[col].dtype)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply the conversion function\n",
    "datacopy = data_md.__deepcopy__()\n",
    "data_conv = convert_dtypes(datacopy)\n",
    "data_cleana, newcols = combine_date_time_strings(data_conv, \"SampleDate\", \"SampleTime\")\n",
    "# Check the result\n",
    "print(data_cleana.dtypes)\n",
    "print(data_cleana.shape)\n",
    "# data.dropna(axis='TotalDepth',how='all')\n",
    "data_cleana = data_cleana[:-1]\n",
    "print(data_cleana.shape)\n",
    "\n",
    "# data_conv = convert_dtypes(data_conv)\n",
    "# for col in data.columns:\n",
    "#     # print(data[col])\n",
    "#     if col not in (set_float + set_date):\n",
    "#         data[col]= pd.Categorical(df[col])(data[col][1:])\n",
    "#     # if col in set_string:\n",
    "#     #     data[col][1:]=data[col][1:].astype(\"string\")\n",
    "#     if col in set_float:\n",
    "#         data[col]= pd.to_numeric(data[col], errors='raise')\n",
    "#     if col in set_date:\n",
    "#         print(col)\n",
    "#         data[col][1:]=pd.to_datetime(data[col][1:],errors='coerce')\n",
    "#         # data[col]=data[col].dt.strftime(f'%m/%d/%Y')\n",
    "data_cleana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 ['EventId', 'CHLA', 'DIN', 'DO', 'DOC', 'DON', 'DOP', 'DO_SAT_P', 'FSS', 'KD', 'NH4F', 'NO23F', 'NO2F', 'NO3F', 'PC', 'PH', 'PHEO', 'PIP', 'PN', 'PO4F', 'PP', 'SALINITY', 'SECCHI', 'SIF', 'SIGMA_T', 'SPCOND', 'TDN', 'TDP', 'TN', 'TON', 'TP', 'TSS', 'TURB_NTU', 'VSS', 'WTEMP', 'Sample_year', 'Sample_month', 'Sample_day', 'Sample_hour', 'Sample_minute', 'Sample_second']\n"
     ]
    }
   ],
   "source": [
    "pivoted_columns= pivoted_columns+newcols\n",
    "print(len(pivoted_columns), pivoted_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBSeg2003        0.000000\n",
      "Cruise           0.026278\n",
      "Program          0.000048\n",
      "Project          0.000048\n",
      "Agency           0.000048\n",
      "                   ...   \n",
      "Sample_month     0.000096\n",
      "Sample_day       0.000096\n",
      "Sample_hour      0.000096\n",
      "Sample_minute    0.000096\n",
      "Sample_second    0.000096\n",
      "Length: 61, dtype: float64\n",
      "Index(['PIP', 'Parameter', 'TURB_NTU'], dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 17143 entries, 0 to 20817\n",
      "Data columns (total 58 columns):\n",
      " #   Column               Non-Null Count  Dtype   \n",
      "---  ------               --------------  -----   \n",
      " 0   CBSeg2003            17143 non-null  category\n",
      " 1   Cruise               16642 non-null  category\n",
      " 2   Program              17143 non-null  category\n",
      " 3   Project              17143 non-null  category\n",
      " 4   Agency               17143 non-null  category\n",
      " 5   Source               17143 non-null  category\n",
      " 6   Station              17143 non-null  category\n",
      " 7   TotalDepth           17128 non-null  float64 \n",
      " 8   UpperPycnocline      4574 non-null   float64 \n",
      " 9   LowerPycnocline      4574 non-null   float64 \n",
      " 10  Depth                17143 non-null  float64 \n",
      " 11  Layer                17143 non-null  category\n",
      " 12  SampleType           17143 non-null  category\n",
      " 13  SampleReplicateType  17143 non-null  category\n",
      " 14  Qualifier            17143 non-null  category\n",
      " 15  Method               17143 non-null  category\n",
      " 16  Lab                  17143 non-null  category\n",
      " 17  Latitude             17143 non-null  float64 \n",
      " 18  Longitude            17143 non-null  float64 \n",
      " 19  TierLevel            17143 non-null  category\n",
      " 20  CHLA                 17143 non-null  float64 \n",
      " 21  DIN                  12631 non-null  float64 \n",
      " 22  DO                   17029 non-null  float64 \n",
      " 23  DOC                  2232 non-null   float64 \n",
      " 24  DON                  12557 non-null  float64 \n",
      " 25  DOP                  12512 non-null  float64 \n",
      " 26  DO_SAT_P             2639 non-null   float64 \n",
      " 27  FSS                  8940 non-null   float64 \n",
      " 28  KD                   13177 non-null  float64 \n",
      " 29  NH4F                 12558 non-null  float64 \n",
      " 30  NO23F                12558 non-null  float64 \n",
      " 31  NO2F                 12558 non-null  float64 \n",
      " 32  NO3F                 12511 non-null  float64 \n",
      " 33  PC                   12552 non-null  float64 \n",
      " 34  PH                   16879 non-null  float64 \n",
      " 35  PHEO                 16884 non-null  float64 \n",
      " 36  PN                   12538 non-null  float64 \n",
      " 37  PO4F                 12533 non-null  float64 \n",
      " 38  PP                   12416 non-null  float64 \n",
      " 39  SALINITY             16575 non-null  float64 \n",
      " 40  SECCHI               16346 non-null  float64 \n",
      " 41  SIF                  7866 non-null   float64 \n",
      " 42  SIGMA_T              16216 non-null  float64 \n",
      " 43  SPCOND               15297 non-null  float64 \n",
      " 44  TDN                  12144 non-null  float64 \n",
      " 45  TDP                  12164 non-null  float64 \n",
      " 46  TN                   12080 non-null  float64 \n",
      " 47  TON                  11999 non-null  float64 \n",
      " 48  TP                   11671 non-null  float64 \n",
      " 49  TSS                  15740 non-null  float64 \n",
      " 50  VSS                  7553 non-null   float64 \n",
      " 51  WTEMP                16116 non-null  float64 \n",
      " 52  Sample_year          17143 non-null  float64 \n",
      " 53  Sample_month         17143 non-null  float64 \n",
      " 54  Sample_day           17143 non-null  float64 \n",
      " 55  Sample_hour          17143 non-null  float64 \n",
      " 56  Sample_minute        17143 non-null  float64 \n",
      " 57  Sample_second        17143 non-null  float64 \n",
      "dtypes: category(14), float64(44)\n",
      "memory usage: 6.2 MB\n"
     ]
    }
   ],
   "source": [
    "# pivoted_columns = [col for col in pivoted_columns if col in data_clean.columns]\n",
    "\n",
    "# data_clean = data_clean.replace([np.inf, -np.inf], np.nan).dropna(\n",
    "#     subset=[pivoted_columns]\n",
    "# )\n",
    "data_cleana = data_cleana.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "\n",
    "missing_percentage = data_cleana.isnull().mean()\n",
    "print(missing_percentage)\n",
    "clean_columns = data_cleana.columns.tolist()\n",
    "\n",
    "# Drop columns where more than 95% of the data is missing\n",
    "columns_to_drop = missing_percentage[missing_percentage > 0.90].index\n",
    "print(columns_to_drop)\n",
    "data_cleana = data_cleana.drop(columns=columns_to_drop)\n",
    "data_clean = data_cleana.copy()\n",
    "data_clean = data_cleana.dropna(subset=['CHLA'])\n",
    "# data_clean = data_cleana.dropna(\n",
    "#     subset=[\n",
    "#         \"CHLA\",\n",
    "#         \"DIN\",\n",
    "#         \"DO\",\n",
    "#         \"DOC\",\n",
    "#         \"DON\",\n",
    "#         \"DOP\",\n",
    "#         \"DO_SAT_P\",\n",
    "#         \"FSS\",\n",
    "#         \"KD\",\n",
    "#         \"NH4F\",\n",
    "#         \"NO23F\",\n",
    "#         \"NO2F\",\n",
    "#         \"NO3F\",\n",
    "#         \"PC\",\n",
    "#         \"PH\",\n",
    "#         \"PHEO\",\n",
    "#         \"PN\",\n",
    "#         \"PO4F\",\n",
    "#         \"PP\",\n",
    "#         \"SALINITY\",\n",
    "#         \"SECCHI\",\n",
    "#         \"SIF\",\n",
    "#         \"SIGMA_T\",\n",
    "#         \"SPCOND\",\n",
    "#         \"TDN\",\n",
    "#         \"TDP\",\n",
    "#         \"TN\",\n",
    "#         \"TON\",\n",
    "#         \"TP\",\n",
    "#         \"TSS\",\n",
    "#         \"VSS\",\n",
    "#         \"WTEMP\",\n",
    "#     ]\n",
    "# )\n",
    "dropped_columns = [col for col in clean_columns if col not in data_clean.columns]\n",
    "\n",
    "data_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PIP', 'Parameter', 'TURB_NTU']\n"
     ]
    }
   ],
   "source": [
    "print(dropped_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17143, 57), (17143,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X, y = data_clean.drop(['CHLA'], axis=1), data_clean['CHLA']\n",
    "# X = pd.get_dummies(X, drop_first=True) # TOO much memory\n",
    "# Label encode categorical variables\n",
    "for col in X.select_dtypes(include=['category']).columns:\n",
    "    X[col] = X[col].cat.codes\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def visualize_param_map(df, parameter, path):\n",
    "    latitude = df['Latitude']\n",
    "    longitude = df['Longitude']\n",
    "    cha = df[parameter]\n",
    "\n",
    "    # Create a scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(longitude, latitude, c=cha, cmap='coolwarm', marker='o')\n",
    "\n",
    "    # Add a color bar\n",
    "    plt.colorbar(scatter, label=f'{parameter} Values')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.title(f'{parameter} Map')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.grid(True)\n",
    "    # plt.show()\n",
    "    plt.savefig(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "listparams =['DIN', 'DO', 'DOC', 'DON', 'DOP',\n",
    "       'DO_SAT_P', 'FSS', 'KD', 'NH4F', 'NO23F', 'NO2F', 'NO3F', 'PC', 'PH',\n",
    "       'PHEO', 'PN', 'PO4F', 'PP', 'SALINITY', 'SECCHI',\n",
    "       'SIF', 'SIGMA_T', 'SPCOND', 'TDN', 'TDP', 'TN', 'TON', 'TP', 'TSS',\n",
    "       'VSS', 'WTEMP']\n",
    "\n",
    "\n",
    "import os\n",
    "path = \"../data/visualizations/waterquality\"\n",
    "# for p in listparams:\n",
    "#     visualize_param_map(data_clean, p, os.path.join(path, f\"{p}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = X_train\n",
    "# print(\"Original DataFrame:\")\n",
    "# print(df)\n",
    "\n",
    "# # Identify NaNs in the DataFrame\n",
    "# nan_locations = df.isna()\n",
    "# print(\"\\nLocations of NaNs in the DataFrame:\")\n",
    "# print(nan_locations)\n",
    "\n",
    "# # Count NaNs in each column\n",
    "# nan_count_per_column = df.isna().sum()\n",
    "# print(\"\\nCount of NaNs in each column:\")\n",
    "# print(nan_count_per_column)\n",
    "\n",
    "# # Count NaNs in each row\n",
    "# nan_count_per_row = df.isna().sum(axis=1)\n",
    "# print(\"\\nCount of NaNs in each row:\")\n",
    "# print(nan_count_per_row)\n",
    "\n",
    "# # Rows with at least one NaN\n",
    "# rows_with_nans = df[df.isna().any(axis=1)]\n",
    "# print(\"\\nRows with at least one NaN:\")\n",
    "# print(rows_with_nans)\n",
    "\n",
    "# # Columns with at least one NaN\n",
    "# columns_with_nans = df.columns[df.isna().any()].tolist()\n",
    "# print(\"\\nColumns with at least one NaN:\")\n",
    "# print(columns_with_nans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train an XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid to search over\n",
    "# model = xgboost.XGBRegressor(\n",
    "#     max_depth=3,  # Maximum depth of a tree\n",
    "#     learning_rate=0.1,  # Learning rate\n",
    "#     n_estimators=100,  # Number of trees\n",
    "#     subsample=0.8,  # Subsample ratio of the training instances\n",
    "#     colsample_bytree=0.8,  # Subsample ratio of columns when constructing each tree\n",
    "#     gamma=0,  # Minimum loss reduction required to make a further partition\n",
    "#     min_child_weight=1,  # Minimum sum of instance weight needed in a child\n",
    "#     device=\"cuda\",\n",
    "#     enable_categorical=True,\n",
    "# )\n",
    "param_grid = {\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"learning_rate\": [0.1, 0.01, 0.001, 0.0001],\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "}\n",
    "models = {\n",
    "    # \"LightGBM\": (\n",
    "    #     lgb.LGBMRegressor(min_gain_to_split=0.1, force_col_wise=True),\n",
    "    #     {\n",
    "    #         \"max_depth\": [3, 5, 7],\n",
    "    #         \"n_estimators\": [50, 100, 200],\n",
    "    #         \"learning_rate\": [0.1, 0.01, 0.001, 0.0001],\n",
    "    #     },\n",
    "    # ),\n",
    "    # \"XGBoost\": (\n",
    "    #     xgb.XGBRegressor(device=\"cuda\"),\n",
    "    #     {\n",
    "    #         \"max_depth\": [3, 5, 7],\n",
    "    #         \"n_estimators\": [50, 100, 200],\n",
    "    #         \"learning_rate\": [0.1, 0.01, 0.001, 0.0001],\n",
    "    #     },\n",
    "    # ),\n",
    "    \"MLPRegressor\": (\n",
    "        MLPRegressor(max_iter=1000),\n",
    "        {\"hidden_layer_sizes\": [(50, 50), (100,)], \"learning_rate_init\": [0.001, 0.01]},\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Make custom scorer\n",
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "mse_scores = []\n",
    "kfold_bestparams = []\n",
    "allshapvalues = []\n",
    "best_models = {}\n",
    "kf = KFold(n_splits=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (model, param_grid) in models.items():\n",
    "\n",
    "    # Create a GridSearchCV object\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model, param_grid=param_grid, cv=kf, scoring=mse_scorer\n",
    "    )\n",
    "\n",
    "    # Fit the grid search to the training data\n",
    "    # grid_search.fit(X_train, y_train)\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Get the best parameters\n",
    "    # best_params = grid_search.best_params_\n",
    "    # print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "    # Create a new model with the best parameters\n",
    "    # best_model = model(**best_params)\n",
    "    # # Train the best model\n",
    "    # best_model.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Store the best model and its parameters\n",
    "    best_models[model_name] = {\n",
    "        \"best_estimator\": best_model,\n",
    "        \"best_params\": grid_search.best_params_,\n",
    "        \"best_score\": grid_search.best_score_,\n",
    "    }\n",
    "    # Predict on the test set\n",
    "    # y_pred_best = best_model.predict(X_test)\n",
    "    # mse = mean_squared_error(y_test, y_pred_best)\n",
    "\n",
    "    # Evaluate the best model\n",
    "    # accuracy_best = (y_pred_best == y_test).mean()\n",
    "\n",
    "    print(f\"MSE with Best Parameters: {best_models[model_name]['best_score']}\")\n",
    "    mse_scores.append(best_models[model_name][\"best_score\"])\n",
    "    kfold_bestparams.append(best_models[model_name][\"best_params\"])\n",
    "\n",
    "    # plt.figure()\n",
    "    explainer = shap.Explainer(model=best_model, masker=X)\n",
    "    # plt.savefig(\n",
    "    # f\"shap_explainer_best_{model_name}_fold-{i}.png\", bbox_inches=\"tight\"\n",
    "    # )\n",
    "    # plt.close()\n",
    "    # for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    #     X_train, X_test, y_train, y_test = (\n",
    "    #         X.iloc[train_index],\n",
    "    #         X.iloc[test_index],\n",
    "    #         y.iloc[train_index],\n",
    "    #         y.iloc[test_index],\n",
    "    #     )\n",
    "    shap_values = explainer(X)\n",
    "    allshapvalues.append(shap_values)\n",
    "    explainer.__class__\n",
    "\n",
    "    # plt.figure()\n",
    "    shap.summary_plot(shap_values, X, show=False)\n",
    "    # plt.savefig(\n",
    "    #     f\"shap_summary_plot_best_{model_name}_fold-{i}.png\", bbox_inches=\"tight\"\n",
    "    # )\n",
    "    # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for shap_values in allshapvalues:\n",
    "    # shap.plots.waterfall(shap_values[50], max_display=58)\n",
    "    # shap.initjs()\n",
    "    shap.plots.bar(shap_values, max_display=11)\n",
    "\n",
    "    # shap.plots.beeswarm(shap_values, max_display=58)\n",
    "    shap.plots.beeswarm(shap_values.abs, max_display=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# most_freq = max(set(kfold_bestparams.items()), key=kfold_bestparams.count)\n",
    "\n",
    "print(44.31 + 79.62)\n",
    "print(38.40+89.59)\n",
    "# for bestparams,msescore in zip(kfold_bestparams, mse_scores):\n",
    "        \n",
    "# # params = dict()\n",
    "# # params[\"device\"] = \"cuda\"\n",
    "# # params[\"tree_method\"] = \"hist\"\n",
    "\n",
    "# model = xgboost.XGBRegressor(missing=np.nan, enable_categorical=True, device=\"cuda\")\n",
    "# model.fit(X_train, y_train)\n",
    "# grid_search.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = X.select_dtypes(include=['number']).columns.tolist() \n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='mean'), numerical_features),  # Impute missing values in numerical features\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values in categorical features\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode categorical features\n",
    "        ]), categorical_features)\n",
    "    ])\n",
    "\n",
    "# pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('mlp', MLPRegressor())\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL NAME:  MLPRegressor\n",
      "Iteration 1, loss = 817510.67702937\n",
      "Iteration 2, loss = 347.08281729\n",
      "Iteration 3, loss = 120.48861486\n",
      "Iteration 4, loss = 110.30708918\n",
      "Iteration 5, loss = 101.66758825\n",
      "Iteration 6, loss = 92.49814245\n",
      "Iteration 7, loss = 84.93871339\n",
      "Iteration 8, loss = 78.30511510\n",
      "Iteration 9, loss = 73.70392730\n",
      "Iteration 10, loss = 69.85052026\n",
      "Iteration 11, loss = 63.98375008\n",
      "Iteration 12, loss = 60.89357347\n",
      "Iteration 13, loss = 58.67484654\n",
      "Iteration 14, loss = 56.90558670\n",
      "Iteration 15, loss = 56.10184154\n",
      "Iteration 16, loss = 55.01003676\n",
      "Iteration 17, loss = 54.22384378\n",
      "Iteration 18, loss = 53.64309999\n",
      "Iteration 19, loss = 53.06069917\n",
      "Iteration 20, loss = 51.37097687\n",
      "Iteration 21, loss = 50.09672472\n",
      "Iteration 22, loss = 49.29391022\n",
      "Iteration 23, loss = 48.69717909\n",
      "Iteration 24, loss = 47.81869300\n",
      "Iteration 25, loss = 47.34237399\n",
      "Iteration 26, loss = 47.02596618\n",
      "Iteration 27, loss = 46.49559832\n",
      "Iteration 28, loss = 45.93750117\n",
      "Iteration 29, loss = 45.77150130\n",
      "Iteration 30, loss = 45.10984499\n",
      "Iteration 31, loss = 45.22557745\n",
      "Iteration 32, loss = 44.28564793\n",
      "Iteration 33, loss = 44.09949133\n",
      "Iteration 34, loss = 43.83432543\n",
      "Iteration 35, loss = 43.39054127\n",
      "Iteration 36, loss = 43.57604256\n",
      "Iteration 37, loss = 43.03553103\n",
      "Iteration 38, loss = 43.14991694\n",
      "Iteration 39, loss = 42.41598365\n",
      "Iteration 40, loss = 42.41628304\n",
      "Iteration 41, loss = 43.00653092\n",
      "Iteration 42, loss = 42.09669681\n",
      "Iteration 43, loss = 42.58685089\n",
      "Iteration 44, loss = 41.66765508\n",
      "Iteration 45, loss = 41.52449669\n",
      "Iteration 46, loss = 42.44566559\n",
      "Iteration 47, loss = 42.36716065\n",
      "Iteration 48, loss = 41.22478319\n",
      "Iteration 49, loss = 41.75163581\n",
      "Iteration 50, loss = 41.94312288\n",
      "Iteration 51, loss = 40.97589064\n",
      "Iteration 52, loss = 41.14091434\n",
      "Iteration 53, loss = 40.93317720\n",
      "Iteration 54, loss = 40.39506238\n",
      "Iteration 55, loss = 41.84681384\n",
      "Iteration 56, loss = 40.14517263\n",
      "Iteration 57, loss = 40.20956958\n",
      "Iteration 58, loss = 40.20864133\n",
      "Iteration 59, loss = 41.14756953\n",
      "Iteration 60, loss = 42.88124899\n",
      "Iteration 61, loss = 40.20626242\n",
      "Iteration 62, loss = 39.83717774\n",
      "Iteration 63, loss = 40.23743553\n",
      "Iteration 64, loss = 40.56408271\n",
      "Iteration 65, loss = 39.90037410\n",
      "Iteration 66, loss = 40.18809645\n",
      "Iteration 67, loss = 39.59135024\n",
      "Iteration 68, loss = 39.45766188\n",
      "Iteration 69, loss = 40.12606440\n",
      "Iteration 70, loss = 40.60712025\n",
      "Iteration 71, loss = 40.10159899\n",
      "Iteration 72, loss = 39.74387905\n",
      "Iteration 73, loss = 39.34733269\n",
      "Iteration 74, loss = 41.48657426\n",
      "Iteration 75, loss = 40.37283309\n",
      "Iteration 76, loss = 41.35781988\n",
      "Iteration 77, loss = 38.90944843\n",
      "Iteration 78, loss = 39.36613686\n",
      "Iteration 79, loss = 38.63596140\n",
      "Iteration 80, loss = 40.26781531\n",
      "Iteration 81, loss = 40.78101627\n",
      "Iteration 82, loss = 39.13805563\n",
      "Iteration 83, loss = 39.67594884\n",
      "Iteration 84, loss = 39.90004889\n",
      "Iteration 85, loss = 38.98215147\n",
      "Iteration 86, loss = 41.49939581\n",
      "Iteration 87, loss = 39.23675013\n",
      "Iteration 88, loss = 38.96863387\n",
      "Iteration 89, loss = 38.99615689\n",
      "Iteration 90, loss = 38.79350011\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 439761.95969405\n",
      "Iteration 2, loss = 177.01534934\n",
      "Iteration 3, loss = 84.94107320\n",
      "Iteration 4, loss = 62.03749689\n",
      "Iteration 5, loss = 57.98115793\n",
      "Iteration 6, loss = 49.35054440\n",
      "Iteration 7, loss = 48.28759655\n",
      "Iteration 8, loss = 45.13775799\n",
      "Iteration 9, loss = 43.86158815\n",
      "Iteration 10, loss = 46.18051683\n",
      "Iteration 11, loss = 43.44045266\n",
      "Iteration 12, loss = 42.77329767\n",
      "Iteration 13, loss = 42.19064594\n",
      "Iteration 14, loss = 43.88596648\n",
      "Iteration 15, loss = 45.24347649\n",
      "Iteration 16, loss = 45.27549651\n",
      "Iteration 17, loss = 40.43974291\n",
      "Iteration 18, loss = 42.58733224\n",
      "Iteration 19, loss = 43.35547566\n",
      "Iteration 20, loss = 40.52569338\n",
      "Iteration 21, loss = 42.41260213\n",
      "Iteration 22, loss = 40.34769118\n",
      "Iteration 23, loss = 41.24372807\n",
      "Iteration 24, loss = 39.65670437\n",
      "Iteration 25, loss = 39.21034636\n",
      "Iteration 26, loss = 41.94015526\n",
      "Iteration 27, loss = 40.20516395\n",
      "Iteration 28, loss = 40.50618379\n",
      "Iteration 29, loss = 41.17127204\n",
      "Iteration 30, loss = 40.30370619\n",
      "Iteration 31, loss = 38.78723257\n",
      "Iteration 32, loss = 39.89880061\n",
      "Iteration 33, loss = 41.17350411\n",
      "Iteration 34, loss = 38.14765643\n",
      "Iteration 35, loss = 39.98722513\n",
      "Iteration 36, loss = 42.39171322\n",
      "Iteration 37, loss = 40.63570078\n",
      "Iteration 38, loss = 40.05066212\n",
      "Iteration 39, loss = 39.70969819\n",
      "Iteration 40, loss = 45.32067248\n",
      "Iteration 41, loss = 37.52745332\n",
      "Iteration 42, loss = 38.37045163\n",
      "Iteration 43, loss = 41.76017433\n",
      "Iteration 44, loss = 40.01901924\n",
      "Iteration 45, loss = 38.51210024\n",
      "Iteration 46, loss = 38.98134384\n",
      "Iteration 47, loss = 40.23522417\n",
      "Iteration 48, loss = 42.21422055\n",
      "Iteration 49, loss = 45.93084496\n",
      "Iteration 50, loss = 38.10599746\n",
      "Iteration 51, loss = 38.16854699\n",
      "Iteration 52, loss = 38.80511037\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 41705.21068286\n",
      "Iteration 2, loss = 263.59224734\n",
      "Iteration 3, loss = 124.53984680\n",
      "Iteration 4, loss = 95.17653739\n",
      "Iteration 5, loss = 76.24147333\n",
      "Iteration 6, loss = 62.94099488\n",
      "Iteration 7, loss = 52.76870461\n",
      "Iteration 8, loss = 54.32984880\n",
      "Iteration 9, loss = 49.88287375\n",
      "Iteration 10, loss = 48.95489224\n",
      "Iteration 11, loss = 49.69271075\n",
      "Iteration 12, loss = 47.64425836\n",
      "Iteration 13, loss = 47.65453853\n",
      "Iteration 14, loss = 49.60283424\n",
      "Iteration 15, loss = 48.14192929\n",
      "Iteration 16, loss = 47.49231794\n",
      "Iteration 17, loss = 46.65703421\n",
      "Iteration 18, loss = 46.73046515\n",
      "Iteration 19, loss = 45.54003024\n",
      "Iteration 20, loss = 45.88952002\n",
      "Iteration 21, loss = 45.61417871\n",
      "Iteration 22, loss = 45.65543204\n",
      "Iteration 23, loss = 50.23664228\n",
      "Iteration 24, loss = 46.40005716\n",
      "Iteration 25, loss = 45.52527494\n",
      "Iteration 26, loss = 56.08069397\n",
      "Iteration 27, loss = 45.22681942\n",
      "Iteration 28, loss = 45.59825056\n",
      "Iteration 29, loss = 45.48891681\n",
      "Iteration 30, loss = 44.79194364\n",
      "Iteration 31, loss = 45.08199308\n",
      "Iteration 32, loss = 44.30056828\n",
      "Iteration 33, loss = 44.10368704\n",
      "Iteration 34, loss = 44.04958975\n",
      "Iteration 35, loss = 48.08741286\n",
      "Iteration 36, loss = 46.26276614\n",
      "Iteration 37, loss = 43.06580590\n",
      "Iteration 38, loss = 44.65754234\n",
      "Iteration 39, loss = 47.79816897\n",
      "Iteration 40, loss = 46.90025351\n",
      "Iteration 41, loss = 46.20118005\n",
      "Iteration 42, loss = 44.90944792\n",
      "Iteration 43, loss = 44.26560187\n",
      "Iteration 44, loss = 43.59625731\n",
      "Iteration 45, loss = 44.38176072\n",
      "Iteration 46, loss = 45.91167949\n",
      "Iteration 47, loss = 45.95129202\n",
      "Iteration 48, loss = 46.21267852\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 275625.87479798\n",
      "Iteration 2, loss = 477.66709320\n",
      "Iteration 3, loss = 282.15878486\n",
      "Iteration 4, loss = 54.12412472\n",
      "Iteration 5, loss = 46.83706065\n",
      "Iteration 6, loss = 46.39608106\n",
      "Iteration 7, loss = 45.33296252\n",
      "Iteration 8, loss = 44.73143357\n",
      "Iteration 9, loss = 44.44685920\n",
      "Iteration 10, loss = 44.36512663\n",
      "Iteration 11, loss = 44.03207526\n",
      "Iteration 12, loss = 43.60584706\n",
      "Iteration 13, loss = 43.94271856\n",
      "Iteration 14, loss = 42.98907403\n",
      "Iteration 15, loss = 43.65097642\n",
      "Iteration 16, loss = 43.58299003\n",
      "Iteration 17, loss = 42.58488539\n",
      "Iteration 18, loss = 42.41130338\n",
      "Iteration 19, loss = 42.62626150\n",
      "Iteration 20, loss = 41.94308238\n",
      "Iteration 21, loss = 42.27656845\n",
      "Iteration 22, loss = 41.65526972\n",
      "Iteration 23, loss = 41.48266703\n",
      "Iteration 24, loss = 41.24007284\n",
      "Iteration 25, loss = 41.67137743\n",
      "Iteration 26, loss = 41.76669468\n",
      "Iteration 27, loss = 41.50706665\n",
      "Iteration 28, loss = 41.42356269\n",
      "Iteration 29, loss = 40.53621737\n",
      "Iteration 30, loss = 40.73398380\n",
      "Iteration 31, loss = 41.31292238\n",
      "Iteration 32, loss = 42.06663619\n",
      "Iteration 33, loss = 40.89616138\n",
      "Iteration 34, loss = 40.92952998\n",
      "Iteration 35, loss = 40.09678032\n",
      "Iteration 36, loss = 41.01645471\n",
      "Iteration 37, loss = 40.03616051\n",
      "Iteration 38, loss = 39.66905266\n",
      "Iteration 39, loss = 39.94466311\n",
      "Iteration 40, loss = 42.19735342\n",
      "Iteration 41, loss = 39.38509618\n",
      "Iteration 42, loss = 39.79055318\n",
      "Iteration 43, loss = 39.07390438\n",
      "Iteration 44, loss = 40.56728633\n",
      "Iteration 45, loss = 39.04483706\n",
      "Iteration 46, loss = 39.10931941\n",
      "Iteration 47, loss = 39.25472731\n",
      "Iteration 48, loss = 39.94917294\n",
      "Iteration 49, loss = 39.21659336\n",
      "Iteration 50, loss = 39.33472325\n",
      "Iteration 51, loss = 39.36894013\n",
      "Iteration 52, loss = 38.84910162\n",
      "Iteration 53, loss = 38.88607636\n",
      "Iteration 54, loss = 40.93329580\n",
      "Iteration 55, loss = 39.54846670\n",
      "Iteration 56, loss = 39.82381247\n",
      "Iteration 57, loss = 40.44681040\n",
      "Iteration 58, loss = 39.12358855\n",
      "Iteration 59, loss = 38.91838424\n",
      "Iteration 60, loss = 38.44272473\n",
      "Iteration 61, loss = 39.99857415\n",
      "Iteration 62, loss = 38.72591333\n",
      "Iteration 63, loss = 38.08063452\n",
      "Iteration 64, loss = 41.19957368\n",
      "Iteration 65, loss = 39.56799633\n",
      "Iteration 66, loss = 38.27302555\n",
      "Iteration 67, loss = 39.22077665\n",
      "Iteration 68, loss = 37.84423689\n",
      "Iteration 69, loss = 43.71843044\n",
      "Iteration 70, loss = 38.88774823\n",
      "Iteration 71, loss = 40.47190753\n",
      "Iteration 72, loss = 43.44274477\n",
      "Iteration 73, loss = 44.36716682\n",
      "Iteration 74, loss = 39.20678927\n",
      "Iteration 75, loss = 40.91706769\n",
      "Iteration 76, loss = 41.34434451\n",
      "Iteration 77, loss = 40.44738543\n",
      "Iteration 78, loss = 40.18928923\n",
      "Iteration 79, loss = 45.08212091\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2555.35802906\n",
      "Iteration 2, loss = 74.93065005\n",
      "Iteration 3, loss = 60.39962618\n",
      "Iteration 4, loss = 52.27070583\n",
      "Iteration 5, loss = 49.08952008\n",
      "Iteration 6, loss = 46.40882567\n",
      "Iteration 7, loss = 44.75039029\n",
      "Iteration 8, loss = 44.76963971\n",
      "Iteration 9, loss = 44.82211396\n",
      "Iteration 10, loss = 44.98856560\n",
      "Iteration 11, loss = 43.63978978\n",
      "Iteration 12, loss = 43.85968949\n",
      "Iteration 13, loss = 43.71161959\n",
      "Iteration 14, loss = 45.21262214\n",
      "Iteration 15, loss = 45.63307467\n",
      "Iteration 16, loss = 48.98159271\n",
      "Iteration 17, loss = 45.95052749\n",
      "Iteration 18, loss = 53.28986334\n",
      "Iteration 19, loss = 47.11147313\n",
      "Iteration 20, loss = 53.24484839\n",
      "Iteration 21, loss = 57.92090601\n",
      "Iteration 22, loss = 61.02423895\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 465713.76239016\n",
      "Iteration 2, loss = 85.96258170\n",
      "Iteration 3, loss = 57.36822769\n",
      "Iteration 4, loss = 56.16810046\n",
      "Iteration 5, loss = 52.35331335\n",
      "Iteration 6, loss = 53.61692367\n",
      "Iteration 7, loss = 53.66689222\n",
      "Iteration 8, loss = 52.84126286\n",
      "Iteration 9, loss = 54.02019471\n",
      "Iteration 10, loss = 60.92364312\n",
      "Iteration 11, loss = 54.24094523\n",
      "Iteration 12, loss = 54.78477150\n",
      "Iteration 13, loss = 51.59118644\n",
      "Iteration 14, loss = 52.27996625\n",
      "Iteration 15, loss = 53.92195677\n",
      "Iteration 16, loss = 52.24310486\n",
      "Iteration 17, loss = 52.37055368\n",
      "Iteration 18, loss = 48.95983294\n",
      "Iteration 19, loss = 50.42438606\n",
      "Iteration 20, loss = 51.25241318\n",
      "Iteration 21, loss = 50.74448322\n",
      "Iteration 22, loss = 53.81622773\n",
      "Iteration 23, loss = 52.02799875\n",
      "Iteration 24, loss = 47.62724820\n",
      "Iteration 25, loss = 48.76576643\n",
      "Iteration 26, loss = 54.24054360\n",
      "Iteration 27, loss = 50.78194447\n",
      "Iteration 28, loss = 49.44132139\n",
      "Iteration 29, loss = 47.99663397\n",
      "Iteration 30, loss = 49.17457406\n",
      "Iteration 31, loss = 52.55264098\n",
      "Iteration 32, loss = 53.81711460\n",
      "Iteration 33, loss = 49.87382786\n",
      "Iteration 34, loss = 56.31312328\n",
      "Iteration 35, loss = 48.16097290\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 592733.50225527\n",
      "Iteration 2, loss = 121.66745284\n",
      "Iteration 3, loss = 74.61244506\n",
      "Iteration 4, loss = 69.04241161\n",
      "Iteration 5, loss = 63.44790731\n",
      "Iteration 6, loss = 59.17806129\n",
      "Iteration 7, loss = 54.38436496\n",
      "Iteration 8, loss = 51.10186410\n",
      "Iteration 9, loss = 48.32691903\n",
      "Iteration 10, loss = 45.87422289\n",
      "Iteration 11, loss = 44.57667455\n",
      "Iteration 12, loss = 44.24328744\n",
      "Iteration 13, loss = 45.72908584\n",
      "Iteration 14, loss = 41.81590677\n",
      "Iteration 15, loss = 42.49146329\n",
      "Iteration 16, loss = 41.79416949\n",
      "Iteration 17, loss = 42.52306514\n",
      "Iteration 18, loss = 41.72953365\n",
      "Iteration 19, loss = 42.02986981\n",
      "Iteration 20, loss = 40.85733092\n",
      "Iteration 21, loss = 40.99666447\n",
      "Iteration 22, loss = 40.71903598\n",
      "Iteration 23, loss = 40.42401245\n",
      "Iteration 24, loss = 40.04294681\n",
      "Iteration 25, loss = 40.44005256\n",
      "Iteration 26, loss = 40.15495134\n",
      "Iteration 27, loss = 42.30677900\n",
      "Iteration 28, loss = 39.76500706\n",
      "Iteration 29, loss = 41.16456647\n",
      "Iteration 30, loss = 40.70414642\n",
      "Iteration 31, loss = 40.95342283\n",
      "Iteration 32, loss = 41.23832862\n",
      "Iteration 33, loss = 43.31008520\n",
      "Iteration 34, loss = 40.02385839\n",
      "Iteration 35, loss = 41.51206378\n",
      "Iteration 36, loss = 40.25453959\n",
      "Iteration 37, loss = 38.54521980\n",
      "Iteration 38, loss = 40.41554234\n",
      "Iteration 39, loss = 40.13800570\n",
      "Iteration 40, loss = 40.41107130\n",
      "Iteration 41, loss = 42.28402783\n",
      "Iteration 42, loss = 45.46928230\n",
      "Iteration 43, loss = 43.94017944\n",
      "Iteration 44, loss = 48.91334858\n",
      "Iteration 45, loss = 44.98786916\n",
      "Iteration 46, loss = 46.98458400\n",
      "Iteration 47, loss = 52.15211491\n",
      "Iteration 48, loss = 50.22772444\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 166446.37380067\n",
      "Iteration 2, loss = 201.61095715\n",
      "Iteration 3, loss = 99.53161281\n",
      "Iteration 4, loss = 63.33590083\n",
      "Iteration 5, loss = 54.91440680\n",
      "Iteration 6, loss = 52.81599896\n",
      "Iteration 7, loss = 51.91695175\n",
      "Iteration 8, loss = 51.95152327\n",
      "Iteration 9, loss = 50.74441304\n",
      "Iteration 10, loss = 50.10337161\n",
      "Iteration 11, loss = 49.65263538\n",
      "Iteration 12, loss = 49.06165827\n",
      "Iteration 13, loss = 50.88544735\n",
      "Iteration 14, loss = 52.49419715\n",
      "Iteration 15, loss = 48.91883395\n",
      "Iteration 16, loss = 49.73034507\n",
      "Iteration 17, loss = 49.81515637\n",
      "Iteration 18, loss = 52.99110768\n",
      "Iteration 19, loss = 53.11128735\n",
      "Iteration 20, loss = 48.06583957\n",
      "Iteration 21, loss = 49.02927565\n",
      "Iteration 22, loss = 52.35240230\n",
      "Iteration 23, loss = 49.17336284\n",
      "Iteration 24, loss = 48.27347378\n",
      "Iteration 25, loss = 47.24724239\n",
      "Iteration 26, loss = 48.71977629\n",
      "Iteration 27, loss = 51.01144497\n",
      "Iteration 28, loss = 49.90145493\n",
      "Iteration 29, loss = 48.61378361\n",
      "Iteration 30, loss = 52.18335333\n",
      "Iteration 31, loss = 60.67905654\n",
      "Iteration 32, loss = 58.05668974\n",
      "Iteration 33, loss = 51.62689226\n",
      "Iteration 34, loss = 57.42547820\n",
      "Iteration 35, loss = 53.05885662\n",
      "Iteration 36, loss = 82.30491377\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 459552.32259095\n",
      "Iteration 2, loss = 79.80822752\n",
      "Iteration 3, loss = 49.57200525\n",
      "Iteration 4, loss = 51.47168220\n",
      "Iteration 5, loss = 67.58247906\n",
      "Iteration 6, loss = 46.31940318\n",
      "Iteration 7, loss = 44.72109334\n",
      "Iteration 8, loss = 45.30280025\n",
      "Iteration 9, loss = 47.16966597\n",
      "Iteration 10, loss = 51.32503535\n",
      "Iteration 11, loss = 47.60274083\n",
      "Iteration 12, loss = 45.25106582\n",
      "Iteration 13, loss = 46.36830382\n",
      "Iteration 14, loss = 58.88834585\n",
      "Iteration 15, loss = 42.99874687\n",
      "Iteration 16, loss = 48.23439511\n",
      "Iteration 17, loss = 53.52856089\n",
      "Iteration 18, loss = 44.65939276\n",
      "Iteration 19, loss = 41.41572694\n",
      "Iteration 20, loss = 41.07371438\n",
      "Iteration 21, loss = 42.42726743\n",
      "Iteration 22, loss = 45.49393764\n",
      "Iteration 23, loss = 44.52083042\n",
      "Iteration 24, loss = 43.31356858\n",
      "Iteration 25, loss = 47.02949516\n",
      "Iteration 26, loss = 48.22503641\n",
      "Iteration 27, loss = 47.77927042\n",
      "Iteration 28, loss = 56.77963659\n",
      "Iteration 29, loss = 47.49161229\n",
      "Iteration 30, loss = 47.51370910\n",
      "Iteration 31, loss = 46.51163505\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 144101.37016145\n",
      "Iteration 2, loss = 65.82071243\n",
      "Iteration 3, loss = 53.33344030\n",
      "Iteration 4, loss = 48.46531372\n",
      "Iteration 5, loss = 46.54660426\n",
      "Iteration 6, loss = 45.54436378\n",
      "Iteration 7, loss = 47.77696436\n",
      "Iteration 8, loss = 43.37732152\n",
      "Iteration 9, loss = 43.09049819\n",
      "Iteration 10, loss = 44.57345585\n",
      "Iteration 11, loss = 44.28346320\n",
      "Iteration 12, loss = 42.77957010\n",
      "Iteration 13, loss = 45.70476317\n",
      "Iteration 14, loss = 46.17563429\n",
      "Iteration 15, loss = 43.68559434\n",
      "Iteration 16, loss = 41.66911858\n",
      "Iteration 17, loss = 44.17414605\n",
      "Iteration 18, loss = 43.17150368\n",
      "Iteration 19, loss = 45.47853825\n",
      "Iteration 20, loss = 46.51089455\n",
      "Iteration 21, loss = 44.27171282\n",
      "Iteration 22, loss = 43.56755865\n",
      "Iteration 23, loss = 57.45498776\n",
      "Iteration 24, loss = 49.43069286\n",
      "Iteration 25, loss = 41.98083012\n",
      "Iteration 26, loss = 44.81197737\n",
      "Iteration 27, loss = 45.18457229\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 264548.97725830\n",
      "Iteration 2, loss = 281.73316369\n",
      "Iteration 3, loss = 152.21665913\n",
      "Iteration 4, loss = 143.89338726\n",
      "Iteration 5, loss = 135.84892145\n",
      "Iteration 6, loss = 127.36707491\n",
      "Iteration 7, loss = 119.55038536\n",
      "Iteration 8, loss = 111.95695778\n",
      "Iteration 9, loss = 104.76495585\n",
      "Iteration 10, loss = 97.94285735\n",
      "Iteration 11, loss = 87.67977439\n",
      "Iteration 12, loss = 64.02721610\n",
      "Iteration 13, loss = 52.03504073\n",
      "Iteration 14, loss = 49.77739498\n",
      "Iteration 15, loss = 48.39987458\n",
      "Iteration 16, loss = 47.71368730\n",
      "Iteration 17, loss = 47.14245212\n",
      "Iteration 18, loss = 46.51400725\n",
      "Iteration 19, loss = 45.98473906\n",
      "Iteration 20, loss = 45.12162707\n",
      "Iteration 21, loss = 44.77826253\n",
      "Iteration 22, loss = 45.19509293\n",
      "Iteration 23, loss = 44.56196765\n",
      "Iteration 24, loss = 45.24262222\n",
      "Iteration 25, loss = 44.36890571\n",
      "Iteration 26, loss = 44.20016215\n",
      "Iteration 27, loss = 43.85151413\n",
      "Iteration 28, loss = 44.18872317\n",
      "Iteration 29, loss = 43.00105734\n",
      "Iteration 30, loss = 42.67942158\n",
      "Iteration 31, loss = 42.48451206\n",
      "Iteration 32, loss = 43.18558904\n",
      "Iteration 33, loss = 42.48391164\n",
      "Iteration 34, loss = 42.42248586\n",
      "Iteration 35, loss = 42.65668515\n",
      "Iteration 36, loss = 42.37255450\n",
      "Iteration 37, loss = 42.32249369\n",
      "Iteration 38, loss = 41.38228186\n",
      "Iteration 39, loss = 42.63657144\n",
      "Iteration 40, loss = 41.56248875\n",
      "Iteration 41, loss = 41.51225338\n",
      "Iteration 42, loss = 41.12949224\n",
      "Iteration 43, loss = 41.26651630\n",
      "Iteration 44, loss = 40.94345823\n",
      "Iteration 45, loss = 40.77284547\n",
      "Iteration 46, loss = 41.47333284\n",
      "Iteration 47, loss = 40.66202971\n",
      "Iteration 48, loss = 40.56299881\n",
      "Iteration 49, loss = 41.40980199\n",
      "Iteration 50, loss = 40.24532141\n",
      "Iteration 51, loss = 40.28472466\n",
      "Iteration 52, loss = 40.35784167\n",
      "Iteration 53, loss = 41.44406670\n",
      "Iteration 54, loss = 42.34661271\n",
      "Iteration 55, loss = 40.98689713\n",
      "Iteration 56, loss = 39.65282231\n",
      "Iteration 57, loss = 39.67597722\n",
      "Iteration 58, loss = 39.85670238\n",
      "Iteration 59, loss = 40.27469551\n",
      "Iteration 60, loss = 40.84464517\n",
      "Iteration 61, loss = 40.34302692\n",
      "Iteration 62, loss = 40.77226098\n",
      "Iteration 63, loss = 40.40451604\n",
      "Iteration 64, loss = 41.19442042\n",
      "Iteration 65, loss = 41.79518368\n",
      "Iteration 66, loss = 41.24110075\n",
      "Iteration 67, loss = 39.87258011\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 371927.76193521\n",
      "Iteration 2, loss = 626.45500451\n",
      "Iteration 3, loss = 63.27168016\n",
      "Iteration 4, loss = 50.17578950\n",
      "Iteration 5, loss = 45.48491118\n",
      "Iteration 6, loss = 44.74182744\n",
      "Iteration 7, loss = 43.07849613\n",
      "Iteration 8, loss = 42.81562779\n",
      "Iteration 9, loss = 42.45226219\n",
      "Iteration 10, loss = 42.32844195\n",
      "Iteration 11, loss = 41.64010855\n",
      "Iteration 12, loss = 42.17586023\n",
      "Iteration 13, loss = 41.49832050\n",
      "Iteration 14, loss = 41.88995478\n",
      "Iteration 15, loss = 41.32769791\n",
      "Iteration 16, loss = 40.20546300\n",
      "Iteration 17, loss = 41.44813648\n",
      "Iteration 18, loss = 40.87646778\n",
      "Iteration 19, loss = 40.02503940\n",
      "Iteration 20, loss = 41.47459451\n",
      "Iteration 21, loss = 41.46768982\n",
      "Iteration 22, loss = 39.74542356\n",
      "Iteration 23, loss = 40.20713136\n",
      "Iteration 24, loss = 40.75946219\n",
      "Iteration 25, loss = 39.39736255\n",
      "Iteration 26, loss = 39.52202277\n",
      "Iteration 27, loss = 40.37149799\n",
      "Iteration 28, loss = 38.39195916\n",
      "Iteration 29, loss = 39.21109256\n",
      "Iteration 30, loss = 38.93735040\n",
      "Iteration 31, loss = 39.11811091\n",
      "Iteration 32, loss = 38.56799163\n",
      "Iteration 33, loss = 38.27935308\n",
      "Iteration 34, loss = 38.82661843\n",
      "Iteration 35, loss = 39.04136466\n",
      "Iteration 36, loss = 38.89467942\n",
      "Iteration 37, loss = 37.95040816\n",
      "Iteration 38, loss = 37.75938231\n",
      "Iteration 39, loss = 40.57377360\n",
      "Iteration 40, loss = 39.17321778\n",
      "Iteration 41, loss = 38.54733295\n",
      "Iteration 42, loss = 37.32207448\n",
      "Iteration 43, loss = 37.44752532\n",
      "Iteration 44, loss = 39.06457360\n",
      "Iteration 45, loss = 38.15251256\n",
      "Iteration 46, loss = 37.59801771\n",
      "Iteration 47, loss = 36.51375429\n",
      "Iteration 48, loss = 38.58805923\n",
      "Iteration 49, loss = 37.65660189\n",
      "Iteration 50, loss = 37.83917626\n",
      "Iteration 51, loss = 36.42678965\n",
      "Iteration 52, loss = 37.48403044\n",
      "Iteration 53, loss = 39.36407880\n",
      "Iteration 54, loss = 37.93759649\n",
      "Iteration 55, loss = 38.02130207\n",
      "Iteration 56, loss = 36.16422397\n",
      "Iteration 57, loss = 37.55285048\n",
      "Iteration 58, loss = 40.69844940\n",
      "Iteration 59, loss = 38.22469122\n",
      "Iteration 60, loss = 37.55505099\n",
      "Iteration 61, loss = 36.19289306\n",
      "Iteration 62, loss = 37.66425570\n",
      "Iteration 63, loss = 36.28377903\n",
      "Iteration 64, loss = 38.00317060\n",
      "Iteration 65, loss = 37.29636117\n",
      "Iteration 66, loss = 39.61745177\n",
      "Iteration 67, loss = 41.68745712\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 235263.25769709\n",
      "Iteration 2, loss = 290.08569496\n",
      "Iteration 3, loss = 72.32380915\n",
      "Iteration 4, loss = 58.94483904\n",
      "Iteration 5, loss = 53.47005207\n",
      "Iteration 6, loss = 52.56255827\n",
      "Iteration 7, loss = 51.12237914\n",
      "Iteration 8, loss = 50.98690469\n",
      "Iteration 9, loss = 52.60177693\n",
      "Iteration 10, loss = 49.84528726\n",
      "Iteration 11, loss = 49.23251795\n",
      "Iteration 12, loss = 49.18607604\n",
      "Iteration 13, loss = 49.02961730\n",
      "Iteration 14, loss = 48.27027437\n",
      "Iteration 15, loss = 49.34973369\n",
      "Iteration 16, loss = 47.32281316\n",
      "Iteration 17, loss = 46.14849230\n",
      "Iteration 18, loss = 46.51530182\n",
      "Iteration 19, loss = 46.00192259\n",
      "Iteration 20, loss = 45.66554634\n",
      "Iteration 21, loss = 45.51843133\n",
      "Iteration 22, loss = 45.11026413\n",
      "Iteration 23, loss = 46.85461873\n",
      "Iteration 24, loss = 44.84327657\n",
      "Iteration 25, loss = 45.00365308\n",
      "Iteration 26, loss = 43.97942460\n",
      "Iteration 27, loss = 45.36409244\n",
      "Iteration 28, loss = 48.27454797\n",
      "Iteration 29, loss = 43.56497002\n",
      "Iteration 30, loss = 43.48085247\n",
      "Iteration 31, loss = 43.89693629\n",
      "Iteration 32, loss = 46.79518253\n",
      "Iteration 33, loss = 43.70450239\n",
      "Iteration 34, loss = 43.98551991\n",
      "Iteration 35, loss = 44.08352304\n",
      "Iteration 36, loss = 42.62749185\n",
      "Iteration 37, loss = 43.40441670\n",
      "Iteration 38, loss = 44.89604177\n",
      "Iteration 39, loss = 41.85636200\n",
      "Iteration 40, loss = 42.54884998\n",
      "Iteration 41, loss = 43.51890382\n",
      "Iteration 42, loss = 42.48468384\n",
      "Iteration 43, loss = 41.82642391\n",
      "Iteration 44, loss = 41.97363131\n",
      "Iteration 45, loss = 43.24617765\n",
      "Iteration 46, loss = 41.71832672\n",
      "Iteration 47, loss = 40.60812261\n",
      "Iteration 48, loss = 47.12962632\n",
      "Iteration 49, loss = 41.15344106\n",
      "Iteration 50, loss = 40.65457059\n",
      "Iteration 51, loss = 44.70767947\n",
      "Iteration 52, loss = 41.63811140\n",
      "Iteration 53, loss = 42.70612659\n",
      "Iteration 54, loss = 40.28752182\n",
      "Iteration 55, loss = 45.80488277\n",
      "Iteration 56, loss = 40.50063082\n",
      "Iteration 57, loss = 40.88650434\n",
      "Iteration 58, loss = 41.13749707\n",
      "Iteration 59, loss = 40.71346133\n",
      "Iteration 60, loss = 40.64249974\n",
      "Iteration 61, loss = 41.27428927\n",
      "Iteration 62, loss = 44.73863562\n",
      "Iteration 63, loss = 42.67172831\n",
      "Iteration 64, loss = 42.60743364\n",
      "Iteration 65, loss = 45.92635226\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 44150.37163110\n",
      "Iteration 2, loss = 165.15994627\n",
      "Iteration 3, loss = 53.03153092\n",
      "Iteration 4, loss = 47.56611925\n",
      "Iteration 5, loss = 45.25025636\n",
      "Iteration 6, loss = 44.46194075\n",
      "Iteration 7, loss = 44.07621327\n",
      "Iteration 8, loss = 43.20633210\n",
      "Iteration 9, loss = 43.02964554\n",
      "Iteration 10, loss = 42.42072840\n",
      "Iteration 11, loss = 41.72455596\n",
      "Iteration 12, loss = 42.04093195\n",
      "Iteration 13, loss = 41.22583680\n",
      "Iteration 14, loss = 41.07989129\n",
      "Iteration 15, loss = 41.50922606\n",
      "Iteration 16, loss = 40.39226792\n",
      "Iteration 17, loss = 40.35194482\n",
      "Iteration 18, loss = 41.08756085\n",
      "Iteration 19, loss = 40.07887755\n",
      "Iteration 20, loss = 41.55989422\n",
      "Iteration 21, loss = 40.08998629\n",
      "Iteration 22, loss = 40.16949198\n",
      "Iteration 23, loss = 40.45848660\n",
      "Iteration 24, loss = 39.04280884\n",
      "Iteration 25, loss = 40.07032390\n",
      "Iteration 26, loss = 39.67898871\n",
      "Iteration 27, loss = 39.10194230\n",
      "Iteration 28, loss = 39.00529957\n",
      "Iteration 29, loss = 39.13228076\n",
      "Iteration 30, loss = 39.25114382\n",
      "Iteration 31, loss = 38.36152220\n",
      "Iteration 32, loss = 37.91523211\n",
      "Iteration 33, loss = 37.98927993\n",
      "Iteration 34, loss = 37.81000467\n",
      "Iteration 35, loss = 37.65345492\n",
      "Iteration 36, loss = 38.65057421\n",
      "Iteration 37, loss = 39.29555028\n",
      "Iteration 38, loss = 38.29454858\n",
      "Iteration 39, loss = 38.75171153\n",
      "Iteration 40, loss = 38.04861120\n",
      "Iteration 41, loss = 37.06405388\n",
      "Iteration 42, loss = 37.79516919\n",
      "Iteration 43, loss = 39.04194476\n",
      "Iteration 44, loss = 36.30988499\n",
      "Iteration 45, loss = 36.71096533\n",
      "Iteration 46, loss = 41.47481113\n",
      "Iteration 47, loss = 37.03501488\n",
      "Iteration 48, loss = 38.75729775\n",
      "Iteration 49, loss = 40.34162369\n",
      "Iteration 50, loss = 40.33045806\n",
      "Iteration 51, loss = 39.98609334\n",
      "Iteration 52, loss = 38.53954349\n",
      "Iteration 53, loss = 37.80069481\n",
      "Iteration 54, loss = 37.91595878\n",
      "Iteration 55, loss = 42.52277673\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 32540.65599526\n",
      "Iteration 2, loss = 161.86863733\n",
      "Iteration 3, loss = 81.27872748\n",
      "Iteration 4, loss = 60.40590227\n",
      "Iteration 5, loss = 52.48125623\n",
      "Iteration 6, loss = 48.98699604\n",
      "Iteration 7, loss = 46.07521275\n",
      "Iteration 8, loss = 44.54062651\n",
      "Iteration 9, loss = 44.91838877\n",
      "Iteration 10, loss = 42.99998462\n",
      "Iteration 11, loss = 41.93861452\n",
      "Iteration 12, loss = 41.47647935\n",
      "Iteration 13, loss = 42.76675011\n",
      "Iteration 14, loss = 41.21763680\n",
      "Iteration 15, loss = 41.97919917\n",
      "Iteration 16, loss = 41.53844189\n",
      "Iteration 17, loss = 40.30010383\n",
      "Iteration 18, loss = 40.29892048\n",
      "Iteration 19, loss = 40.75495011\n",
      "Iteration 20, loss = 39.18257944\n",
      "Iteration 21, loss = 40.38618849\n",
      "Iteration 22, loss = 40.63906098\n",
      "Iteration 23, loss = 39.35127555\n",
      "Iteration 24, loss = 40.34894743\n",
      "Iteration 25, loss = 39.79642546\n",
      "Iteration 26, loss = 38.74730501\n",
      "Iteration 27, loss = 39.77094429\n",
      "Iteration 28, loss = 38.90253092\n",
      "Iteration 29, loss = 38.81286126\n",
      "Iteration 30, loss = 39.82208557\n",
      "Iteration 31, loss = 39.88257974\n",
      "Iteration 32, loss = 39.46651506\n",
      "Iteration 33, loss = 38.95180456\n",
      "Iteration 34, loss = 37.36660500\n",
      "Iteration 35, loss = 41.23213853\n",
      "Iteration 36, loss = 37.38490073\n",
      "Iteration 37, loss = 38.19074945\n",
      "Iteration 38, loss = 38.77212566\n",
      "Iteration 39, loss = 40.25200680\n",
      "Iteration 40, loss = 38.20466775\n",
      "Iteration 41, loss = 40.05402498\n",
      "Iteration 42, loss = 38.69364624\n",
      "Iteration 43, loss = 39.30638861\n",
      "Iteration 44, loss = 39.20836563\n",
      "Iteration 45, loss = 41.87441499\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 848149.48326504\n",
      "Iteration 2, loss = 137.08404351\n",
      "Iteration 3, loss = 56.71241762\n",
      "Iteration 4, loss = 53.01682204\n",
      "Iteration 5, loss = 51.04600992\n",
      "Iteration 6, loss = 49.06816104\n",
      "Iteration 7, loss = 48.24285106\n",
      "Iteration 8, loss = 46.72915659\n",
      "Iteration 9, loss = 46.85555867\n",
      "Iteration 10, loss = 46.22251564\n",
      "Iteration 11, loss = 44.86389013\n",
      "Iteration 12, loss = 44.31602576\n",
      "Iteration 13, loss = 44.76873455\n",
      "Iteration 14, loss = 44.23600036\n",
      "Iteration 15, loss = 44.12168584\n",
      "Iteration 16, loss = 43.46321227\n",
      "Iteration 17, loss = 44.23327394\n",
      "Iteration 18, loss = 43.50654864\n",
      "Iteration 19, loss = 44.12828024\n",
      "Iteration 20, loss = 43.61715702\n",
      "Iteration 21, loss = 41.51597123\n",
      "Iteration 22, loss = 42.19992993\n",
      "Iteration 23, loss = 44.35619261\n",
      "Iteration 24, loss = 42.50994478\n",
      "Iteration 25, loss = 42.05984535\n",
      "Iteration 26, loss = 41.58911946\n",
      "Iteration 27, loss = 43.72210678\n",
      "Iteration 28, loss = 41.28828834\n",
      "Iteration 29, loss = 41.49713206\n",
      "Iteration 30, loss = 52.32650667\n",
      "Iteration 31, loss = 44.28899597\n",
      "Iteration 32, loss = 44.64622284\n",
      "Iteration 33, loss = 45.09481179\n",
      "Iteration 34, loss = 43.59017831\n",
      "Iteration 35, loss = 50.43250303\n",
      "Iteration 36, loss = 42.05550240\n",
      "Iteration 37, loss = 46.85622418\n",
      "Iteration 38, loss = 47.13480016\n",
      "Iteration 39, loss = 44.13135850\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 419026.60646429\n",
      "Iteration 2, loss = 156.59028932\n",
      "Iteration 3, loss = 69.48940050\n",
      "Iteration 4, loss = 55.28720662\n",
      "Iteration 5, loss = 48.76624954\n",
      "Iteration 6, loss = 45.77615692\n",
      "Iteration 7, loss = 44.50053735\n",
      "Iteration 8, loss = 43.35771002\n",
      "Iteration 9, loss = 42.98898986\n",
      "Iteration 10, loss = 42.26778904\n",
      "Iteration 11, loss = 41.39004959\n",
      "Iteration 12, loss = 40.91033028\n",
      "Iteration 13, loss = 41.04188671\n",
      "Iteration 14, loss = 40.82604617\n",
      "Iteration 15, loss = 39.65130806\n",
      "Iteration 16, loss = 39.43667120\n",
      "Iteration 17, loss = 40.23048755\n",
      "Iteration 18, loss = 39.31936633\n",
      "Iteration 19, loss = 38.72067011\n",
      "Iteration 20, loss = 40.36157011\n",
      "Iteration 21, loss = 39.99732020\n",
      "Iteration 22, loss = 40.27311044\n",
      "Iteration 23, loss = 39.47331976\n",
      "Iteration 24, loss = 40.54822717\n",
      "Iteration 25, loss = 40.14598768\n",
      "Iteration 26, loss = 41.07249503\n",
      "Iteration 27, loss = 39.26428572\n",
      "Iteration 28, loss = 40.03696399\n",
      "Iteration 29, loss = 40.51061300\n",
      "Iteration 30, loss = 40.02323209\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 140054.19164096\n",
      "Iteration 2, loss = 68.60113342\n",
      "Iteration 3, loss = 50.58579257\n",
      "Iteration 4, loss = 50.86180507\n",
      "Iteration 5, loss = 49.11739510\n",
      "Iteration 6, loss = 49.35187891\n",
      "Iteration 7, loss = 50.78096403\n",
      "Iteration 8, loss = 50.54802926\n",
      "Iteration 9, loss = 59.84991255\n",
      "Iteration 10, loss = 52.04571761\n",
      "Iteration 11, loss = 49.59118238\n",
      "Iteration 12, loss = 58.53281740\n",
      "Iteration 13, loss = 59.48681872\n",
      "Iteration 14, loss = 59.43883064\n",
      "Iteration 15, loss = 68.24154357\n",
      "Iteration 16, loss = 63.40988880\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 178750.18632713\n",
      "Iteration 2, loss = 93.42799945\n",
      "Iteration 3, loss = 66.16397990\n",
      "Iteration 4, loss = 52.41684681\n",
      "Iteration 5, loss = 49.19767562\n",
      "Iteration 6, loss = 49.05776477\n",
      "Iteration 7, loss = 48.67199506\n",
      "Iteration 8, loss = 46.87379889\n",
      "Iteration 9, loss = 45.45612390\n",
      "Iteration 10, loss = 43.71369114\n",
      "Iteration 11, loss = 43.70737005\n",
      "Iteration 12, loss = 45.35311902\n",
      "Iteration 13, loss = 45.54487583\n",
      "Iteration 14, loss = 42.93485999\n",
      "Iteration 15, loss = 46.46236817\n",
      "Iteration 16, loss = 47.21542634\n",
      "Iteration 17, loss = 42.39322708\n",
      "Iteration 18, loss = 41.64520287\n",
      "Iteration 19, loss = 47.15949954\n",
      "Iteration 20, loss = 51.23234549\n",
      "Iteration 21, loss = 53.51490785\n",
      "Iteration 22, loss = 59.01403996\n",
      "Iteration 23, loss = 83.53910074\n",
      "Iteration 24, loss = 72.29831126\n",
      "Iteration 25, loss = 9784.66276962\n",
      "Iteration 26, loss = 62.62960593\n",
      "Iteration 27, loss = 44.56110777\n",
      "Iteration 28, loss = 78.90674797\n",
      "Iteration 29, loss = 77.66670410\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 241599.31113006\n",
      "Iteration 2, loss = 88.98644769\n",
      "Iteration 3, loss = 48.67144552\n",
      "Iteration 4, loss = 45.56022411\n",
      "Iteration 5, loss = 44.42618130\n",
      "Iteration 6, loss = 41.47754961\n",
      "Iteration 7, loss = 42.38199647\n",
      "Iteration 8, loss = 43.54105667\n",
      "Iteration 9, loss = 45.48481355\n",
      "Iteration 10, loss = 44.69521763\n",
      "Iteration 11, loss = 41.70843355\n",
      "Iteration 12, loss = 44.33042268\n",
      "Iteration 13, loss = 40.81933850\n",
      "Iteration 14, loss = 44.03327652\n",
      "Iteration 15, loss = 44.13704757\n",
      "Iteration 16, loss = 43.32997743\n",
      "Iteration 17, loss = 46.57222569\n",
      "Iteration 18, loss = 45.18676800\n",
      "Iteration 19, loss = 44.79369946\n",
      "Iteration 20, loss = 65.60305526\n",
      "Iteration 21, loss = 48.27450384\n",
      "Iteration 22, loss = 44.89433734\n",
      "Iteration 23, loss = 51.36007560\n",
      "Iteration 24, loss = 55.78611454\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2936639.38448522\n",
      "Iteration 2, loss = 1484.73640337\n",
      "Iteration 3, loss = 1155.64429239\n",
      "Iteration 4, loss = 997.12443516\n",
      "Iteration 5, loss = 789.25964892\n",
      "Iteration 6, loss = 279.03404014\n",
      "Iteration 7, loss = 124.27571950\n",
      "Iteration 8, loss = 89.57582487\n",
      "Iteration 9, loss = 81.79308994\n",
      "Iteration 10, loss = 78.79019510\n",
      "Iteration 11, loss = 74.16329505\n",
      "Iteration 12, loss = 71.26434109\n",
      "Iteration 13, loss = 68.07983648\n",
      "Iteration 14, loss = 65.32479643\n",
      "Iteration 15, loss = 62.77572767\n",
      "Iteration 16, loss = 59.63028673\n",
      "Iteration 17, loss = 53.95647561\n",
      "Iteration 18, loss = 50.24807172\n",
      "Iteration 19, loss = 48.24333986\n",
      "Iteration 20, loss = 46.52263403\n",
      "Iteration 21, loss = 45.98200291\n",
      "Iteration 22, loss = 45.87804929\n",
      "Iteration 23, loss = 44.38976184\n",
      "Iteration 24, loss = 45.24652659\n",
      "Iteration 25, loss = 43.70701132\n",
      "Iteration 26, loss = 43.99107117\n",
      "Iteration 27, loss = 43.24214862\n",
      "Iteration 28, loss = 42.17451542\n",
      "Iteration 29, loss = 42.79423678\n",
      "Iteration 30, loss = 43.13183896\n",
      "Iteration 31, loss = 41.93698307\n",
      "Iteration 32, loss = 42.18710309\n",
      "Iteration 33, loss = 42.17203246\n",
      "Iteration 34, loss = 41.86451151\n",
      "Iteration 35, loss = 41.24300725\n",
      "Iteration 36, loss = 42.20101523\n",
      "Iteration 37, loss = 40.38314515\n",
      "Iteration 38, loss = 41.35960196\n",
      "Iteration 39, loss = 40.65988693\n",
      "Iteration 40, loss = 40.93885365\n",
      "Iteration 41, loss = 40.82216296\n",
      "Iteration 42, loss = 40.17423273\n",
      "Iteration 43, loss = 41.44951217\n",
      "Iteration 44, loss = 40.46717928\n",
      "Iteration 45, loss = 39.86661422\n",
      "Iteration 46, loss = 40.00834977\n",
      "Iteration 47, loss = 40.10810644\n",
      "Iteration 48, loss = 40.11538616\n",
      "Iteration 49, loss = 40.22951198\n",
      "Iteration 50, loss = 39.54403302\n",
      "Iteration 51, loss = 40.10583888\n",
      "Iteration 52, loss = 39.26795195\n",
      "Iteration 53, loss = 39.55091552\n",
      "Iteration 54, loss = 39.21455166\n",
      "Iteration 55, loss = 38.68618477\n",
      "Iteration 56, loss = 38.76160024\n",
      "Iteration 57, loss = 38.48343914\n",
      "Iteration 58, loss = 38.54487556\n",
      "Iteration 59, loss = 40.05808124\n",
      "Iteration 60, loss = 39.27585631\n",
      "Iteration 61, loss = 40.52168988\n",
      "Iteration 62, loss = 39.16954788\n",
      "Iteration 63, loss = 38.99237819\n",
      "Iteration 64, loss = 38.77354768\n",
      "Iteration 65, loss = 38.01604923\n",
      "Iteration 66, loss = 38.25937897\n",
      "Iteration 67, loss = 37.64258797\n",
      "Iteration 68, loss = 39.88074200\n",
      "Iteration 69, loss = 37.43512702\n",
      "Iteration 70, loss = 38.29469858\n",
      "Iteration 71, loss = 37.26399986\n",
      "Iteration 72, loss = 37.29299671\n",
      "Iteration 73, loss = 37.74791224\n",
      "Iteration 74, loss = 38.52105130\n",
      "Iteration 75, loss = 38.21535759\n",
      "Iteration 76, loss = 38.89683811\n",
      "Iteration 77, loss = 37.35281684\n",
      "Iteration 78, loss = 37.72206744\n",
      "Iteration 79, loss = 38.12056027\n",
      "Iteration 80, loss = 38.08960297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 17143 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 81, loss = 37.74022469\n",
      "Iteration 82, loss = 37.84630692\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "MSE with Best Parameters: -89.32297448017917\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989f85d05c214321b951fd65961f2933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m     explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mExplainer(model\u001b[38;5;241m=\u001b[39mbest_model, masker\u001b[38;5;241m=\u001b[39mX_calc)\n\u001b[1;32m---> 47\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_calc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m allshapvalues\u001b[38;5;241m.\u001b[39mappend(shap_values)\n\u001b[0;32m     49\u001b[0m explainer\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\pushk\\anaconda3\\envs\\plankton\\lib\\site-packages\\shap\\explainers\\_kernel.py:163\u001b[0m, in \u001b[0;36mKernelExplainer.__call__\u001b[1;34m(self, X, l1_reg, silent)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     feature_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_feature_names\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 163\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml1_reg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml1_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    165\u001b[0m     v \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(v, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# put outputs at the end\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pushk\\anaconda3\\envs\\plankton\\lib\\site-packages\\shap\\explainers\\_kernel.py:271\u001b[0m, in \u001b[0;36mKernelExplainer.shap_values\u001b[1;34m(self, X, **kwargs)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index:\n\u001b[0;32m    270\u001b[0m     data \u001b[38;5;241m=\u001b[39m convert_to_instance_with_index(data, column_name, index_value[i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], index_name)\n\u001b[1;32m--> 271\u001b[0m explanations\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplain(data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgc_collect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    273\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[1;32mc:\\Users\\pushk\\anaconda3\\envs\\plankton\\lib\\site-packages\\shap\\explainers\\_kernel.py:476\u001b[0m, in \u001b[0;36mKernelExplainer.explain\u001b[1;34m(self, incoming_instance, **kwargs)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernelWeights[nfixed_samples:] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m weight_left \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernelWeights[nfixed_samples:]\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m    475\u001b[0m \u001b[38;5;66;03m# execute the model on the synthetic samples we have created\u001b[39;00m\n\u001b[1;32m--> 476\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;66;03m# solve then expand the feature importance (Shapley value) vector to contain the non-varying features\u001b[39;00m\n\u001b[0;32m    479\u001b[0m phi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mgroups_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD))\n",
      "File \u001b[1;32mc:\\Users\\pushk\\anaconda3\\envs\\plankton\\lib\\site-packages\\shap\\explainers\\_kernel.py:627\u001b[0m, in \u001b[0;36mKernelExplainer.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m eyVal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD)\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN):\n\u001b[1;32m--> 627\u001b[0m     eyVal \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[i \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN \u001b[38;5;241m+\u001b[39m j, :] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mweights[j]\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mey[i, :] \u001b[38;5;241m=\u001b[39m eyVal\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnsamplesRun \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    # \"LightGBM\": (\n",
    "    #     lgb.LGBMRegressor(min_gain_to_split=0.1),\n",
    "    #     {\n",
    "    #         \"max_depth\": [3, 5, 7],\n",
    "    #         \"n_estimators\": [50, 100, 200],\n",
    "    #         \"learning_rate\": [0.1, 0.01, 0.001, 0.0001],\n",
    "    #     },\n",
    "    # ),\n",
    "    \"MLPRegressor\": (\n",
    "        MLPRegressor(max_iter=100, verbose=True),\n",
    "        {\n",
    "            \"hidden_layer_sizes\": [(50, 50), (100,)],\n",
    "            \"learning_rate_init\": [0.001, 0.01],\n",
    "        },\n",
    "    ),\n",
    "}\n",
    "\n",
    "with config_context(target_offload=\"gpu:0\"):\n",
    "    for model_name, (model, param_grid) in models.items():\n",
    "        print(\"MODEL NAME: \", model_name)\n",
    "        # Create a GridSearchCV object\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model, param_grid=param_grid, cv=kf, scoring=mse_scorer\n",
    "        )\n",
    "        if model_name == \"MLPRegressor\":\n",
    "            X_calc = preprocessor.fit_transform(X)\n",
    "        else:\n",
    "            X_calc=X.__deepcopy__()\n",
    "        grid_search.fit(X_calc, y)\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Store the best model and its parameters\n",
    "        best_models[model_name] = {\n",
    "            \"best_estimator\": best_model,\n",
    "            \"best_params\": grid_search.best_params_,\n",
    "            \"best_score\": grid_search.best_score_,\n",
    "        }\n",
    "\n",
    "        print(f\"MSE with Best Parameters: {best_models[model_name]['best_score']}\")\n",
    "        mse_scores.append(best_models[model_name][\"best_score\"])\n",
    "        kfold_bestparams.append(best_models[model_name][\"best_params\"])\n",
    "        if model_name == \"MLPRegressor\":\n",
    "            explainer = shap.KernelExplainer(model=best_model.predict, data=X_calc)\n",
    "        else:\n",
    "            explainer = shap.Explainer(model=best_model, masker=X_calc)\n",
    "        shap_values = explainer(X_calc)\n",
    "        allshapvalues.append(shap_values)\n",
    "        explainer.__class__\n",
    "\n",
    "        shap.summary_plot(shap_values, X_calc, show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Estimate the Shapley values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(allshapvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "explainer = shap.Explainer(model=model, masker=X_train)\n",
    "explainer.__class__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.force(shap_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values, max_display=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap.initjs()\n",
    "shap.plots.force(shap_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values,max_display=58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap[:, \"PN\"], color=shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
