{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary: SHAP and XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import shap\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from catboost import CatBoostRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6222422 entries, 0 to 6222421\n",
      "Data columns (total 36 columns):\n",
      " #   Column                   Dtype  \n",
      "---  ------                   -----  \n",
      " 0   Latitude_x               float64\n",
      " 1   Longitude_x              float64\n",
      " 2   Air Temperature QC       float64\n",
      " 3   Air Temperature          float64\n",
      " 4   Air pressure QC          float64\n",
      " 5   Air pressure             float64\n",
      " 6   Humidity QC              float64\n",
      " 7   Humidity                 float64\n",
      " 8   Wind speed QC            float64\n",
      " 9   Wind speed               float64\n",
      " 10  Wind Direction QC        float64\n",
      " 11  Wind Direction           float64\n",
      " 12  Latitude_y               float64\n",
      " 13  Longitude_y              float64\n",
      " 14  Temperature QC           float64\n",
      " 15  Temperature              float64\n",
      " 16  Salinity QC              float64\n",
      " 17  Salinity                 float64\n",
      " 18  Chlorophyll QC           float64\n",
      " 19  Chlorophyll              float64\n",
      " 20  Turbidity QC             float64\n",
      " 21  Turbidity                float64\n",
      " 22  Oxygen QC                float64\n",
      " 23  Oxygen                   float64\n",
      " 24  Waves QC                 float64\n",
      " 25  Significant wave height  float64\n",
      " 26  Wave from direction      float64\n",
      " 27  Wave period              float64\n",
      " 28  North surface currents   float64\n",
      " 29  East surface currents    float64\n",
      " 30  Sample_year              int64  \n",
      " 31  Sample_month             int64  \n",
      " 32  Sample_day               int64  \n",
      " 33  Sample_hour              int64  \n",
      " 34  Sample_minute            int64  \n",
      " 35  Sample_second            int64  \n",
      "dtypes: float64(30), int64(6)\n",
      "memory usage: 1.7 GB\n"
     ]
    }
   ],
   "source": [
    "combinedbuoyspath = \"../data/plankton-patrol/Plankton Patrol/Data/plank_ChesapeakeBay_all_buoys_clean.csv\"\n",
    "data = pd.read_csv(combinedbuoyspath)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Latitude_x', 'Longitude_x', 'Air Temperature QC', 'Air Temperature',\n",
       "       'Air pressure QC', 'Air pressure', 'Humidity QC', 'Humidity',\n",
       "       'Wind speed QC', 'Wind speed', 'Wind Direction QC', 'Wind Direction',\n",
       "       'Latitude_y', 'Longitude_y', 'Temperature QC', 'Temperature',\n",
       "       'Salinity QC', 'Salinity', 'Chlorophyll QC', 'Chlorophyll',\n",
       "       'Turbidity QC', 'Turbidity', 'Oxygen QC', 'Oxygen', 'Waves QC',\n",
       "       'Significant wave height', 'Wave from direction', 'Wave period',\n",
       "       'North surface currents', 'East surface currents', 'Sample_year',\n",
       "       'Sample_month', 'Sample_day', 'Sample_hour', 'Sample_minute',\n",
       "       'Sample_second'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_value(row, col1, col2, min_valid, max_valid):\n",
    "    \n",
    "    if pd.notna(row[col1]) and min_valid <= row[col1] <= max_valid:\n",
    "        return row[col1]\n",
    "    elif pd.notna(row[col2]) and min_valid <= row[col2] <= max_valid:\n",
    "        return row[col2]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def merge_valid_if_equal(df, col1, col2, constraint_type, newcol=None, show_diffs=True):\n",
    "    # Check if the columns are the same\n",
    "    condition =df[col1] != df[col2]\n",
    "    df_diffs = df[condition]\n",
    "    df.loc[condition, col1] = df.loc[condition, col1].fillna(df[col1])\n",
    "    df.loc[condition, col2] = df.loc[condition, col2].fillna(df[col2])\n",
    "    # df = df.fillna(-999)\n",
    "    if show_diffs:\n",
    "        print(df_diffs[[col1, col2]])\n",
    "    if newcol is None:\n",
    "        newcol = constraint_type\n",
    "    if constraint_type == \"Latitude\":\n",
    "        minval, maxval = -90, 90\n",
    "    if constraint_type == \"Longitude\":\n",
    "        minval, maxval = -180, 180\n",
    "    # if df[col1].equals(df[col2]):\n",
    "    # Merge the columns into a new column\n",
    "    df[newcol] = df.apply(get_valid_value, axis=1, args=(col1, col2, minval, maxval))\n",
    "    df.drop([col1, col2], axis=1, inplace=True)\n",
    "    return df\n",
    "    # else:\n",
    "    #     raise ValueError(\"The columns are not identical, cannot merge\")\n",
    "\n",
    "\n",
    "# def add_constraints(df, col, constraint_type):\n",
    "#     if constraint_type == \"Latitude\":\n",
    "#         df_filtered = df[(df[col] >= -90) & (df[col] <= 90)]\n",
    "\n",
    "#     if constraint_type == \"Longitude\":\n",
    "#         df_filtered = df[(df[col] >= -180) & (df[col] <= 180)]\n",
    "\n",
    "#     return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Latitude_x  Latitude_y\n",
      "0          39.20141         NaN\n",
      "1          39.20141         NaN\n",
      "2          39.20141         NaN\n",
      "3           0.00000         NaN\n",
      "4          39.20142         NaN\n",
      "...             ...         ...\n",
      "6222417         NaN         NaN\n",
      "6222418         NaN         NaN\n",
      "6222419         NaN         NaN\n",
      "6222420         NaN         NaN\n",
      "6222421         NaN         NaN\n",
      "\n",
      "[3700815 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data_temp1 = merge_valid_if_equal(data, 'Latitude_x','Latitude_y', 'Latitude')\n",
    "data_temp2 = merge_valid_if_equal(data_temp1, 'Longitude_x','Longitude_y', 'Longitude')\n",
    "data_temp2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_temp2.head()\n",
    "# raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data_temp2.replace([np.inf, -np.inf], np.nan)\n",
    "missing_percentage = data_clean.isnull().mean()\n",
    "print(missing_percentage)\n",
    "clean_columns = data_clean.columns.tolist()\n",
    "\n",
    "# Drop columns where more than 90% of the data is missing\n",
    "columns_to_drop = missing_percentage[missing_percentage > 0.90].index\n",
    "print(columns_to_drop)\n",
    "data_clean = data.dropna(subset=['Chlorophyll'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data_clean.drop(['Chlorophyll'], axis=1), data_clean['Chlorophyll']\n",
    "\n",
    "# X = pd.get_dummies(X, drop_first=True) # TOO much memory\n",
    "# Label encode categorical variables\n",
    "for col in X.select_dtypes(include=['category']).columns:\n",
    "    X[col] = X[col].cat.codes\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def visualize_param_map(df, parameter, path):\n",
    "    latitude = df['Latitude']\n",
    "    longitude = df['Longitude']\n",
    "    cha = df[parameter]\n",
    "\n",
    "    # Create a scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(longitude, latitude, c=cha, cmap='coolwarm', marker='o')\n",
    "\n",
    "    # Add a color bar\n",
    "    plt.colorbar(scatter, label='Chlorophyll')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.title(f'{parameter} Map')\n",
    "    plt.xlabel('Latitude')\n",
    "    plt.ylabel('Latitude')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.grid(True)\n",
    "    # plt.show()\n",
    "    plt.savefig(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listparams =['Air Temperature QC', 'Air Temperature',\n",
    "       'Air pressure QC', 'Air pressure', 'Humidity QC', 'Humidity',\n",
    "       'Wind speed QC', 'Wind speed', 'Wind Direction QC', 'Wind Direction',\n",
    "       'Latitude', 'Longitude', 'Temperature QC', 'Temperature',\n",
    "       'Salinity QC', 'Salinity', 'Chlorophyll QC', 'Chlorophyll',\n",
    "       'Turbidity QC', 'Turbidity', 'Oxygen QC', 'Oxygen', 'Waves QC',\n",
    "       'Significant wave height', 'Wave from direction', 'Wave period',\n",
    "       'North surface currents', 'East surface currents', 'Sample_year',\n",
    "       'Sample_month', 'Sample_day', 'Sample_hour', 'Sample_minute',\n",
    "       'Sample_second']\n",
    "\n",
    "\n",
    "import os\n",
    "path = \"../data/visualizations/buoys\"\n",
    "# if not os.path.exists(path):\n",
    "#        os.mkdir(path)\n",
    "# for p in listparams:\n",
    "#     visualize_param_map(data_clean, p, os.path.join(path, f\"{p}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train an XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid to search over\n",
    "# model = xgboost.XGBRegressor(\n",
    "#     max_depth=3,  # Maximum depth of a tree\n",
    "#     learning_rate=0.1,  # Learning rate\n",
    "#     n_estimators=100,  # Number of trees\n",
    "#     subsample=0.8,  # Subsample ratio of the training instances\n",
    "#     colsample_bytree=0.8,  # Subsample ratio of columns when constructing each tree\n",
    "#     gamma=0,  # Minimum loss reduction required to make a further partition\n",
    "#     min_child_weight=1,  # Minimum sum of instance weight needed in a child\n",
    "#     device=\"cuda\",\n",
    "#     enable_categorical=True,\n",
    "# )\n",
    "param_grid = {\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"learning_rate\": [0.1, 0.01, 0.001, 0.0001],\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "}\n",
    "models = {\n",
    "    \"LightGBM\": (\n",
    "        lgb.LGBMRegressor(),\n",
    "        {\n",
    "            \"max_depth\": [3, 5, 7],\n",
    "            \"n_estimators\": [50, 100, 200],\n",
    "            \"learning_rate\": [0.1, 0.01, 0.001, 0.0001]\n",
    "        },\n",
    "    ),\n",
    "    \"XGBoost\": (\n",
    "        xgb.XGBRegressor(),\n",
    "        {\n",
    "            \"max_depth\": [3, 5, 7],\n",
    "            \"n_estimators\": [50, 100, 200],\n",
    "            \"learning_rate\": [0.1, 0.01, 0.001, 0.0001]\n",
    "        },\n",
    "    ),\n",
    "    \"MLPRegressor\": (\n",
    "        MLPRegressor(max_iter=1000),\n",
    "        {\"hidden_layer_sizes\": [(50, 50), (100,)], \"learning_rate_init\": [0.001, 0.01]},\n",
    "    ),\n",
    "}\n",
    "\n",
    "# models = {\n",
    "#     'LightGBM': lgb.LGBMRegressor(),\n",
    "#     'XGBoost': xgb.XGBRegressor(),\n",
    "#     'MLPRegressor': CatBoostRegressor(silent=True)\n",
    "# }\n",
    "# Make custom scorer\n",
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "mse_scores = []\n",
    "kfold_bestparams = []\n",
    "allshapvalues = []\n",
    "best_models = {}\n",
    "kf = KFold(n_splits=5)\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "\n",
    "\n",
    "    # Create a GridSearchCV object\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model, param_grid=param_grid, cv=kf, scoring=mse_scorer\n",
    "    )\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(\n",
    "    #     X, y, test_size=0.2, random_state=42\n",
    "    # )\n",
    "    # Fit the grid search to the training data\n",
    "    # grid_search.fit(X_train, y_train)\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Get the best parameters\n",
    "    # best_params = grid_search.best_params_\n",
    "    # print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "    # Create a new model with the best parameters\n",
    "    # best_model = model(**best_params)\n",
    "    # # Train the best model\n",
    "    # best_model.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Store the best model and its parameters\n",
    "    best_models[model_name] = {\n",
    "        \"best_estimator\": best_model,\n",
    "        \"best_params\": grid_search.best_params_,\n",
    "        \"best_score\": grid_search.best_score_\n",
    "    }\n",
    "    # Predict on the test set\n",
    "    # y_pred_best = best_model.predict(X_test)\n",
    "    # mse = mean_squared_error(y_test, y_pred_best)\n",
    "\n",
    "    # Evaluate the best model\n",
    "    # accuracy_best = (y_pred_best == y_test).mean()\n",
    "    \n",
    "    print(f\"MSE with Best Parameters: {best_models[model_name]['best_score']}\")\n",
    "    mse_scores.append(best_models[model_name]['best_score'])\n",
    "    kfold_bestparams.append(best_models[model_name]['best_params'])\n",
    "\n",
    "    # plt.figure()\n",
    "    explainer = shap.Explainer(model=best_model, masker=X)\n",
    "    # plt.savefig(\n",
    "    # f\"shap_explainer_best_{model_name}_fold-{i}.png\", bbox_inches=\"tight\"\n",
    "    # )\n",
    "    # plt.close()\n",
    "    # for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    #     X_train, X_test, y_train, y_test = (\n",
    "    #         X.iloc[train_index],\n",
    "    #         X.iloc[test_index],\n",
    "    #         y.iloc[train_index],\n",
    "    #         y.iloc[test_index],\n",
    "    #     )\n",
    "    shap_values = explainer(X)\n",
    "    allshapvalues.append(shap_values)\n",
    "    explainer.__class__\n",
    "\n",
    "    # plt.figure()\n",
    "    shap.summary_plot(shap_values, X, show=False)\n",
    "    # plt.savefig(\n",
    "    #     f\"shap_summary_plot_best_{model_name}_fold-{i}.png\", bbox_inches=\"tight\"\n",
    "    # )\n",
    "    # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allshapvalues\n",
    "\n",
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = X_train\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Identify NaNs in the DataFrame\n",
    "nan_locations = df.isna()\n",
    "print(\"\\nLocations of NaNs in the DataFrame:\")\n",
    "print(nan_locations)\n",
    "\n",
    "# Count NaNs in each column\n",
    "nan_count_per_column = df.isna().sum()\n",
    "print(\"\\nCount of NaNs in each column:\")\n",
    "print(nan_count_per_column)\n",
    "\n",
    "# Count NaNs in each row\n",
    "nan_count_per_row = df.isna().sum(axis=1)\n",
    "print(\"\\nCount of NaNs in each row:\")\n",
    "print(nan_count_per_row)\n",
    "\n",
    "# Rows with at least one NaN\n",
    "rows_with_nans = df[df.isna().any(axis=1)]\n",
    "print(\"\\nRows with at least one NaN:\")\n",
    "print(rows_with_nans)\n",
    "\n",
    "# Columns with at least one NaN\n",
    "columns_with_nans = df.columns[df.isna().any()].tolist()\n",
    "print(\"\\nColumns with at least one NaN:\")\n",
    "print(columns_with_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = dict()\n",
    "# params[\"device\"] = \"cuda\"\n",
    "# params[\"tree_method\"] = \"hist\"\n",
    "\n",
    "model = xgboost.XGBRegressor(missing=np.nan, enable_categorical=True, device=\"cuda\")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"MSE: %f\" % (mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Estimate the Shapley values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "explainer = shap.Explainer(model=model, masker=X_train)\n",
    "explainer.__class__\n",
    "\n",
    "# explainer = shap.TreeExplainer(model, data=cp.asarray(X_train_cp))\n",
    "# shap_values = explainer.shap_values(cp.asarray(X_test_cp))\n",
    "\n",
    "# Convert Shapley values back to NumPy arrays for compatibility\n",
    "# shap_values = cp.asnumpy(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values[0], max_display=58)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.force(shap_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values, max_display=58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.force(shap_values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values,max_display=58)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
