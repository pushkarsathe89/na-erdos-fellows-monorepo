{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary: SHAP and XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import shap\n",
    "# import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from catboost import CatBoostRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/plankton-patrol/Plankton Patrol/Data/plank_ChesapeakeBay_all_buoys_clean.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m combinedbuoyspath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/plankton-patrol/Plankton Patrol/Data/plank_ChesapeakeBay_all_buoys_clean.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(combinedbuoyspath)\n\u001b[1;32m      3\u001b[0m data\u001b[38;5;241m.\u001b[39minfo()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/plankton-patrol/Plankton Patrol/Data/plank_ChesapeakeBay_all_buoys_clean.csv'"
     ]
    }
   ],
   "source": [
    "combinedbuoyspath = \"../data/plankton-patrol/Plankton Patrol/Data/plank_ChesapeakeBay_all_buoys_clean.csv\"\n",
    "data = pd.read_csv(combinedbuoyspath)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Latitude_x', 'Longitude_x', 'Air Temperature QC', 'Air Temperature',\n",
       "       'Air pressure QC', 'Air pressure', 'Humidity QC', 'Humidity',\n",
       "       'Wind speed QC', 'Wind speed', 'Wind Direction QC', 'Wind Direction',\n",
       "       'Latitude_y', 'Longitude_y', 'Temperature QC', 'Temperature',\n",
       "       'Salinity QC', 'Salinity', 'Chlorophyll QC', 'Chlorophyll',\n",
       "       'Turbidity QC', 'Turbidity', 'Oxygen QC', 'Oxygen', 'Waves QC',\n",
       "       'Significant wave height', 'Wave from direction', 'Wave period',\n",
       "       'North surface currents', 'East surface currents', 'Sample_year',\n",
       "       'Sample_month', 'Sample_day', 'Sample_hour', 'Sample_minute',\n",
       "       'Sample_second'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_value(row, col1, col2, min_valid, max_valid):\n",
    "    \n",
    "    if pd.notna(row[col1]) and min_valid <= row[col1] <= max_valid:\n",
    "        return row[col1]\n",
    "    elif pd.notna(row[col2]) and min_valid <= row[col2] <= max_valid:\n",
    "        return row[col2]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def merge_valid_if_equal(df, col1, col2, constraint_type, newcol=None, show_diffs=True):\n",
    "    # Check if the columns are the same\n",
    "    condition =df[col1] != df[col2]\n",
    "    df_diffs = df[condition]\n",
    "    df.loc[condition, col1] = df.loc[condition, col1].fillna(df[col1])\n",
    "    df.loc[condition, col2] = df.loc[condition, col2].fillna(df[col2])\n",
    "    # df = df.fillna(-999)\n",
    "    if show_diffs:\n",
    "        print(df_diffs[[col1, col2]])\n",
    "    if newcol is None:\n",
    "        newcol = constraint_type\n",
    "    if constraint_type == \"Latitude\":\n",
    "        minval, maxval = -90, 90\n",
    "    if constraint_type == \"Longitude\":\n",
    "        minval, maxval = -180, 180\n",
    "    # if df[col1].equals(df[col2]):\n",
    "    # Merge the columns into a new column\n",
    "    df[newcol] = df.apply(get_valid_value, axis=1, args=(col1, col2, minval, maxval))\n",
    "    df.drop([col1, col2], axis=1, inplace=True)\n",
    "    return df\n",
    "    # else:\n",
    "    #     raise ValueError(\"The columns are not identical, cannot merge\")\n",
    "\n",
    "\n",
    "# def add_constraints(df, col, constraint_type):\n",
    "#     if constraint_type == \"Latitude\":\n",
    "#         df_filtered = df[(df[col] >= -90) & (df[col] <= 90)]\n",
    "\n",
    "#     if constraint_type == \"Longitude\":\n",
    "#         df_filtered = df[(df[col] >= -180) & (df[col] <= 180)]\n",
    "\n",
    "#     return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Latitude_x  Latitude_y\n",
      "0          39.20141         NaN\n",
      "1          39.20141         NaN\n",
      "2          39.20141         NaN\n",
      "3           0.00000         NaN\n",
      "4          39.20142         NaN\n",
      "...             ...         ...\n",
      "6222417         NaN         NaN\n",
      "6222418         NaN         NaN\n",
      "6222419         NaN         NaN\n",
      "6222420         NaN         NaN\n",
      "6222421         NaN         NaN\n",
      "\n",
      "[3700815 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data_temp1 = merge_valid_if_equal(data, 'Latitude_x','Latitude_y', 'Latitude')\n",
    "data_temp2 = merge_valid_if_equal(data_temp1, 'Longitude_x','Longitude_y', 'Longitude')\n",
    "data_temp2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_temp2.head()\n",
    "# raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data_temp2.replace([np.inf, -np.inf], np.nan)\n",
    "missing_percentage = data_clean.isnull().mean()\n",
    "print(missing_percentage)\n",
    "clean_columns = data_clean.columns.tolist()\n",
    "\n",
    "# Drop columns where more than 90% of the data is missing\n",
    "columns_to_drop = missing_percentage[missing_percentage > 0.90].index\n",
    "print(columns_to_drop)\n",
    "data_clean = data.dropna(subset=['Chlorophyll'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data_clean.drop(['Chlorophyll'], axis=1), data_clean['Chlorophyll']\n",
    "\n",
    "# X = pd.get_dummies(X, drop_first=True) # TOO much memory\n",
    "# Label encode categorical variables\n",
    "for col in X.select_dtypes(include=['category']).columns:\n",
    "    X[col] = X[col].cat.codes\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def visualize_param_map(df, parameter, path):\n",
    "    latitude = df['Latitude']\n",
    "    longitude = df['Longitude']\n",
    "    cha = df[parameter]\n",
    "\n",
    "    # Create a scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(longitude, latitude, c=cha, cmap='coolwarm', marker='o')\n",
    "\n",
    "    # Add a color bar\n",
    "    plt.colorbar(scatter, label='Chlorophyll')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.title(f'{parameter} Map')\n",
    "    plt.xlabel('Latitude')\n",
    "    plt.ylabel('Latitude')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.grid(True)\n",
    "    # plt.show()\n",
    "    plt.savefig(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listparams =['Air Temperature QC', 'Air Temperature',\n",
    "       'Air pressure QC', 'Air pressure', 'Humidity QC', 'Humidity',\n",
    "       'Wind speed QC', 'Wind speed', 'Wind Direction QC', 'Wind Direction',\n",
    "       'Latitude', 'Longitude', 'Temperature QC', 'Temperature',\n",
    "       'Salinity QC', 'Salinity', 'Chlorophyll QC', 'Chlorophyll',\n",
    "       'Turbidity QC', 'Turbidity', 'Oxygen QC', 'Oxygen', 'Waves QC',\n",
    "       'Significant wave height', 'Wave from direction', 'Wave period',\n",
    "       'North surface currents', 'East surface currents', 'Sample_year',\n",
    "       'Sample_month', 'Sample_day', 'Sample_hour', 'Sample_minute',\n",
    "       'Sample_second']\n",
    "\n",
    "\n",
    "import os\n",
    "path = \"../data/visualizations/buoys\"\n",
    "# if not os.path.exists(path):\n",
    "#        os.mkdir(path)\n",
    "# for p in listparams:\n",
    "#     visualize_param_map(data_clean, p, os.path.join(path, f\"{p}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train an XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid to search over\n",
    "# model = xgboost.XGBRegressor(\n",
    "#     max_depth=3,  # Maximum depth of a tree\n",
    "#     learning_rate=0.1,  # Learning rate\n",
    "#     n_estimators=100,  # Number of trees\n",
    "#     subsample=0.8,  # Subsample ratio of the training instances\n",
    "#     colsample_bytree=0.8,  # Subsample ratio of columns when constructing each tree\n",
    "#     gamma=0,  # Minimum loss reduction required to make a further partition\n",
    "#     min_child_weight=1,  # Minimum sum of instance weight needed in a child\n",
    "#     device=\"cuda\",\n",
    "#     enable_categorical=True,\n",
    "# )\n",
    "param_grid = {\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"learning_rate\": [0.1, 0.01, 0.001, 0.0001],\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "}\n",
    "models = {\n",
    "    \"LightGBM\": (\n",
    "        lgb.LGBMRegressor(),\n",
    "        {\n",
    "            \"max_depth\": [3, 5, 7],\n",
    "            \"n_estimators\": [50, 100, 200],\n",
    "            \"learning_rate\": [0.1, 0.01, 0.001, 0.0001]\n",
    "        },\n",
    "    ),\n",
    "    \"XGBoost\": (\n",
    "        xgb.XGBRegressor(),\n",
    "        {\n",
    "            \"max_depth\": [3, 5, 7],\n",
    "            \"n_estimators\": [50, 100, 200],\n",
    "            \"learning_rate\": [0.1, 0.01, 0.001, 0.0001]\n",
    "        },\n",
    "    ),\n",
    "    \"MLPRegressor\": (\n",
    "        MLPRegressor(max_iter=1000),\n",
    "        {\"hidden_layer_sizes\": [(50, 50), (100,)], \"learning_rate_init\": [0.001, 0.01]},\n",
    "    ),\n",
    "}\n",
    "\n",
    "# models = {\n",
    "#     'LightGBM': lgb.LGBMRegressor(),\n",
    "#     'XGBoost': xgb.XGBRegressor(),\n",
    "#     'MLPRegressor': CatBoostRegressor(silent=True)\n",
    "# }\n",
    "# Make custom scorer\n",
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "mse_scores = []\n",
    "kfold_bestparams = []\n",
    "allshapvalues = []\n",
    "best_models = {}\n",
    "kf = KFold(n_splits=5)\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "\n",
    "\n",
    "    # Create a GridSearchCV object\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model, param_grid=param_grid, cv=kf, scoring=mse_scorer\n",
    "    )\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(\n",
    "    #     X, y, test_size=0.2, random_state=42\n",
    "    # )\n",
    "    # Fit the grid search to the training data\n",
    "    # grid_search.fit(X_train, y_train)\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Get the best parameters\n",
    "    # best_params = grid_search.best_params_\n",
    "    # print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "    # Create a new model with the best parameters\n",
    "    # best_model = model(**best_params)\n",
    "    # # Train the best model\n",
    "    # best_model.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Store the best model and its parameters\n",
    "    best_models[model_name] = {\n",
    "        \"best_estimator\": best_model,\n",
    "        \"best_params\": grid_search.best_params_,\n",
    "        \"best_score\": grid_search.best_score_\n",
    "    }\n",
    "    # Predict on the test set\n",
    "    # y_pred_best = best_model.predict(X_test)\n",
    "    # mse = mean_squared_error(y_test, y_pred_best)\n",
    "\n",
    "    # Evaluate the best model\n",
    "    # accuracy_best = (y_pred_best == y_test).mean()\n",
    "    \n",
    "    print(f\"MSE with Best Parameters: {best_models[model_name]['best_score']}\")\n",
    "    mse_scores.append(best_models[model_name]['best_score'])\n",
    "    kfold_bestparams.append(best_models[model_name]['best_params'])\n",
    "\n",
    "    # plt.figure()\n",
    "    explainer = shap.Explainer(model=best_model, masker=X)\n",
    "    # plt.savefig(\n",
    "    # f\"shap_explainer_best_{model_name}_fold-{i}.png\", bbox_inches=\"tight\"\n",
    "    # )\n",
    "    # plt.close()\n",
    "    # for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    #     X_train, X_test, y_train, y_test = (\n",
    "    #         X.iloc[train_index],\n",
    "    #         X.iloc[test_index],\n",
    "    #         y.iloc[train_index],\n",
    "    #         y.iloc[test_index],\n",
    "    #     )\n",
    "    shap_values = explainer(X)\n",
    "    allshapvalues.append(shap_values)\n",
    "    explainer.__class__\n",
    "\n",
    "    # plt.figure()\n",
    "    shap.summary_plot(shap_values, X, show=False)\n",
    "    # plt.savefig(\n",
    "    #     f\"shap_summary_plot_best_{model_name}_fold-{i}.png\", bbox_inches=\"tight\"\n",
    "    # )\n",
    "    # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allshapvalues\n",
    "\n",
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = X_train\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Identify NaNs in the DataFrame\n",
    "nan_locations = df.isna()\n",
    "print(\"\\nLocations of NaNs in the DataFrame:\")\n",
    "print(nan_locations)\n",
    "\n",
    "# Count NaNs in each column\n",
    "nan_count_per_column = df.isna().sum()\n",
    "print(\"\\nCount of NaNs in each column:\")\n",
    "print(nan_count_per_column)\n",
    "\n",
    "# Count NaNs in each row\n",
    "nan_count_per_row = df.isna().sum(axis=1)\n",
    "print(\"\\nCount of NaNs in each row:\")\n",
    "print(nan_count_per_row)\n",
    "\n",
    "# Rows with at least one NaN\n",
    "rows_with_nans = df[df.isna().any(axis=1)]\n",
    "print(\"\\nRows with at least one NaN:\")\n",
    "print(rows_with_nans)\n",
    "\n",
    "# Columns with at least one NaN\n",
    "columns_with_nans = df.columns[df.isna().any()].tolist()\n",
    "print(\"\\nColumns with at least one NaN:\")\n",
    "print(columns_with_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = dict()\n",
    "# params[\"device\"] = \"cuda\"\n",
    "# params[\"tree_method\"] = \"hist\"\n",
    "\n",
    "model = xgboost.XGBRegressor(missing=np.nan, enable_categorical=True, device=\"cuda\")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"MSE: %f\" % (mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Estimate the Shapley values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "explainer = shap.Explainer(model=model, masker=X_train)\n",
    "explainer.__class__\n",
    "\n",
    "# explainer = shap.TreeExplainer(model, data=cp.asarray(X_train_cp))\n",
    "# shap_values = explainer.shap_values(cp.asarray(X_test_cp))\n",
    "\n",
    "# Convert Shapley values back to NumPy arrays for compatibility\n",
    "# shap_values = cp.asnumpy(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values[0], max_display=58)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.force(shap_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values, max_display=58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.force(shap_values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values,max_display=58)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
