{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean buoy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "buoypath = \"../data/data//plank_ChesapeakeBay_all_buoys_clean.csv\"\n",
    "data = pd.read_csv(buoypath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_value(row, col1, col2, min_valid, max_valid):\n",
    "    \n",
    "    if pd.notna(row[col1]) and min_valid <= row[col1] <= max_valid:\n",
    "        return row[col1]\n",
    "    elif pd.notna(row[col2]) and min_valid <= row[col2] <= max_valid:\n",
    "        return row[col2]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def merge_valid_if_equal(df, col1, col2, constraint_type, newcol=None, show_diffs=True):\n",
    "    # Check if the columns are the same\n",
    "    condition =df[col1] != df[col2]\n",
    "    df_diffs = df[condition]\n",
    "    df.loc[condition, col1] = df.loc[condition, col1].fillna(df[col1])\n",
    "    df.loc[condition, col2] = df.loc[condition, col2].fillna(df[col2])\n",
    "    # df = df.fillna(-999)\n",
    "    if show_diffs:\n",
    "        print(df_diffs[[col1, col2]])\n",
    "    if newcol is None:\n",
    "        newcol = constraint_type\n",
    "    if constraint_type == \"Latitude\":\n",
    "        minval, maxval = -90, 90\n",
    "    if constraint_type == \"Longitude\":\n",
    "        minval, maxval = -180, 180\n",
    "    # if df[col1].equals(df[col2]):\n",
    "    # Merge the columns into a new column\n",
    "    df[newcol] = df.apply(get_valid_value, axis=1, args=(col1, col2, minval, maxval))\n",
    "    df.drop([col1, col2], axis=1, inplace=True)\n",
    "    return df\n",
    "    # else:\n",
    "    #     raise ValueError(\"The columns are not identical, cannot merge\")\n",
    "\n",
    "\n",
    "# def add_constraints(df, col, constraint_type):\n",
    "#     if constraint_type == \"Latitude\":\n",
    "#         df_filtered = df[(df[col] >= -90) & (df[col] <= 90)]\n",
    "\n",
    "#     if constraint_type == \"Longitude\":\n",
    "#         df_filtered = df[(df[col] >= -180) & (df[col] <= 180)]\n",
    "\n",
    "#     return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Latitude_x  Latitude_y\n",
      "0          39.20141         NaN\n",
      "1          39.20141         NaN\n",
      "2          39.20141         NaN\n",
      "3           0.00000         NaN\n",
      "4          39.20142         NaN\n",
      "...             ...         ...\n",
      "6222417         NaN         NaN\n",
      "6222418         NaN         NaN\n",
      "6222419         NaN         NaN\n",
      "6222420         NaN         NaN\n",
      "6222421         NaN         NaN\n",
      "\n",
      "[3700815 rows x 2 columns]\n",
      "         Longitude_x  Longitude_y\n",
      "0          -76.57479          NaN\n",
      "1          -76.57486          NaN\n",
      "2          -76.57480          NaN\n",
      "3                NaN          NaN\n",
      "4          -76.57483          NaN\n",
      "...              ...          ...\n",
      "6222417          NaN          NaN\n",
      "6222418          NaN          NaN\n",
      "6222419          NaN          NaN\n",
      "6222420          NaN          NaN\n",
      "6222421          NaN          NaN\n",
      "\n",
      "[3719365 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data_temp1 = merge_valid_if_equal(data, 'Latitude_x','Latitude_y', 'Latitude')\n",
    "data_temp2 = merge_valid_if_equal(data_temp1, 'Longitude_x','Longitude_y', 'Longitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data_clean = data_temp2.replace([np.inf, -np.inf], np.nan)\n",
    "missing_percentage = data_clean.isnull().mean()\n",
    "# print(missing_percentage)\n",
    "clean_columns = data_clean.columns.tolist()\n",
    "\n",
    "# Drop columns where more than 90% of the data is missing\n",
    "columns_to_drop = missing_percentage[missing_percentage > 0.90].index\n",
    "print(columns_to_drop)\n",
    "data_clean = data.dropna(subset=['Chlorophyll'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "del data_temp1\n",
    "del data_temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove outliers\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# data_clean.min()\n",
    "# data_clean.max()\n",
    "\n",
    "# humidity is in percentage and the maximum are in reasonable range, take min(current value, 100)\n",
    "data_clean['Humidity'] = data_clean['Humidity'].apply(lambda x: min(x, 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -50< Air temperature <50\n",
    "# Air pressure is around 1000 milibars\n",
    "# Humidity is in percentage\n",
    "# 0 <= Wind speed < 150\n",
    "# -50 < Temperature < 50\n",
    "# 0 < Salinity < 50 (usually 25 is max)\n",
    "# 0 < Chlorophyll < 500 (usually 100 is max)\n",
    "# 0 < nephelometric turbidity units\n",
    "# 0 < oxygen < 20\n",
    "\n",
    "data_clean = data_clean.drop(data_clean.index[\n",
    "    (data_clean['Air Temperature']<-50) |\n",
    "    (data_clean['Air pressure'] < 800) |\n",
    "    (data_clean['Humidity'] < 0) |\n",
    "    (data_clean['Wind speed'] < 0) |\n",
    "    (data_clean['Temperature'] < -50) |\n",
    "    (data_clean['Salinity'] < 0) |\n",
    "    (data_clean['Chlorophyll'] < 0) |\n",
    "    (data_clean['Turbidity'] < 0) |\n",
    "    (data_clean['Oxygen'] < 0) |\n",
    "    (data_clean['Air Temperature'] > 50) |\n",
    "    (data_clean['Air pressure'] > 2000) |\n",
    "    (data_clean['Wind speed'] > 150) |\n",
    "    (data_clean['Temperature'] > 50) |\n",
    "    (data_clean['Salinity'] > 50) |\n",
    "    (data_clean['Chlorophyll'] > 500) |\n",
    "    (data_clean['Oxygen'] > 20)\n",
    "    ].tolist())\n",
    "\n",
    "# surface currents are not accurate\n",
    "# data_clean = data_clean.drop(columns= ['North surface currents', 'East surface currents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Air Temperature QC', 'Air Temperature', 'Air pressure QC',\n",
       "       'Air pressure', 'Humidity QC', 'Humidity', 'Wind speed QC',\n",
       "       'Wind speed', 'Wind Direction QC', 'Wind Direction', 'Temperature QC',\n",
       "       'Temperature', 'Salinity QC', 'Salinity', 'Chlorophyll QC',\n",
       "       'Chlorophyll', 'Turbidity QC', 'Turbidity', 'Oxygen QC', 'Oxygen',\n",
       "       'Waves QC', 'Significant wave height', 'Wave from direction',\n",
       "       'Wave period', 'North surface currents', 'East surface currents',\n",
       "       'Sample_year', 'Sample_month', 'Sample_day', 'Sample_hour',\n",
       "       'Sample_minute', 'Sample_second', 'Latitude', 'Longitude'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete QC columns\n",
    "data_clean = data_clean.drop(columns= ['Air Temperature QC','Air pressure QC','Humidity QC','Wind speed QC','Wind Direction QC', 'Temperature QC','Salinity QC','Chlorophyll QC','Turbidity QC','Oxygen QC','Waves QC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sorted_buoy = data_clean.sort_values(by=['Sample_year', 'Sample_month', 'Sample_day', 'Sample_hour','Sample_minute'])\n",
    "# data_sorted_buoy = data_sorted_buoy.reset_index()\n",
    "del data_clean\n",
    "\n",
    "data_sorted_buoy.to_csv('../data/to_merge_buoy.csv', index=False)\n",
    "\n",
    "del data_sorted_buoy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean water quality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/plankton-patrol_ChesapeakeWaterQuality.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m waterqualitypath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/plankton-patrol_ChesapeakeWaterQuality.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaterqualitypath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pushk\\anaconda3\\envs\\plankton\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pushk\\anaconda3\\envs\\plankton\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\pushk\\anaconda3\\envs\\plankton\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pushk\\anaconda3\\envs\\plankton\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\pushk\\anaconda3\\envs\\plankton\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/plankton-patrol_ChesapeakeWaterQuality.csv'"
     ]
    }
   ],
   "source": [
    "waterqualitypath = \"../data/plankton-patrol_ChesapeakeWaterQuality.csv\"\n",
    "data = pd.read_csv(waterqualitypath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Qualifier\"] = data[\"Qualifier\"].replace(np.nan, \"=\")\n",
    "\n",
    "\n",
    "columns_to_exclude = [\"Parameter\", \"MeasureValue\", \"Unit\"]\n",
    "unique_columns = [col for col in data.columns if col not in columns_to_exclude]\n",
    "\n",
    "df_unique = data[unique_columns].drop_duplicates(subset=\"EventId\")\n",
    "print(df_unique.shape, data.shape)\n",
    "data_r = data.pivot_table(\n",
    "    index=[\"EventId\"], columns=\"Parameter\", values=\"MeasureValue\", aggfunc=\"first\"\n",
    ").reset_index()\n",
    "exclude_from_pivoted = [\"Parameter\", \"MeasureValue\", \"Unit\", \"SampleDate\", \"SampleTime\"]\n",
    "pivoted_columns = data_r.columns.tolist()\n",
    "for ce in exclude_from_pivoted:\n",
    "    if ce in pivoted_columns:\n",
    "        pivoted_columns.remove(ce)\n",
    "data_m = pd.merge(df_unique, data_r, on=\"EventId\", how=\"left\")\n",
    "\n",
    "\n",
    "print(data_m.columns, data_m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['EventId','CBSeg2003','Cruise','Program','Project','Agency','Source','Lab','TierLevel']\n",
    "\n",
    "# Drop the specified columns\n",
    "data_md = data_m.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_numeric = [\n",
    "    \"TotalDepth\",\n",
    "    \"UpperPycnocline\",\n",
    "    \"LowerPycnocline\",\n",
    "    \"Depth\",\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "    \"MeasureValue\",\n",
    "    \"CHLA\",\n",
    "    \"DIN\",\n",
    "    \"DO\",\n",
    "    \"DOC\",\n",
    "    \"DON\",\n",
    "    \"DOP\",\n",
    "    \"DO_SAT_P\",\n",
    "    \"FSS\",\n",
    "    \"KD\",\n",
    "    \"NH4F\",\n",
    "    \"NO23F\",\n",
    "    \"NO2F\",\n",
    "    \"NO3F\",\n",
    "    \"PC\",\n",
    "    \"PH\",\n",
    "    \"PHEO\",\n",
    "    \"PIP\",\n",
    "    \"PN\",\n",
    "    \"PO4F\",\n",
    "    \"PP\",\n",
    "    \"Parameter\",\n",
    "    \"SALINITY\",\n",
    "    \"SECCHI\",\n",
    "    \"SIF\",\n",
    "    \"SIGMA_T\",\n",
    "    \"SPCOND\",\n",
    "    \"TDN\",\n",
    "    \"TDP\",\n",
    "    \"TN\",\n",
    "    \"TON\",\n",
    "    \"TP\",\n",
    "    \"TSS\",\n",
    "    \"TURB_NTU\",\n",
    "    \"VSS\",\n",
    "    \"WTEMP\",\n",
    "]\n",
    "# set_string = ['\n",
    "set_date = [\"SampleDate\", \"SampleTime\"]\n",
    "\n",
    "\n",
    "def combine_date_time_strings(df, date_col, time_col):\n",
    "    # Combine date and time strings\n",
    "    combined_col = df[date_col] + \" \" + df[time_col]\n",
    "    # Convert the combined string to datetime\n",
    "    datetime_col = \"Sample\"\n",
    "\n",
    "    df[datetime_col] = pd.to_datetime(combined_col, errors=\"coerce\")\n",
    "    df[datetime_col + \"_year\"] = df[datetime_col].dt.year\n",
    "    df[datetime_col + \"_month\"] = df[datetime_col].dt.month\n",
    "    df[datetime_col + \"_day\"] = df[datetime_col].dt.day\n",
    "    df[datetime_col + \"_hour\"] = df[datetime_col].dt.hour\n",
    "    df[datetime_col + \"_minute\"] = df[datetime_col].dt.minute\n",
    "    df[datetime_col + \"_second\"] = df[datetime_col].dt.second\n",
    "    newcolnames = [\n",
    "        f\"{datetime_col}_year\",\n",
    "        f\"{datetime_col}_month\",\n",
    "        f\"{datetime_col}_day\",\n",
    "        f\"{datetime_col}_hour\",\n",
    "        f\"{datetime_col}_minute\",\n",
    "        f\"{datetime_col}_second\",\n",
    "    ]\n",
    "    # Drop the original date, time, and combined datetime columns\n",
    "    df.drop(columns=[date_col, time_col, datetime_col], inplace=True)\n",
    "\n",
    "    return df, newcolnames\n",
    "\n",
    "\n",
    "# Function to convert columns to appropriate types\n",
    "def convert_dtypes(df):\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].replace(\"nan\", np.nan)\n",
    "        # print(df[col][564463])\n",
    "        print(f\"converting column {col}\", end=\"\\t\")\n",
    "        print(df[col].dtype)\n",
    "\n",
    "        if col not in (set_numeric + set_date):\n",
    "            df[col] = pd.Categorical(df[col])\n",
    "            print(\"Categorical\")\n",
    "        elif col in set_numeric:\n",
    "            # try:\n",
    "            # Try converting to numeric (float)\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "            print(\"Numeric\")\n",
    "        # except (ValueError, TypeError):\n",
    "        # try:\n",
    "        elif col in set_date:\n",
    "            print(\"Date, skipped\")\n",
    "            pass\n",
    "            # Try converting to datetime (date)\n",
    "            # df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "            # except (ValueError, TypeError):\n",
    "            # Check for categorical\n",
    "            # unique_ratio = df[col].nunique() / df[col].count()\n",
    "            # if unique_ratio < 0.2:  # heuristic for categorical, adjustable threshold\n",
    "            # Convert to string if not numeric, date, or categorical\n",
    "        else:\n",
    "            print(f\"{col}: string\")\n",
    "            df[col] = df[col].astype(str)\n",
    "            print(\"string\")\n",
    "        print(df[col].dtype)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply the conversion function\n",
    "datacopy = data_md.__deepcopy__()\n",
    "data_conv = convert_dtypes(datacopy)\n",
    "data_cleana, newcols = combine_date_time_strings(data_conv, \"SampleDate\", \"SampleTime\")\n",
    "# Check the result\n",
    "print(data_cleana.dtypes)\n",
    "print(data_cleana.shape)\n",
    "# data.dropna(axis='TotalDepth',how='all')\n",
    "data_cleana = data_cleana[:-1]\n",
    "print(data_cleana.shape)\n",
    "\n",
    "# data_conv = convert_dtypes(data_conv)\n",
    "# for col in data.columns:\n",
    "#     # print(data[col])\n",
    "#     if col not in (set_float + set_date):\n",
    "#         data[col]= pd.Categorical(df[col])(data[col][1:])\n",
    "#     # if col in set_string:\n",
    "#     #     data[col][1:]=data[col][1:].astype(\"string\")\n",
    "#     if col in set_float:\n",
    "#         data[col]= pd.to_numeric(data[col], errors='raise')\n",
    "#     if col in set_date:\n",
    "#         print(col)\n",
    "#         data[col][1:]=pd.to_datetime(data[col][1:],errors='coerce')\n",
    "#         # data[col]=data[col].dt.strftime(f'%m/%d/%Y')\n",
    "data_cleana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_columns= pivoted_columns+newcols\n",
    "print(len(pivoted_columns), pivoted_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivoted_columns = [col for col in pivoted_columns if col in data_clean.columns]\n",
    "\n",
    "# data_clean = data_clean.replace([np.inf, -np.inf], np.nan).dropna(\n",
    "#     subset=[pivoted_columns]\n",
    "# )\n",
    "data_cleana = data_cleana.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "\n",
    "missing_percentage = data_cleana.isnull().mean()\n",
    "print(missing_percentage)\n",
    "clean_columns = data_cleana.columns.tolist()\n",
    "\n",
    "# Drop columns where more than 95% of the data is missing\n",
    "columns_to_drop = missing_percentage[missing_percentage > 0.90].index\n",
    "print(columns_to_drop)\n",
    "data_cleana = data_cleana.drop(columns=columns_to_drop)\n",
    "data_clean = data_cleana.copy()\n",
    "data_clean = data_cleana.dropna(subset=['CHLA'])\n",
    "# data_clean = data_cleana.dropna(\n",
    "#     subset=[\n",
    "#         \"CHLA\",\n",
    "#         \"DIN\",\n",
    "#         \"DO\",\n",
    "#         \"DOC\",\n",
    "#         \"DON\",\n",
    "#         \"DOP\",\n",
    "#         \"DO_SAT_P\",\n",
    "#         \"FSS\",\n",
    "#         \"KD\",\n",
    "#         \"NH4F\",\n",
    "#         \"NO23F\",\n",
    "#         \"NO2F\",\n",
    "#         \"NO3F\",\n",
    "#         \"PC\",\n",
    "#         \"PH\",\n",
    "#         \"PHEO\",\n",
    "#         \"PN\",\n",
    "#         \"PO4F\",\n",
    "#         \"PP\",\n",
    "#         \"SALINITY\",\n",
    "#         \"SECCHI\",\n",
    "#         \"SIF\",\n",
    "#         \"SIGMA_T\",\n",
    "#         \"SPCOND\",\n",
    "#         \"TDN\",\n",
    "#         \"TDP\",\n",
    "#         \"TN\",\n",
    "#         \"TON\",\n",
    "#         \"TP\",\n",
    "#         \"TSS\",\n",
    "#         \"VSS\",\n",
    "#         \"WTEMP\",\n",
    "#     ]\n",
    "# )\n",
    "dropped_columns = [col for col in clean_columns if col not in data_clean.columns]\n",
    "\n",
    "data_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dropped_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert = ['Sample_year', 'Sample_month', 'Sample_day', 'Sample_hour', 'Sample_minute', 'Sample_second']\n",
    "data_clean[columns_to_convert] = data_clean[columns_to_convert].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.drop(columns=['Station','Layer', 'SampleType', 'SampleReplicateType','Qualifier','Method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sorted_water = data_clean.sort_values(by=['Sample_year', 'Sample_month', 'Sample_day', 'Sample_hour','Sample_minute','Sample_second'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sorted_water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sorted_water.to_csv('../data/to_merge_water.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxic data (deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedbuoyspath = \"../data/ToxicsChemicalContaminantHUC8_updated.csv\"\n",
    "# New file with deleted row no 131050, for 'new' file row no 275590\n",
    "\n",
    "# data = pd.read_csv(combinedbuoyspath, on_bad_lines='skip')\n",
    "data = pd.read_csv(combinedbuoyspath,sep=',')\n",
    "# data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['SampleDateTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_r = data.pivot_table(\n",
    "    index=[\"SampleDateTime\"], columns=\"ChemicalName\", values=\"ReportedValue\", aggfunc=\"first\"\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivotcols = data_r.columns\n",
    "print(pivotcols.shape)\n",
    "data_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_date_time_strings(df, originaldtcolumn):\n",
    "    # Combine date and time strings\n",
    "    combined_col = df[originaldtcolumn]\n",
    "    # Convert the combined string to datetime\n",
    "    datetime_col = \"Sample\"\n",
    "\n",
    "    df[datetime_col] = pd.to_datetime(combined_col, errors=\"coerce\")\n",
    "    df[datetime_col + \"_year\"] = df[datetime_col].dt.year\n",
    "    df[datetime_col + \"_month\"] = df[datetime_col].dt.month\n",
    "    df[datetime_col + \"_day\"] = df[datetime_col].dt.day\n",
    "    df[datetime_col + \"_hour\"] = df[datetime_col].dt.hour\n",
    "    df[datetime_col + \"_minute\"] = df[datetime_col].dt.minute\n",
    "    df[datetime_col + \"_second\"] = df[datetime_col].dt.second\n",
    "    newcolnames = [\n",
    "        f\"{datetime_col}_year\",\n",
    "        f\"{datetime_col}_month\",\n",
    "        f\"{datetime_col}_day\",\n",
    "        f\"{datetime_col}_hour\",\n",
    "        f\"{datetime_col}_minute\",\n",
    "        f\"{datetime_col}_second\",\n",
    "    ]\n",
    "    # Drop the original date, time, and combined datetime columns\n",
    "    df.drop(columns=[originaldtcolumn, datetime_col], inplace=True)\n",
    "\n",
    "    return df, newcolnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_exclude = [\"ChemicalName\", \"ReportedValue\", \"Unit\", \"Qualifier\"]\n",
    "unique_columns = [col for col in data.columns if col not in columns_to_exclude]\n",
    "df_unique = data[unique_columns].drop_duplicates(subset=\"SampleDateTime\")\n",
    "exclude_from_pivoted = [\"ChemicalName\", \"ReportedValue\", \"Unit\", \"Qualifier\"]\n",
    "data_cleana, newcols = combine_date_time_strings(data_r, \"SampleDateTime\")\n",
    "newcols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleana = data_cleana.replace([np.inf, -np.inf], np.nan)\n",
    "missing_percentage = data_cleana.isnull().mean()\n",
    "# print(missing_percentage)\n",
    "clean_columns = data_cleana.columns.tolist()\n",
    "\n",
    "# Drop columns where more than 95% of the data is missing\n",
    "columns_to_drop = missing_percentage[missing_percentage > 0.90].index\n",
    "print(columns_to_drop)\n",
    "# data_clean = data_clean.dropna(subset=['Chlorophyll'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sorted_toxic = data_cleana.sort_values(by=['Sample_year', 'Sample_month', 'Sample_day', 'Sample_hour','Sample_minute','Sample_second'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sorted_toxic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge buoy and water quality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buoypath = \"../data/to_merge_buoy.csv\"\n",
    "data_buoy = pd.read_csv(buoypath)\n",
    "\n",
    "waterpath = \"../data/to_merge_water.csv\"\n",
    "data_water = pd.read_csv(waterpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_buoy['Sample_datetime'] = pd.to_datetime(dict(year=data_buoy.Sample_year,month= data_buoy.Sample_month,day=data_buoy.Sample_day,hour=data_buoy.Sample_hour,minute=data_buoy.Sample_minute,second=data_buoy.Sample_second))\n",
    "\n",
    "data_water['Sample_datetime'] = pd.to_datetime(dict(year=data_water.Sample_year,month= data_water.Sample_month,day=data_water.Sample_day,hour=data_water.Sample_hour,minute=data_water.Sample_minute,second=data_water.Sample_second))\n",
    "\n",
    "# data_buoy.set_index('Sample_datetime',inplace=True)\n",
    "# data_buoy_hourly = data_buoy.resample('H').mean()\n",
    "\n",
    "# data_water.set_index('Sample_datetime',inplace=True)\n",
    "# data_water_hourly = data_water.resample('H').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merge = pd.merge(data_buoy,data_water, on = ['Sample_datetime','Latitude','Longitude'], how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merge = data_merge.drop(columns = ['index','Sample_year_x', 'Sample_month_x','Sample_day_x','Sample_hour_x','Sample_minute_x','Sample_second_x','Sample_year_y', 'Sample_month_y','Sample_day_y','Sample_hour_y','Sample_minute_y','Sample_second_y'])\n",
    "\n",
    "# helper functions\n",
    "def combine_date_time_strings(df):\n",
    "    # Convert the combined string to datetime\n",
    "    datetime_col = \"Sample\"\n",
    "\n",
    "    df[datetime_col + \"_year\"] = df['Sample_datetime'].dt.year\n",
    "    df[datetime_col + \"_month\"] = df['Sample_datetime'].dt.month\n",
    "    df[datetime_col + \"_day\"] = df['Sample_datetime'].dt.day\n",
    "    df[datetime_col + \"_hour\"] = df['Sample_datetime'].dt.hour\n",
    "    df[datetime_col + \"_minute\"] = df['Sample_datetime'].dt.minute\n",
    "    df[datetime_col + \"_second\"] = df['Sample_datetime'].dt.second\n",
    "    return df\n",
    "\n",
    "data_merge = combine_date_time_strings(data_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merge.to_csv('../data/plank_Chesapeake_buoywater_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split merged data into seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# winter : Dec - Feb\n",
    "# spring : Mar - May\n",
    "# summer : Jun - Aug\n",
    "# fall   : Sep - Nov\n",
    "\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'summer'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'fall'\n",
    "\n",
    "data_merge['season'] = data_merge['Sample_month'].apply(get_season)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winter = data_merge[data_merge['season'] == 'winter']\n",
    "spring = data_merge[data_merge['season'] == 'spring']\n",
    "summer = data_merge[data_merge['season'] == 'summer']\n",
    "fall = data_merge[data_merge['season'] == 'fall']\n",
    "\n",
    "winter.to_csv('../data/plank_Chesapeake_buoywater_merged_winter.csv', index=False)\n",
    "spring.to_csv('../data/plank_Chesapeake_buoywater_merged_spring.csv', index=False)\n",
    "summer.to_csv('../data/plank_Chesapeake_buoywater_merged_summer.csv', index=False)\n",
    "fall.to_csv('../data/plank_Chesapeake_buoywater_merged_fall.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost and SHAP for seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import xgboost\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = ['winter','spring','summer','fall']\n",
    "for season in seasons:\n",
    "    print(\"------------------------\")\n",
    "    print(season)\n",
    "    print(\"------------------------\")\n",
    "    \n",
    "    filepath = \"../data/plank_Chesapeake_buoywater_merged_\" + season +\".csv\"\n",
    "    data = pd.read_csv(filepath)\n",
    "    \n",
    "    # drop columns with dtype = object\n",
    "    # drop surface currents since the values are too abnormal\n",
    "    object_columns = data.select_dtypes(include=['object']).columns\n",
    "    data = data.drop(columns=object_columns)\n",
    "    data = data.drop(columns = ['North surface currents','East surface currents'])\n",
    "    \n",
    "    X, y = data.drop(['Chlorophyll'], axis=1), data['Chlorophyll']\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    df = X_train\n",
    "    print(\"Original DataFrame:\")\n",
    "    print(df)\n",
    "\n",
    "    # Identify NaNs in the DataFrame\n",
    "    nan_locations = df.isna()\n",
    "    print(\"\\nLocations of NaNs in the DataFrame:\")\n",
    "    print(nan_locations)\n",
    "\n",
    "    # Count NaNs in each column\n",
    "    nan_count_per_column = df.isna().sum()\n",
    "    print(\"\\nCount of NaNs in each column:\")\n",
    "    print(nan_count_per_column)\n",
    "\n",
    "    # Count NaNs in each row\n",
    "    nan_count_per_row = df.isna().sum(axis=1)\n",
    "    print(\"\\nCount of NaNs in each row:\")\n",
    "    print(nan_count_per_row)\n",
    "\n",
    "    # Rows with at least one NaN\n",
    "    rows_with_nans = df[df.isna().any(axis=1)]\n",
    "    print(\"\\nRows with at least one NaN:\")\n",
    "    print(rows_with_nans)\n",
    "\n",
    "    # Columns with at least one NaN\n",
    "    columns_with_nans = df.columns[df.isna().any()].tolist()\n",
    "    print(\"\\nColumns with at least one NaN:\")\n",
    "    print(columns_with_nans)\n",
    "    \n",
    "    # params = dict()\n",
    "    # params[\"device\"] = \"cuda\"\n",
    "    # params[\"tree_method\"] = \"hist\"\n",
    "\n",
    "    model = xgboost.XGBRegressor(missing=np.nan, enable_categorical=True, device=\"cuda\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    print(\"MSE: %f\" % (mse))\n",
    "    \n",
    "    explainer = shap.Explainer(model=model, masker=X_train)\n",
    "    explainer.__class__\n",
    "\n",
    "    # explainer = shap.TreeExplainer(model, data=cp.asarray(X_train_cp))\n",
    "    # shap_values = explainer.shap_values(cp.asarray(X_test_cp))\n",
    "\n",
    "    # Convert Shapley values back to NumPy arrays for compatibility\n",
    "    # shap_values = cp.asnumpy(shap_values)\n",
    "    \n",
    "    shap_values = explainer(X_test)\n",
    "    \n",
    "    print(shap.plots.waterfall(shap_values[0], max_display=58))\n",
    "    print(shap.plots.bar(shap_values, max_display=58))\n",
    "    print(shap.plots.beeswarm(shap_values,max_display=58))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
