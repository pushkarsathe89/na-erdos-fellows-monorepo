{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "The Chesapeake Bay Program [DataHub](https://datahub.chesapeakebay.net/Home) contains many datasets for the Chesapeake Bay. The Water Quality Data is still updating and measures many field and lab parameters including: phosphorus, nitrogen, carbon, various other lab parameters (suspended solids, disolved solids, chlorophyll-a, alkalinkity, etc), dissolved oxygen, pH, salinity, turbitity, water temperature, and climate condition. \n",
    "\n",
    "See [Guide to Using Chesapeake Bay Program Water Quality Monitoring Data](https://d18lev1ok5leia.cloudfront.net/chesapeakebay/documents/wq_data_userguide_10feb12_mod.pdf) for more information.\n",
    " \n",
    "There is also a [DataHub API](https://datahub.chesapeakebay.net/API) which we will use to access the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What data do we want?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Chesapeake Bay segements are based on circulation and salinity properties of different areas of the Bay. We want the segements in the Chesapeake Bay proper, which haveCBSeg2003Name that starts \"CB\".\n",
    "[Map of the segments](https://www.chesapeakebay.net/what/maps/chesapeake-bay-2003-segmentation-scheme-codes).\n",
    "\n",
    "Since we want the same geographic information for every dataset, let's go ahead and generate that now. We need to retrieve the `Geographical-Id`s from the [Geographical-Attribute, CBSeg2003 list](https://datahub.chesapeakebay.net/api.json/CBSeg2003). Since we only want the segments in the Bay proper, we will search for the segment names that start with `CB`. We will then add the sounds and bays that adjoin the Chesapeake Bay. We will not include the segments for bays on the other side of the Eastern Shore from the Chesapeake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBSeg2003/10,11,12,13,14,15,16,17,28,49,84/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Add CBSeg2003 to idValues to specify type of geographic ID\n",
    "GeographicID_values = 'CBSeg2003/'\n",
    "\n",
    "# Define the URL with the CBSeg2003 list\n",
    "CBSeg2003_url = \"http://datahub.chesapeakebay.net/api.json/CBSeg2003\"\n",
    "\n",
    "# Send a GET request to the list\n",
    "response = requests.get(CBSeg2003_url)\n",
    "filtered_segments=[]\n",
    "if response.status_code == 200:\n",
    "    try:\n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "\n",
    "        # Filter the results to find CBSeg2003Name that start with \"CB\"\n",
    "        filtered_segments = [\n",
    "            segment['CBSeg2003Id'] for segment in data\n",
    "            if segment.get('CBSeg2003Name', '').startswith('CB') or \n",
    "                # Add the adjacent bays and Tangier sound\n",
    "               segment.get('CBSeg2003Name', '') in ['EASMH ', 'MOBPH ', 'TANMH ']\n",
    "        ]\n",
    "\n",
    "        # Append ids to idValues\n",
    "        if filtered_segments:\n",
    "            GeographicID_values += ','.join(map(str, filtered_segments)) + '/'\n",
    "        else:\n",
    "            print(\"No matching segments found.\")\n",
    "        \n",
    "        print(GeographicID_values)\n",
    "    \n",
    "    except ValueError as e:\n",
    "        print(f\"Failed to parse JSON data: {e}\")\n",
    "else:\n",
    "    # Handle the error\n",
    "    print(f\"Failed to retrieve data: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Water Quality\n",
    "\n",
    "CBP Water Quality Data (1984-present): measured and calculated physical and nutrient parameters\n",
    "\n",
    "The long list of parameters includes phosphorus, nitrogen, carbon, suspended solids, disolved solids, chlorophyll_a, pH, salinity, turbitidy, water temperature, and atmospheric conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Water Quality data contains quite a lot of data, so we will define functions to download 5 years of data at once. This method will also work for other large datasets, although the plankton databases seem to download over long time periods without issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def generate_date_ranges(start_date, end_date, delta_years=5):\n",
    "    # Generate date ranges of specified years between start_date and end_date.\n",
    "    date_list =[]\n",
    "    current_date = start_date\n",
    "    while current_date < end_date:\n",
    "        next_date = current_date + relativedelta(years=delta_years)\n",
    "        date_list.append(current_date.strftime('%m-%d-%Y') +'/' \n",
    "                         + next_date.strftime('%m-%d-%Y') +'/') \n",
    "        current_date = next_date\n",
    "    return(date_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a function to call the API for our entire desired time frame and output the data in one CSV. Note that for each database, we will need a `base_url` which points to the desired database and output format, a `url_idValues` which tells the API which values to download (the length of this url also depends on the database), and an `output_file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def api_to_csv_by_date(base_url, url_idValues, start_date, end_date, output_file):\n",
    "    # Generate list of date ranges\n",
    "    date_list = generate_date_ranges(start_date, end_date)\n",
    "    \n",
    "    # Open the output file in append mode\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        csv_writer = None\n",
    "        \n",
    "        for dates in date_list:\n",
    "            \n",
    "            # Define the API endpoint URL with CSV format\n",
    "            api_url = f\"{base_url}{dates}{url_idValues}\"\n",
    "\n",
    "            try:\n",
    "                response = requests.get(api_url)  # Added timeout for reliability\n",
    "                response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "            except requests.RequestException as e:\n",
    "                # Handle the error\n",
    "                print(f\"Failed to retrieve data from {api_url}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Read the CSV response\n",
    "            csv_data = response.text.splitlines()\n",
    "            csv_reader = csv.reader(csv_data)\n",
    "            \n",
    "            # Write the CSV data to the file\n",
    "            if csv_writer is None:\n",
    "                # Write the header only once\n",
    "                csv_writer = csv.writer(csvfile)\n",
    "                csv_writer.writerow(next(csv_reader))  # Write header\n",
    "            \n",
    "            for row in csv_reader:\n",
    "                csv_writer.writerow(row)\n",
    "            \n",
    "            print(f\"Data from {dates} saved to {output_file}\")\n",
    "\n",
    "\n",
    "def api_to_dataframe_by_date(base_url, url_idValues, start_date, end_date):\n",
    "    # Generate list of date ranges\n",
    "    date_list = generate_date_ranges(start_date, end_date)\n",
    "    \n",
    "    # Initialize an empty DataFrame\n",
    "    all_data = pd.DataFrame()\n",
    "    \n",
    "    for dates in date_list:\n",
    "        # Define the API endpoint URL with CSV format\n",
    "        api_url = base_url + dates + url_idValues\n",
    "        # Send a GET request to the API endpoint\n",
    "        response = requests.get(api_url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Read the CSV response into a DataFrame\n",
    "            data = pd.read_csv(api_url)\n",
    "            \n",
    "            # Append the data to the all_data DataFrame\n",
    "            all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "            \n",
    "            print(f\"Data from {dates} added to DataFrame\")\n",
    "        else:\n",
    "            # Handle the error\n",
    "            print(f\"Failed to retrieve data from {dates}: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format for the API url for \n",
    "Water Quality is: `http://datahub.chesapeakebay.net/api.{format}/WaterQuality/WaterQuality/<Start-Date>/<End-Date>/<Data-Stream-Value>/<Program-Id>/<Project-Id>/<Geographical-Attribute>/<Attribute-Id>/<Substance-Id>`\n",
    "\n",
    "We will start with defining the `base_url` which is everything before the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base URL\n",
    "base_url = 'https://datahub.chesapeakebay.net/api.CSV/WaterQuality/WaterQuality/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want all [Data-Stream-Value](https://datahub.chesapeakebay.net/api.json/DataStreams) data, so that part of our url is `0,1`\n",
    "\n",
    "We also want all three programs in the [Program-Id list for water quality](https://datahub.chesapeakebay.net/api.json/WaterQuality/Programs) `2,4,6`\n",
    "\n",
    "We can update our url to `https://datahub.chesapeakebay.net/api.CSV/WaterQuality/WaterQuality/<Start-Date>/<End-Date>/0,1/2,4,6/<Project-Id>/<Geographical-Attribute>/<Attribute-Id>/<Substance-Id>`. We will deal with the date last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the id values that go after <Start-Date>/<End-Date>\n",
    "url_idValues = '0,1/2,4,6/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get all project ids \n",
    "[Project-ID list for water quality](https://datahub.chesapeakebay.net/api.json/WaterQuality/Projects).\n",
    "Note that some projects might not have any water quality data for the desired segments, but that does not seem to cause a problem with the API timing out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1/2,4,6/12,13,14,15,35,36,2,3,11,7,33,34,23,24,16/\n"
     ]
    }
   ],
   "source": [
    "# Define the URL with the Projects list\n",
    "projectList_url = \"https://datahub.chesapeakebay.net/api.json/WaterQuality/Projects\"\n",
    "\n",
    "# Send a GET request to the list\n",
    "response = requests.get(projectList_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    data = response.json()\n",
    "    # Find ProjectId\n",
    "    projectIds = [segment['ProjectId'] for segment in data]\n",
    "    \n",
    "    # Append ids to idValues\n",
    "    for projectId in projectIds:\n",
    "        url_idValues = url_idValues + str(projectId) +','\n",
    "\n",
    "    # Remove final comma, append /\n",
    "    url_idValues = url_idValues[:-1] + '/'\n",
    "    print(url_idValues)\n",
    "else:\n",
    "    # Handle the error\n",
    "    print(f\"Failed to retrieve data: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we append the geographic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_idValues =url_idValues + GeographicID_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to append the `Substance-Id`. Now, there are many substances that are not measured in the dataset and regions we want. Using the online download form, it looks like the largest possible list is `21,30,31,35,36,49,55,60,63,65,67,71,73,74,77,78,82,83,85,87,88,94,104,105,109,111,114,116,121,123,33,76,113,34,119`. I don't see a more systematic way to do this step, since different stations are measuring different things.\n",
    "\n",
    "In addition to updating the `url_idValues`, let's also pull a dictionary for these substances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SubstanceId': 21, 'SubstanceIdentificationName': 'CHLA', 'SubstanceIdentificationDescription': 'Active Chlorophyll-A'}\n",
      "{'SubstanceId': 30, 'SubstanceIdentificationName': 'DIN', 'SubstanceIdentificationDescription': 'Dissolved Inorganic Nitrogen'}\n",
      "{'SubstanceId': 31, 'SubstanceIdentificationName': 'DO', 'SubstanceIdentificationDescription': 'Dissolved Oxygen In MG/L'}\n",
      "{'SubstanceId': 33, 'SubstanceIdentificationName': 'DO_SAT_P', 'SubstanceIdentificationDescription': 'DO Saturation Using Probe Units In Percent'}\n",
      "{'SubstanceId': 34, 'SubstanceIdentificationName': 'DOC', 'SubstanceIdentificationDescription': 'Dissolved Organic Carbon'}\n",
      "{'SubstanceId': 35, 'SubstanceIdentificationName': 'DON', 'SubstanceIdentificationDescription': 'Dissolved Organic Nitrogen'}\n",
      "{'SubstanceId': 36, 'SubstanceIdentificationName': 'DOP', 'SubstanceIdentificationDescription': 'Dissolved Organic Phosphorus'}\n",
      "{'SubstanceId': 49, 'SubstanceIdentificationName': 'FSS', 'SubstanceIdentificationDescription': 'Fixed Suspended Solids'}\n",
      "{'SubstanceId': 55, 'SubstanceIdentificationName': 'KD', 'SubstanceIdentificationDescription': 'Light Attenuation'}\n",
      "{'SubstanceId': 60, 'SubstanceIdentificationName': 'NH4F', 'SubstanceIdentificationDescription': 'Ammonium Nitrogen As N (Filtered Sample)'}\n",
      "{'SubstanceId': 63, 'SubstanceIdentificationName': 'NO23F', 'SubstanceIdentificationDescription': 'Nitrite+Nitrate Nitrogen As N (Filtered Sample)'}\n",
      "{'SubstanceId': 65, 'SubstanceIdentificationName': 'NO2F', 'SubstanceIdentificationDescription': 'Nitrite Nitrogen As N (Filtered Sample)'}\n",
      "{'SubstanceId': 67, 'SubstanceIdentificationName': 'NO3F', 'SubstanceIdentificationDescription': 'Nitrate Nitrogen As N (Filtered Sample)'}\n",
      "{'SubstanceId': 71, 'SubstanceIdentificationName': 'PC', 'SubstanceIdentificationDescription': 'Particulate Carbon; Inorganic + Organic'}\n",
      "{'SubstanceId': 73, 'SubstanceIdentificationName': 'PH', 'SubstanceIdentificationDescription': 'Ph Corrected For Temperature (25 Deg C)'}\n",
      "{'SubstanceId': 74, 'SubstanceIdentificationName': 'PHEO', 'SubstanceIdentificationDescription': 'Pheophytin'}\n",
      "{'SubstanceId': 76, 'SubstanceIdentificationName': 'PIP', 'SubstanceIdentificationDescription': 'Particulate Inorganic Phosphorus'}\n",
      "{'SubstanceId': 77, 'SubstanceIdentificationName': 'PN', 'SubstanceIdentificationDescription': 'Particulate Nitrogen'}\n",
      "{'SubstanceId': 78, 'SubstanceIdentificationName': 'PO4F', 'SubstanceIdentificationDescription': 'Orthophosphate Phosphorus As P (Filtered Sample)'}\n",
      "{'SubstanceId': 82, 'SubstanceIdentificationName': 'PP', 'SubstanceIdentificationDescription': 'Particulate Phosphorus'}\n",
      "{'SubstanceId': 83, 'SubstanceIdentificationName': 'SALINITY', 'SubstanceIdentificationDescription': 'Salinity  Units Are Parts Per Thousand (Ppt) And Are Equal To Practical Salnity Units (Psu).'}\n",
      "{'SubstanceId': 85, 'SubstanceIdentificationName': 'SECCHI', 'SubstanceIdentificationDescription': 'Secchi Depth'}\n",
      "{'SubstanceId': 87, 'SubstanceIdentificationName': 'SIF', 'SubstanceIdentificationDescription': 'Silica As Si (Filtered Sample)'}\n",
      "{'SubstanceId': 88, 'SubstanceIdentificationName': 'SIGMA_T', 'SubstanceIdentificationDescription': 'Water Density; Dependent On Salinity And Wtemp'}\n",
      "{'SubstanceId': 94, 'SubstanceIdentificationName': 'SPCOND', 'SubstanceIdentificationDescription': 'Conductivity Corrected For Temperature (25 Deg C) And Salinity'}\n",
      "{'SubstanceId': 104, 'SubstanceIdentificationName': 'TDN', 'SubstanceIdentificationDescription': 'Total Dissolved Nitrogen'}\n",
      "{'SubstanceId': 105, 'SubstanceIdentificationName': 'TDP', 'SubstanceIdentificationDescription': 'Total Dissolved Phosphorus'}\n",
      "{'SubstanceId': 109, 'SubstanceIdentificationName': 'TN', 'SubstanceIdentificationDescription': 'Total Nitrogen'}\n",
      "{'SubstanceId': 111, 'SubstanceIdentificationName': 'TON', 'SubstanceIdentificationDescription': 'Total Organic Nitrogen'}\n",
      "{'SubstanceId': 113, 'SubstanceIdentificationName': 'TOTAL_DEPTH', 'SubstanceIdentificationDescription': 'Total Station Depth'}\n",
      "{'SubstanceId': 114, 'SubstanceIdentificationName': 'TP', 'SubstanceIdentificationDescription': 'Total Phosphorus'}\n",
      "{'SubstanceId': 116, 'SubstanceIdentificationName': 'TSS', 'SubstanceIdentificationDescription': 'Total Suspended Solids'}\n",
      "{'SubstanceId': 119, 'SubstanceIdentificationName': 'TURB_NTU', 'SubstanceIdentificationDescription': 'Turbidity; Nephelometric Method'}\n",
      "{'SubstanceId': 121, 'SubstanceIdentificationName': 'VSS', 'SubstanceIdentificationDescription': 'Volatile Suspended Solids'}\n",
      "{'SubstanceId': 123, 'SubstanceIdentificationName': 'WTEMP', 'SubstanceIdentificationDescription': 'Water Temperature'}\n"
     ]
    }
   ],
   "source": [
    "substanceId_list = [21,30,31,33,34,35,36,49,55,60,63,65,67,71,73,74,76,77,78,82,83,85,87,88,94,104,105,109,111,113,114,116,119,121,123]\n",
    "\n",
    "\n",
    "# Define the URL with the substance list\n",
    "substanceId_url = \"https://datahub.chesapeakebay.net/api.json/Substances\"\n",
    "\n",
    "# Send a GET request to the list\n",
    "response = requests.get(substanceId_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    data = response.json()\n",
    "    # Filter the results to find substances with SubstanceId in substanceId_list\n",
    "    filtered_substances = [substance for substance in data if substance['SubstanceId'] in substanceId_list]\n",
    "\n",
    "    # Print the filtered substances\n",
    "    for substance in filtered_substances:\n",
    "        print(substance)\n",
    "else:\n",
    "    # Handle the error\n",
    "    print(f\"Failed to retrieve data: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_idValues = url_idValues +'21,30,31,33,34,35,36,49,55,60,63,65,67,71,73,74,76,77,78,82,83,85,87,88,94,104,105,109,111,113,114,116,119,121,123'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And call the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from 07-29-2004/07-29-2009/ saved to ../data/plankton-patrol_ChesapeakeWaterQuality.csv\n",
      "Data from 07-29-2009/07-29-2014/ saved to ../data/plankton-patrol_ChesapeakeWaterQuality.csv\n",
      "Data from 07-29-2014/07-29-2019/ saved to ../data/plankton-patrol_ChesapeakeWaterQuality.csv\n",
      "Data from 07-29-2019/07-29-2024/ saved to ../data/plankton-patrol_ChesapeakeWaterQuality.csv\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime(2004, 7, 29)\n",
    "end_date = datetime(2024, 7, 29)\n",
    "output_file = '../data/plankton-patrol_ChesapeakeWaterQuality.csv'\n",
    "\n",
    "# For CSV\n",
    "api_to_csv_by_date(base_url, url_idValues, start_date, end_date, output_file)\n",
    "\n",
    "# As dataframe\n",
    "# waterQuality = api_to_dataframe_by_date(base_url, url_idValues, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data with help of VSCode Data Wrangle extension, then save the new csv. Since we are removing empty (or almost empty) columns, we will save over the previous csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_waterQuality(df):\n",
    "    # Drop empty columns: 'PrecisionPC', 'BiasPC'\n",
    "    # Drop columns that are almost all empty or nan\n",
    "    df = df.drop(columns=['PrecisionPC','BiasPC','Details','Problem'])\n",
    "    return df\n",
    "\n",
    "# Loaded variable 'df' from output_file\n",
    "df = pd.read_csv(output_file)\n",
    "\n",
    "df_clean = clean_waterQuality(df.copy())\n",
    "\n",
    "# Same output_file as before\n",
    "df_clean.to_csv(output_file, index=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the `csv` is so large, let's add a print statement for the column headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rl/kqt6tbv90l9_pwc4927vdb340000gn/T/ipykernel_35563/2298346895.py:5: DtypeWarning: Columns (1,10,11,12,13,19,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(output_file)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CBSeg2003</th>\n",
       "      <th>EventId</th>\n",
       "      <th>Cruise</th>\n",
       "      <th>Program</th>\n",
       "      <th>Project</th>\n",
       "      <th>Agency</th>\n",
       "      <th>Source</th>\n",
       "      <th>Station</th>\n",
       "      <th>SampleDate</th>\n",
       "      <th>SampleTime</th>\n",
       "      <th>...</th>\n",
       "      <th>SampleReplicateType</th>\n",
       "      <th>Parameter</th>\n",
       "      <th>Qualifier</th>\n",
       "      <th>MeasureValue</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Method</th>\n",
       "      <th>Lab</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>TierLevel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CB1TF</td>\n",
       "      <td>20021</td>\n",
       "      <td>BAY451</td>\n",
       "      <td>TWQM</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>CB1.1</td>\n",
       "      <td>12/13/2006</td>\n",
       "      <td>10:58:00</td>\n",
       "      <td>...</td>\n",
       "      <td>FS1</td>\n",
       "      <td>CHLA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.598</td>\n",
       "      <td>UG/L</td>\n",
       "      <td>L01</td>\n",
       "      <td>MDHMH</td>\n",
       "      <td>39.54794</td>\n",
       "      <td>-76.08481</td>\n",
       "      <td>T3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CB1TF</td>\n",
       "      <td>20021</td>\n",
       "      <td>BAY451</td>\n",
       "      <td>TWQM</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>CB1.1</td>\n",
       "      <td>12/13/2006</td>\n",
       "      <td>10:58:00</td>\n",
       "      <td>...</td>\n",
       "      <td>FS2</td>\n",
       "      <td>CHLA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.748</td>\n",
       "      <td>UG/L</td>\n",
       "      <td>L01</td>\n",
       "      <td>MDHMH</td>\n",
       "      <td>39.54794</td>\n",
       "      <td>-76.08481</td>\n",
       "      <td>T3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CB1TF</td>\n",
       "      <td>20236</td>\n",
       "      <td>BAY452</td>\n",
       "      <td>TWQM</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>CB1.1</td>\n",
       "      <td>1/12/2007</td>\n",
       "      <td>13:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>FS1</td>\n",
       "      <td>CHLA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.489</td>\n",
       "      <td>UG/L</td>\n",
       "      <td>L01</td>\n",
       "      <td>MDHMH</td>\n",
       "      <td>39.54794</td>\n",
       "      <td>-76.08481</td>\n",
       "      <td>T3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CB1TF</td>\n",
       "      <td>20236</td>\n",
       "      <td>BAY452</td>\n",
       "      <td>TWQM</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>CB1.1</td>\n",
       "      <td>1/12/2007</td>\n",
       "      <td>13:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>FS2</td>\n",
       "      <td>CHLA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.987</td>\n",
       "      <td>UG/L</td>\n",
       "      <td>L01</td>\n",
       "      <td>MDHMH</td>\n",
       "      <td>39.54794</td>\n",
       "      <td>-76.08481</td>\n",
       "      <td>T3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CB1TF</td>\n",
       "      <td>20236</td>\n",
       "      <td>BAY452</td>\n",
       "      <td>TWQM</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>CB1.1</td>\n",
       "      <td>1/12/2007</td>\n",
       "      <td>13:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>S1</td>\n",
       "      <td>CHLA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.984</td>\n",
       "      <td>UG/L</td>\n",
       "      <td>L01</td>\n",
       "      <td>MDHMH</td>\n",
       "      <td>39.54794</td>\n",
       "      <td>-76.08481</td>\n",
       "      <td>T3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  CBSeg2003 EventId  Cruise Program Project Agency Source Station  SampleDate  \\\n",
       "0    CB1TF    20021  BAY451    TWQM    MAIN  MDDNR  MDDNR   CB1.1  12/13/2006   \n",
       "1    CB1TF    20021  BAY451    TWQM    MAIN  MDDNR  MDDNR   CB1.1  12/13/2006   \n",
       "2    CB1TF    20236  BAY452    TWQM    MAIN  MDDNR  MDDNR   CB1.1   1/12/2007   \n",
       "3    CB1TF    20236  BAY452    TWQM    MAIN  MDDNR  MDDNR   CB1.1   1/12/2007   \n",
       "4    CB1TF    20236  BAY452    TWQM    MAIN  MDDNR  MDDNR   CB1.1   1/12/2007   \n",
       "\n",
       "  SampleTime  ... SampleReplicateType Parameter Qualifier MeasureValue  Unit  \\\n",
       "0   10:58:00  ...                 FS1      CHLA       NaN        0.598  UG/L   \n",
       "1   10:58:00  ...                 FS2      CHLA       NaN        0.748  UG/L   \n",
       "2   13:00:00  ...                 FS1      CHLA       NaN        3.489  UG/L   \n",
       "3   13:00:00  ...                 FS2      CHLA       NaN        3.987  UG/L   \n",
       "4   13:00:00  ...                  S1      CHLA       NaN        4.984  UG/L   \n",
       "\n",
       "  Method    Lab  Latitude Longitude TierLevel  \n",
       "0   L01   MDHMH  39.54794 -76.08481        T3  \n",
       "1   L01   MDHMH  39.54794 -76.08481        T3  \n",
       "2   L01   MDHMH  39.54794 -76.08481        T3  \n",
       "3   L01   MDHMH  39.54794 -76.08481        T3  \n",
       "4   L01   MDHMH  39.54794 -76.08481        T3  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "output_file = '../data/plankton-patrol_ChesapeakeWaterQuality.csv'\n",
    "\n",
    "df = pd.read_csv(output_file)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Living Resources\n",
    "\n",
    "The Living Resources database has three data sources and we must access each separately. Also, we cannot access multiple projects at once like we could with the Water Quality database. We will write a function to fetch all projects for a given data source and save in the same csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Define the URL with the substance list\n",
    "projectIdentifier_url = \"https://datahub.chesapeakebay.net/api.json/LivingResources/Projects\"\n",
    "\n",
    "# Function to get the project IDs\n",
    "def get_project_ids(url, projectIdentifiers):\n",
    "\n",
    "    # Send a GET request to the list\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            # Parse the JSON response\n",
    "            data = response.json()\n",
    "            \n",
    "            # Filter the results to find projects with ProjectIdentifier in projectIdentifiers\n",
    "            filtered_projects = [\n",
    "                project for project in data \n",
    "                if project.get('ProjectIdentifier') in projectIdentifiers\n",
    "            ]\n",
    "            \n",
    "            # Extract the ProjectName and ProjectId for the filtered results\n",
    "            project_ids = [\n",
    "                [project['ProjectName'], project['ProjectId']] for project in filtered_projects\n",
    "            ]\n",
    "            \n",
    "            return project_ids\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"Failed to parse JSON data: {e}\")\n",
    "            return []\n",
    "    else:\n",
    "        # Handle the error\n",
    "        print(f\"Failed to retrieve data from {url}: {response.status_code} - {response.text}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a general function for producting `csv`s from the project list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_to_csv_by_project(base_url, url_idValues, start_date, end_date, project_list, output_file):\n",
    "    # Format the dates\n",
    "    start_str = start_date.strftime('%m-%d-%Y')\n",
    "    end_str = end_date.strftime('%m-%d-%Y')\n",
    "    \n",
    "    # Open the output file in write mode\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        csv_writer = None\n",
    "        \n",
    "        for project in project_list:\n",
    "            project_name, project_id = project\n",
    "\n",
    "            # Define the API endpoint URL with CSV format\n",
    "            api_url = f\"{base_url}{start_str}/{end_str}/{project_id}/{url_idValues}\"\n",
    "            \n",
    "            # Send a GET request to the API endpoint\n",
    "            response = requests.get(api_url)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                # Read the CSV response\n",
    "                csv_data = response.text.splitlines()\n",
    "                csv_reader = csv.reader(csv_data)\n",
    "                \n",
    "                # Initialize CSV writer and write header if not already done\n",
    "                if csv_writer is None:\n",
    "                    csv_writer = csv.writer(csvfile)\n",
    "                    # Write header\n",
    "                    csv_writer.writerow(next(csv_reader)) \n",
    "                \n",
    "                # Write data rows\n",
    "                for row in csv_reader:\n",
    "                    csv_writer.writerow(row)\n",
    "                \n",
    "                print(f\"Data from {project_name} saved to {output_file}\")\n",
    "            else:\n",
    "                # Provide detailed error message\n",
    "                print(f\"Failed to retrieve data from {api_url}: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the datasets might be missing columns, so we will need to create some dictionaries for the various stations. \n",
    "- Tidal Plankton does not have CBSeg2003, Latitude, or Longitude. This can be fixed with `https://datahub.chesapeakebay.net/api.JSON/LivingResources/TidalPlankton/Station/CBSeg2003`\n",
    "- Tidal Benthic does not have CBSeg2003, Latitude, or Longitude. This can be fixed with `https://datahub.chesapeakebay.net/api.JSON/LivingResources/TidalBenthic/MonitorEvent/6-29-2010/6-29-2015/1/CBSeg2003/10,11,12,13,14,15,16,17,28,49,84/` and `https://datahub.chesapeakebay.net/api.JSON/LivingResources/TidalBenthic/MonitorEvent/6-29-2010/6-29-2015/32/CBSeg2003/10,11,12,13,14,15,16,17,28,49,84/`\n",
    "- Nontidal Benthic does not have a CBSeg2003 option for data. It has a different station list, but does include latitude and longitude.\n",
    "\n",
    "This will be updated as I download the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# URLs to fetch the data from\n",
    "url1 = \"https://datahub.chesapeakebay.net/api.csv/LivingResources/TidalBenthic/MonitorEvent/7-29-2004/6-29-2015/32/CBSeg2003/10,11,12,13,14,15,16,17,28,49,84/\"\n",
    "url2 = \"https://datahub.chesapeakebay.net/api.csv/LivingResources/TidalBenthic/MonitorEvent/7-29-2004/6-29-2015/1/CBSeg2003/10,11,12,13,14,15,16,17,28,49,84/\"\n",
    "\n",
    "# Read in data from Benthic Monitor Events\n",
    "df1 = pd.read_csv(url1)\n",
    "df2 = pd.read_csv(url2)\n",
    "\n",
    "# Combine the DataFrames\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns that aren't useful for our dictionary using code from Data Wrangler and find the total number of stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stations:  750\n"
     ]
    }
   ],
   "source": [
    "def create_station_dictionary(df):\n",
    "    # Drop columns that do not contain location data\n",
    "    df = df.drop(columns=['ProjectIdentifier','FieldActivityId', 'Source', 'SampleType', 'SampleDate', 'Layer', 'PDepth', 'Salzone', 'SampleVolume', 'Units', 'TotalDepth', 'SampleTime'])\n",
    "\n",
    "    # Remove rows with 'Station' empty\n",
    "    df = df[df['Station'].notna()]\n",
    "\n",
    "    # Remove duplicate rows\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    return df\n",
    "\n",
    "station_dictionary = create_station_dictionary(combined_df.copy())\n",
    "\n",
    "print(\"Number of stations: \", station_dictionary.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tidal Plankton\n",
    "\n",
    "We should only need the data type `Reported`. The API has the form `TidalPlankton/Reported/<Start-Date>/<End-Date>/<Project-Id>/<Geographical-Attribute>/<Attribute-Id>`, where `<Attribute-Id>` is optional.\n",
    "\n",
    "We define our `base_url`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://datahub.chesapeakebay.net/api.csv/LivingResources/TidalPlankton/Reported/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four projects: `MEZ`, `MIZ`, `PHYTP`, and `PICOP`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "projectList_TidalPlankton = ['MEZ','MIZ','PHYTP','PICOP']\n",
    "\n",
    "projectIds_TidalPlankton = get_project_ids(projectIdentifier_url,projectList_TidalPlankton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we generate the `csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from Tidal Phytoplankton Monitoring saved to ../data/plankton-patrol_ChesapeakeTidalPlankton.csv\n",
      "Data from Tidal Picoplankton Monitoring saved to ../data/plankton-patrol_ChesapeakeTidalPlankton.csv\n",
      "Data from Tidal Mesozooplankton Monitoring saved to ../data/plankton-patrol_ChesapeakeTidalPlankton.csv\n",
      "Data from Tidal Microzooplankton Monitoring saved to ../data/plankton-patrol_ChesapeakeTidalPlankton.csv\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime(2004, 7, 29)\n",
    "end_date = datetime(2024, 7, 29)\n",
    "output_file = '../data/plankton-patrol_ChesapeakeTidalPlankton.csv'\n",
    "\n",
    "api_to_csv_by_project(base_url, GeographicID_values, start_date, end_date, projectIds_TidalPlankton, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rl/kqt6tbv90l9_pwc4927vdb340000gn/T/ipykernel_23021/4239831696.py:1: DtypeWarning: Columns (0,1,2,9,10,15,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tidalPlankton = pd.read_csv(output_file)\n"
     ]
    }
   ],
   "source": [
    "tidalPlankton = pd.read_csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Station': ['CB5.2' 'CB7.4' 'CB7.3E' 'CB4.3C' 'CB3.3C' 'CB2.2' 'WE4.2' 'CB6.4'\n",
      " 'CB6.1' 'CB1.1' 'EE3.2' 'EE3.1' 'EE1.1' nan 'Station']\n"
     ]
    }
   ],
   "source": [
    "unique_values = tidalPlankton['Station'].unique()\n",
    "print(f\"Unique values in Station': {unique_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tidal Benthic Data\n",
    "\n",
    "There are four (possibly five) relevant data types for the Tidal Benthic Data. Since Indicator of Benthic Integrity (IBI) is calculated, we will ignore it for now.\n",
    "\n",
    "Tidal Benthic Data (1971-2013): taxonomic abundance and composition, biomass, sediment, water quality and Indicator of Benthic Integrity (IBI)\n",
    "\n",
    "Each of these data types uses the projects BEN - Tidal Benthic Monitoring and SBEN - Special Tidal Benthic Monitoring.\n",
    "\n",
    "The url API format is `http://datahub.chesapeakebay.net/api.{format}/LivingResources/TidalBenthic/<Data-Type>/<Start-Date>/<End-Date>/<Project-Id>/<Geographical-Attribute>/<Attribute-Id>`. It appears the `Attribut-Id` is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://datahub.chesapeakebay.net/api.csv/LivingResources/TidalBenthic/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the list of data types and projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidalBenthic_dataTypes = ['Sediment', 'BioMass','Taxonomic','WaterQuality']\n",
    "tidalBenthic_projectIds = ['BEN','SBEN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we loop through the data types let's run the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type:  Sediment\n",
      "Data from Tidal Benthic Monitoring saved to ../data/plankton-patrol_ChesapeakeBenthicSediment.csv\n",
      "Data from Special Tidal Benthic Monitoring saved to ../data/plankton-patrol_ChesapeakeBenthicSediment.csv\n",
      "Data Type:  BioMass\n",
      "Data from Tidal Benthic Monitoring saved to ../data/plankton-patrol_ChesapeakeBenthicBioMass.csv\n",
      "Data from Special Tidal Benthic Monitoring saved to ../data/plankton-patrol_ChesapeakeBenthicBioMass.csv\n",
      "Data Type:  Taxonomic\n",
      "Data from Tidal Benthic Monitoring saved to ../data/plankton-patrol_ChesapeakeBenthicTaxonomic.csv\n",
      "Data from Special Tidal Benthic Monitoring saved to ../data/plankton-patrol_ChesapeakeBenthicTaxonomic.csv\n",
      "Data Type:  WaterQuality\n",
      "Data from Tidal Benthic Monitoring saved to ../data/plankton-patrol_ChesapeakeBenthicWaterQuality.csv\n",
      "Data from Special Tidal Benthic Monitoring saved to ../data/plankton-patrol_ChesapeakeBenthicWaterQuality.csv\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime(2004, 7, 29)\n",
    "end_date = datetime(2024, 7, 29)\n",
    "\n",
    "projectIds = get_project_ids(projectIdentifier_url,tidalBenthic_projectIds)\n",
    "\n",
    "for dataType in tidalBenthic_dataTypes:\n",
    "    print(\"Data Type: \", dataType)\n",
    "    new_base_url = f\"{base_url}{dataType}/\"\n",
    "    output_file = f\"../data/plankton-patrol_ChesapeakeBenthic{dataType}.csv\"\n",
    "\n",
    "    # For CSV\n",
    "    api_to_csv_by_project(new_base_url, GeographicID_values, start_date, end_date, projectIds,output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
