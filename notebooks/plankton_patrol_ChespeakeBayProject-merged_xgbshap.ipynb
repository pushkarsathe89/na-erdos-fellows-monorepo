{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "# import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from catboost import CatBoostRegressor\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_path = '../data/plank_Chesapeake_buoywater_merged.csv'\n",
    "data = pd.read_csv(merged_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge columns from two datasets - they do not overlap\n",
    "data['Chlorophyll_Merged'] = data['Chlorophyll'].fillna(data['CHLA'])\n",
    "data = data.drop(columns=['Chlorophyll','CHLA'])\n",
    "data = data.rename(columns={'Chlorophyll_Merged': 'Chlorophyll'})\n",
    "\n",
    "data['Salinity_Merged'] = data['Salinity'].fillna(data['SALINITY'])\n",
    "data = data.drop(columns=['Salinity','SALINITY'])\n",
    "data = data.rename(columns={'Salinity_Merged': 'Salinity'})\n",
    "\n",
    "data['Wtemp_Merged'] = data['Temperature'].fillna(data['WTEMP'])\n",
    "data = data.drop(columns=['Temperature','WTEMP'])\n",
    "data = data.rename(columns={'Wtemp_Merged': 'Temperature'})\n",
    "\n",
    "data['DO_Merged'] = data['Oxygen'].fillna(data['DO'])\n",
    "data = data.drop(columns=['Oxygen','DO'])\n",
    "data = data.rename(columns={'DO_Merged': 'Oxygen'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.drop(['Chlorophyll'], axis=1), data['Chlorophyll']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid to search over\n",
    "# model = xgboost.XGBRegressor(\n",
    "#     max_depth=3,  # Maximum depth of a tree\n",
    "#     learning_rate=0.1,  # Learning rate\n",
    "#     n_estimators=100,  # Number of trees\n",
    "#     subsample=0.8,  # Subsample ratio of the training instances\n",
    "#     colsample_bytree=0.8,  # Subsample ratio of columns when constructing each tree\n",
    "#     gamma=0,  # Minimum loss reduction required to make a further partition\n",
    "#     min_child_weight=1,  # Minimum sum of instance weight needed in a child\n",
    "#     device=\"cuda\",\n",
    "#     enable_categorical=True,\n",
    "# )\n",
    "param_grid = {\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"learning_rate\": [0.1, 0.01, 0.001, 0.0001],\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "}\n",
    "models = {\n",
    "    # \"LightGBM\": (\n",
    "    #     lgb.LGBMRegressor(),\n",
    "    #     {\n",
    "    #         \"max_depth\": [3, 5, 7],\n",
    "    #         \"n_estimators\": [50, 100, 200],\n",
    "    #         \"learning_rate\": [0.1, 0.01, 0.001, 0.0001]\n",
    "    #     },\n",
    "    # ),\n",
    "    \"XGBoost\": (\n",
    "        xgb.XGBRegressor(),\n",
    "        {\n",
    "            \"max_depth\": [3, 5, 7],\n",
    "            \"n_estimators\": [50, 100, 200],\n",
    "            \"learning_rate\": [0.1, 0.01, 0.001, 0.0001]\n",
    "        },\n",
    "    ),\n",
    "    \"MLPRegressor\": (\n",
    "        MLPRegressor(max_iter=1000),\n",
    "        {\"hidden_layer_sizes\": [(50, 50), (100,)], \"learning_rate_init\": [0.001, 0.01]},\n",
    "    ),\n",
    "}\n",
    "\n",
    "# models = {\n",
    "#     'LightGBM': lgb.LGBMRegressor(),\n",
    "#     'XGBoost': xgb.XGBRegressor(),\n",
    "#     'MLPRegressor': CatBoostRegressor(silent=True)\n",
    "# }\n",
    "# Make custom scorer\n",
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "mse_scores = []\n",
    "kfold_bestparams = []\n",
    "allshapvalues = []\n",
    "best_models = {}\n",
    "kf = KFold(n_splits=5)\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "\n",
    "\n",
    "    # Create a GridSearchCV object\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model, param_grid=param_grid, cv=kf, scoring=mse_scorer\n",
    "    )\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(\n",
    "    #     X, y, test_size=0.2, random_state=42\n",
    "    # )\n",
    "    # Fit the grid search to the training data\n",
    "    # grid_search.fit(X_train, y_train)\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Get the best parameters\n",
    "    # best_params = grid_search.best_params_\n",
    "    # print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "    # Create a new model with the best parameters\n",
    "    # best_model = model(**best_params)\n",
    "    # # Train the best model\n",
    "    # best_model.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Store the best model and its parameters\n",
    "    best_models[model_name] = {\n",
    "        \"best_estimator\": best_model,\n",
    "        \"best_params\": grid_search.best_params_,\n",
    "        \"best_score\": grid_search.best_score_\n",
    "    }\n",
    "    # Predict on the test set\n",
    "    # y_pred_best = best_model.predict(X_test)\n",
    "    # mse = mean_squared_error(y_test, y_pred_best)\n",
    "\n",
    "    # Evaluate the best model\n",
    "    # accuracy_best = (y_pred_best == y_test).mean()\n",
    "    \n",
    "    print(f\"MSE with Best Parameters: {best_models[model_name]['best_score']}\")\n",
    "    mse_scores.append(best_models[model_name]['best_score'])\n",
    "    kfold_bestparams.append(best_models[model_name]['best_params'])\n",
    "\n",
    "    # plt.figure()\n",
    "    explainer = shap.Explainer(model=best_model, masker=X)\n",
    "    # plt.savefig(\n",
    "    # f\"shap_explainer_best_{model_name}_fold-{i}.png\", bbox_inches=\"tight\"\n",
    "    # )\n",
    "    # plt.close()\n",
    "    # for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    #     X_train, X_test, y_train, y_test = (\n",
    "    #         X.iloc[train_index],\n",
    "    #         X.iloc[test_index],\n",
    "    #         y.iloc[train_index],\n",
    "    #         y.iloc[test_index],\n",
    "    #     )\n",
    "    shap_values = explainer(X)\n",
    "    allshapvalues.append(shap_values)\n",
    "    explainer.__class__\n",
    "\n",
    "    # plt.figure()\n",
    "    shap.summary_plot(shap_values, X, show=False)\n",
    "    # plt.savefig(\n",
    "    #     f\"shap_summary_plot_best_{model_name}_fold-{i}.png\", bbox_inches=\"tight\"\n",
    "    # )\n",
    "    # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
